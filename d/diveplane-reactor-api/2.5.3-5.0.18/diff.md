# Comparing `tmp/diveplane_reactor_api-2.5.3-py3-none-any.whl.zip` & `tmp/diveplane_reactor_api-5.0.18-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,72 +1,72 @@
-Zip file size: 274154 bytes, number of entries: 70
--rw-r--r--  2.0 unx   171737 b- defN 23-Jun-12 15:20 diveplane/client/LICENSE-3RD-PARTY.txt
--rw-r--r--  2.0 unx     2145 b- defN 23-Jun-12 15:20 diveplane/client/__init__.py
--rw-r--r--  2.0 unx    14544 b- defN 23-Jun-12 15:19 diveplane/client/base.py
--rw-r--r--  2.0 unx     3080 b- defN 23-Jun-12 15:19 diveplane/client/cache.py
--rw-r--r--  2.0 unx    16092 b- defN 23-Jun-12 15:19 diveplane/client/client.py
--rw-r--r--  2.0 unx     4594 b- defN 23-Jun-12 15:19 diveplane/client/configuration.py
--rw-r--r--  2.0 unx     4329 b- defN 23-Jun-12 15:19 diveplane/client/exceptions.py
--rw-r--r--  2.0 unx      958 b- defN 23-Jun-12 15:19 diveplane/client/protocols.py
--rw-r--r--  2.0 unx     1900 b- defN 23-Jun-12 15:20 diveplane/client/requirements-3.10.txt
--rw-r--r--  2.0 unx     1900 b- defN 23-Jun-12 15:20 diveplane/client/requirements-3.11.txt
--rw-r--r--  2.0 unx     1919 b- defN 23-Jun-12 15:20 diveplane/client/requirements-3.8.txt
--rw-r--r--  2.0 unx     1898 b- defN 23-Jun-12 15:20 diveplane/client/requirements-3.9.txt
--rw-r--r--  2.0 unx     3291 b- defN 23-Jun-12 15:20 diveplane/client/requirements-dev-3.10.txt
--rw-r--r--  2.0 unx     3060 b- defN 23-Jun-12 15:20 diveplane/client/requirements-dev-3.11.txt
--rw-r--r--  2.0 unx     3278 b- defN 23-Jun-12 15:20 diveplane/client/requirements-dev-3.8.txt
--rw-r--r--  2.0 unx     3278 b- defN 23-Jun-12 15:20 diveplane/client/requirements-dev-3.9.txt
--rw-r--r--  2.0 unx      210 b- defN 23-Jun-12 15:20 diveplane/client/requirements-dev.in
--rw-r--r--  2.0 unx      371 b- defN 23-Jun-12 15:20 diveplane/client/requirements.in
--rw-r--r--  2.0 unx      175 b- defN 23-Jun-12 15:19 diveplane/client/pandas/__init__.py
--rw-r--r--  2.0 unx    10935 b- defN 23-Jun-12 15:19 diveplane/client/pandas/client.py
--rw-r--r--  2.0 unx      845 b- defN 23-Jun-12 15:19 diveplane/client/tests/__init__.py
--rw-r--r--  2.0 unx    50937 b- defN 23-Jun-12 15:19 diveplane/client/tests/test_client.py
--rw-r--r--  2.0 unx    14329 b- defN 23-Jun-12 15:19 diveplane/client/tests/test_infer_feature_attributes.py
--rw-r--r--  2.0 unx     8783 b- defN 23-Jun-12 15:19 diveplane/client/tests/test_scikit.py
--rw-r--r--  2.0 unx      193 b- defN 23-Jun-12 15:19 diveplane/direct/__init__.py
--rw-r--r--  2.0 unx     4538 b- defN 23-Jun-12 15:19 diveplane/direct/_utilities.py
--rw-r--r--  2.0 unx   214199 b- defN 23-Jun-12 15:19 diveplane/direct/client.py
--rw-r--r--  2.0 unx    66688 b- defN 23-Jun-12 15:19 diveplane/direct/core.py
--rw-r--r--  2.0 unx      923 b- defN 23-Jun-12 15:19 diveplane/direct/tests/test_standalone.py
--rw-r--r--  2.0 unx      768 b- defN 23-Jun-12 15:19 diveplane/reactor/__init__.py
--rw-r--r--  2.0 unx     1282 b- defN 23-Jun-12 15:19 diveplane/reactor/client.py
--rw-r--r--  2.0 unx    10613 b- defN 23-Jun-12 15:19 diveplane/reactor/project.py
--rw-r--r--  2.0 unx     9637 b- defN 23-Jun-12 15:19 diveplane/reactor/session.py
--rw-r--r--  2.0 unx   127203 b- defN 23-Jun-12 15:19 diveplane/reactor/trainee.py
--rw-r--r--  2.0 unx      302 b- defN 23-Jun-12 15:19 diveplane/reactor/tests/conftest.py
--rw-r--r--  2.0 unx     3243 b- defN 23-Jun-12 15:19 diveplane/reactor/tests/test_reactor.py
--rw-r--r--  2.0 unx      426 b- defN 23-Jun-12 15:19 diveplane/scikit/__init__.py
--rw-r--r--  2.0 unx    54916 b- defN 23-Jun-12 15:19 diveplane/scikit/scikit.py
--rw-r--r--  2.0 unx     1758 b- defN 23-Jun-12 15:19 diveplane/utilities/__init__.py
--rw-r--r--  2.0 unx     3256 b- defN 23-Jun-12 15:19 diveplane/utilities/eula_helper.py
--rw-r--r--  2.0 unx    22403 b- defN 23-Jun-12 15:19 diveplane/utilities/features.py
--rw-r--r--  2.0 unx      698 b- defN 23-Jun-12 15:19 diveplane/utilities/guess_feature_attributes.py
--rw-r--r--  2.0 unx    39646 b- defN 23-Jun-12 15:19 diveplane/utilities/installation_verification.py
--rw-r--r--  2.0 unx    25873 b- defN 23-Jun-12 15:19 diveplane/utilities/internals.py
--rw-r--r--  2.0 unx     2126 b- defN 23-Jun-12 15:19 diveplane/utilities/json_wrapper.py
--rw-r--r--  2.0 unx     2424 b- defN 23-Jun-12 15:19 diveplane/utilities/locale.py
--rw-r--r--  2.0 unx     4697 b- defN 23-Jun-12 15:19 diveplane/utilities/monitors.py
--rw-r--r--  2.0 unx     2341 b- defN 23-Jun-12 15:19 diveplane/utilities/posix.py
--rw-r--r--  2.0 unx      380 b- defN 23-Jun-12 15:19 diveplane/utilities/random.py
--rw-r--r--  2.0 unx    42853 b- defN 23-Jun-12 15:19 diveplane/utilities/utilities.py
--rw-r--r--  2.0 unx      194 b- defN 23-Jun-12 15:19 diveplane/utilities/feature_attributes/__init__.py
--rw-r--r--  2.0 unx    25413 b- defN 23-Jun-12 15:19 diveplane/utilities/feature_attributes/base.py
--rw-r--r--  2.0 unx    16249 b- defN 23-Jun-12 15:19 diveplane/utilities/feature_attributes/infer_feature_attributes.py
--rw-r--r--  2.0 unx    22625 b- defN 23-Jun-12 15:19 diveplane/utilities/feature_attributes/pandas.py
--rw-r--r--  2.0 unx     6359 b- defN 23-Jun-12 15:19 diveplane/utilities/feature_attributes/protocols.py
--rw-r--r--  2.0 unx    39180 b- defN 23-Jun-12 15:19 diveplane/utilities/feature_attributes/relational.py
--rw-r--r--  2.0 unx    31817 b- defN 23-Jun-12 15:19 diveplane/utilities/feature_attributes/time_series.py
--rw-r--r--  2.0 unx    19545 b- defN 23-Jun-12 15:19 diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py
--rw-r--r--  2.0 unx     6442 b- defN 23-Jun-12 15:19 diveplane/utilities/feature_attributes/tests/test_infer_time_series_attributes.py
--rw-r--r--  2.0 unx     1321 b- defN 23-Jun-12 15:19 diveplane/utilities/tests/__init__.py
--rw-r--r--  2.0 unx       51 b- defN 23-Jun-12 15:19 diveplane/utilities/tests/conftest.py
--rw-r--r--  2.0 unx    13611 b- defN 23-Jun-12 15:19 diveplane/utilities/tests/test_features.py
--rw-r--r--  2.0 unx     8189 b- defN 23-Jun-12 15:19 diveplane/utilities/tests/test_internals.py
--rw-r--r--  2.0 unx    11237 b- defN 23-Jun-12 15:19 diveplane/utilities/tests/test_utilities.py
--rw-r--r--  2.0 unx    12104 b- defN 23-Jun-12 15:20 diveplane_reactor_api-2.5.3.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     1356 b- defN 23-Jun-12 15:20 diveplane_reactor_api-2.5.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-12 15:20 diveplane_reactor_api-2.5.3.dist-info/WHEEL
--rw-r--r--  2.0 unx      157 b- defN 23-Jun-12 15:20 diveplane_reactor_api-2.5.3.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       10 b- defN 23-Jun-12 15:20 diveplane_reactor_api-2.5.3.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     6628 b- defN 23-Jun-12 15:20 diveplane_reactor_api-2.5.3.dist-info/RECORD
-70 files, 1190554 bytes uncompressed, 263442 bytes compressed:  77.9%
+Zip file size: 277923 bytes, number of entries: 70
+-rw-r--r--  2.0 unx   171737 b- defN 23-Jul-04 19:26 diveplane/client/LICENSE-3RD-PARTY.txt
+-rw-r--r--  2.0 unx     2146 b- defN 23-Jul-04 19:26 diveplane/client/__init__.py
+-rw-r--r--  2.0 unx    14410 b- defN 23-Jul-04 19:25 diveplane/client/base.py
+-rw-r--r--  2.0 unx     3080 b- defN 23-Jul-04 19:25 diveplane/client/cache.py
+-rw-r--r--  2.0 unx    16092 b- defN 23-Jul-04 19:25 diveplane/client/client.py
+-rw-r--r--  2.0 unx     4594 b- defN 23-Jul-04 19:25 diveplane/client/configuration.py
+-rw-r--r--  2.0 unx     4329 b- defN 23-Jul-04 19:25 diveplane/client/exceptions.py
+-rw-r--r--  2.0 unx      958 b- defN 23-Jul-04 19:25 diveplane/client/protocols.py
+-rw-r--r--  2.0 unx     1900 b- defN 23-Jul-04 19:26 diveplane/client/requirements-3.10.txt
+-rw-r--r--  2.0 unx     1900 b- defN 23-Jul-04 19:26 diveplane/client/requirements-3.11.txt
+-rw-r--r--  2.0 unx     1919 b- defN 23-Jul-04 19:26 diveplane/client/requirements-3.8.txt
+-rw-r--r--  2.0 unx     1898 b- defN 23-Jul-04 19:26 diveplane/client/requirements-3.9.txt
+-rw-r--r--  2.0 unx     3309 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev-3.10.txt
+-rw-r--r--  2.0 unx     3060 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev-3.11.txt
+-rw-r--r--  2.0 unx     3296 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev-3.8.txt
+-rw-r--r--  2.0 unx     3296 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev-3.9.txt
+-rw-r--r--  2.0 unx      210 b- defN 23-Jul-04 19:26 diveplane/client/requirements-dev.in
+-rw-r--r--  2.0 unx      371 b- defN 23-Jul-04 19:26 diveplane/client/requirements.in
+-rw-r--r--  2.0 unx      175 b- defN 23-Jul-04 19:25 diveplane/client/pandas/__init__.py
+-rw-r--r--  2.0 unx    10935 b- defN 23-Jul-04 19:25 diveplane/client/pandas/client.py
+-rw-r--r--  2.0 unx      845 b- defN 23-Jul-04 19:25 diveplane/client/tests/__init__.py
+-rw-r--r--  2.0 unx    50937 b- defN 23-Jul-04 19:25 diveplane/client/tests/test_client.py
+-rw-r--r--  2.0 unx    13534 b- defN 23-Jul-04 19:25 diveplane/client/tests/test_infer_feature_attributes.py
+-rw-r--r--  2.0 unx     8783 b- defN 23-Jul-04 19:25 diveplane/client/tests/test_scikit.py
+-rw-r--r--  2.0 unx      193 b- defN 23-Jul-04 19:25 diveplane/direct/__init__.py
+-rw-r--r--  2.0 unx     4538 b- defN 23-Jul-04 19:25 diveplane/direct/_utilities.py
+-rw-r--r--  2.0 unx   213772 b- defN 23-Jul-04 19:25 diveplane/direct/client.py
+-rw-r--r--  2.0 unx    66259 b- defN 23-Jul-04 19:25 diveplane/direct/core.py
+-rw-r--r--  2.0 unx      923 b- defN 23-Jul-04 19:25 diveplane/direct/tests/test_standalone.py
+-rw-r--r--  2.0 unx      768 b- defN 23-Jul-04 19:25 diveplane/reactor/__init__.py
+-rw-r--r--  2.0 unx     1282 b- defN 23-Jul-04 19:25 diveplane/reactor/client.py
+-rw-r--r--  2.0 unx    10613 b- defN 23-Jul-04 19:25 diveplane/reactor/project.py
+-rw-r--r--  2.0 unx     9637 b- defN 23-Jul-04 19:25 diveplane/reactor/session.py
+-rw-r--r--  2.0 unx   126345 b- defN 23-Jul-04 19:25 diveplane/reactor/trainee.py
+-rw-r--r--  2.0 unx      302 b- defN 23-Jul-04 19:25 diveplane/reactor/tests/conftest.py
+-rw-r--r--  2.0 unx     3243 b- defN 23-Jul-04 19:25 diveplane/reactor/tests/test_reactor.py
+-rw-r--r--  2.0 unx      426 b- defN 23-Jul-04 19:25 diveplane/scikit/__init__.py
+-rw-r--r--  2.0 unx    54659 b- defN 23-Jul-04 19:25 diveplane/scikit/scikit.py
+-rw-r--r--  2.0 unx     1741 b- defN 23-Jul-04 19:25 diveplane/utilities/__init__.py
+-rw-r--r--  2.0 unx     3655 b- defN 23-Jul-04 19:25 diveplane/utilities/eula_helper.py
+-rw-r--r--  2.0 unx    22403 b- defN 23-Jul-04 19:25 diveplane/utilities/features.py
+-rw-r--r--  2.0 unx      698 b- defN 23-Jul-04 19:25 diveplane/utilities/guess_feature_attributes.py
+-rw-r--r--  2.0 unx    39658 b- defN 23-Jul-04 19:25 diveplane/utilities/installation_verification.py
+-rw-r--r--  2.0 unx    25873 b- defN 23-Jul-04 19:25 diveplane/utilities/internals.py
+-rw-r--r--  2.0 unx     2126 b- defN 23-Jul-04 19:25 diveplane/utilities/json_wrapper.py
+-rw-r--r--  2.0 unx     2424 b- defN 23-Jul-04 19:25 diveplane/utilities/locale.py
+-rw-r--r--  2.0 unx     4697 b- defN 23-Jul-04 19:25 diveplane/utilities/monitors.py
+-rw-r--r--  2.0 unx     2341 b- defN 23-Jul-04 19:25 diveplane/utilities/posix.py
+-rw-r--r--  2.0 unx      380 b- defN 23-Jul-04 19:25 diveplane/utilities/random.py
+-rw-r--r--  2.0 unx    40637 b- defN 23-Jul-04 19:25 diveplane/utilities/utilities.py
+-rw-r--r--  2.0 unx      194 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/__init__.py
+-rw-r--r--  2.0 unx    39499 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/base.py
+-rw-r--r--  2.0 unx    16407 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/infer_feature_attributes.py
+-rw-r--r--  2.0 unx    23164 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/pandas.py
+-rw-r--r--  2.0 unx     6359 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/protocols.py
+-rw-r--r--  2.0 unx    37406 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/relational.py
+-rw-r--r--  2.0 unx    31809 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/time_series.py
+-rw-r--r--  2.0 unx    24901 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py
+-rw-r--r--  2.0 unx     6442 b- defN 23-Jul-04 19:25 diveplane/utilities/feature_attributes/tests/test_infer_time_series_attributes.py
+-rw-r--r--  2.0 unx     1321 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/__init__.py
+-rw-r--r--  2.0 unx       51 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/conftest.py
+-rw-r--r--  2.0 unx    13611 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/test_features.py
+-rw-r--r--  2.0 unx     8189 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/test_internals.py
+-rw-r--r--  2.0 unx    11237 b- defN 23-Jul-04 19:25 diveplane/utilities/tests/test_utilities.py
+-rw-r--r--  2.0 unx    12104 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     3192 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/WHEEL
+-rw-r--r--  2.0 unx      157 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       10 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6634 b- defN 23-Jul-04 19:26 diveplane_reactor_api-5.0.18.dist-info/RECORD
+70 files, 1206086 bytes uncompressed, 267199 bytes compressed:  77.8%
```

## zipnote {}

```diff
@@ -186,26 +186,26 @@
 
 Filename: diveplane/utilities/tests/test_internals.py
 Comment: 
 
 Filename: diveplane/utilities/tests/test_utilities.py
 Comment: 
 
-Filename: diveplane_reactor_api-2.5.3.dist-info/LICENSE.txt
+Filename: diveplane_reactor_api-5.0.18.dist-info/LICENSE.txt
 Comment: 
 
-Filename: diveplane_reactor_api-2.5.3.dist-info/METADATA
+Filename: diveplane_reactor_api-5.0.18.dist-info/METADATA
 Comment: 
 
-Filename: diveplane_reactor_api-2.5.3.dist-info/WHEEL
+Filename: diveplane_reactor_api-5.0.18.dist-info/WHEEL
 Comment: 
 
-Filename: diveplane_reactor_api-2.5.3.dist-info/entry_points.txt
+Filename: diveplane_reactor_api-5.0.18.dist-info/entry_points.txt
 Comment: 
 
-Filename: diveplane_reactor_api-2.5.3.dist-info/top_level.txt
+Filename: diveplane_reactor_api-5.0.18.dist-info/top_level.txt
 Comment: 
 
-Filename: diveplane_reactor_api-2.5.3.dist-info/RECORD
+Filename: diveplane_reactor_api-5.0.18.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## diveplane/client/__init__.py

```diff
@@ -63,8 +63,8 @@
 ]
 
 
 # The version number is automatically incremented by the pipeline
 # It should not be manually changed.
 # To change the major/minor number change the number in azure-pipelines.yml
 
-__version__ = "2.5.3"
+__version__ = "5.0.18"
```

## diveplane/client/base.py

```diff
@@ -239,15 +239,14 @@
     @abstractmethod
     def react_into_features(
         self, trainee_id, *,
         distance_contribution=False,
         familiarity_conviction_addition=False,
         familiarity_conviction_removal=False,
         features=None,
-        high_distance_accuracy=True,
         p_value_of_addition=False,
         p_value_of_removal=False,
         use_case_weights=False,
         weight_feature=None
     ):
         """Calculate conviction and other data for the specified feature(s)."""
 
@@ -276,15 +275,14 @@
     @abstractmethod
     def react_group(
         self, trainee_id, *,
         distance_contributions=False,
         familiarity_conviction_addition=True,
         familiarity_conviction_removal=False,
         features=None,
-        high_distance_accuracy=True,
         kl_divergence_addition=False,
         kl_divergence_removal=False,
         new_cases=None,
         p_value_of_addition=False,
         p_value_of_removal=False,
         trainees_to_compare=None,
         use_case_weights=False,
@@ -387,15 +385,14 @@
 
     @abstractmethod
     def get_feature_conviction(self, trainee_id, *,
                                features=None,
                                action_features=None,
                                familiarity_conviction_addition=True,
                                familiarity_conviction_removal=False,
-                               high_distance_accuracy=True,
                                weight_feature=None,
                                use_case_weights=False):
         """Get familiarity conviction for features in the model."""
 
     @abstractmethod
     def add_feature(self, trainee_id, feature, feature_value=None, *,
                     condition=None, condition_session=None,
```

## diveplane/client/requirements-3.10.txt

```diff
@@ -9,40 +9,40 @@
     # via -r requirements.in
 cffi==1.15.1
     # via cryptography
 cryptography==41.0.1
     # via -r requirements.in
 deprecation==2.1.0
     # via -r requirements.in
-diveplane-amalgam-api==2.3.7
+diveplane-amalgam-api==2.3.9
     # via -r requirements.in
-diveplane-openapi-client==22.2.4
+diveplane-openapi-client==23.0.5
     # via -r requirements.in
-faker==18.10.1
+faker==18.11.2
     # via -r requirements.in
-humanize==4.6.0
+humanize==4.7.0
     # via -r requirements.in
-joblib==1.2.0
+joblib==1.3.1
     # via scikit-learn
-markdown-it-py==2.2.0
+markdown-it-py==3.0.0
     # via rich
 mdurl==0.1.2
     # via markdown-it-py
 mmh3==4.0.0
     # via -r requirements.in
-numpy==1.24.3
+numpy==1.25.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
     #   pandas
     #   scikit-learn
     #   scipy
 packaging==23.1
     # via deprecation
-pandas==2.0.2
+pandas==2.0.3
     # via -r requirements.in
 pycparser==2.21
     # via cffi
 pygments==2.15.1
     # via rich
 pyjwt==2.7.0
     # via -r requirements.in
@@ -51,35 +51,35 @@
     #   diveplane-openapi-client
     #   faker
     #   pandas
 pytz==2023.3
     # via pandas
 pyyaml==6.0
     # via -r requirements.in
-rich==13.4.1
+rich==13.4.2
     # via -r requirements.in
-scikit-learn==1.2.2
+scikit-learn==1.3.0
     # via -r requirements.in
-scipy==1.10.1
+scipy==1.11.1
     # via scikit-learn
 semantic-version==2.10.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
 six==1.16.0
     # via
     #   diveplane-openapi-client
     #   python-dateutil
 threadpoolctl==3.1.0
     # via scikit-learn
-typing-extensions==4.6.3
+typing-extensions==4.7.1
     # via -r requirements.in
 tzdata==2023.3
     # via pandas
 urllib3==1.26.16
     # via
     #   -r requirements.in
     #   diveplane-openapi-client
 
 # The following packages are considered to be unsafe in a requirements file:
-setuptools==67.8.0
+setuptools==68.0.0
     # via diveplane-openapi-client
```

## diveplane/client/requirements-3.11.txt

```diff
@@ -9,40 +9,40 @@
     # via -r requirements.in
 cffi==1.15.1
     # via cryptography
 cryptography==41.0.1
     # via -r requirements.in
 deprecation==2.1.0
     # via -r requirements.in
-diveplane-amalgam-api==2.3.7
+diveplane-amalgam-api==2.3.9
     # via -r requirements.in
-diveplane-openapi-client==22.2.4
+diveplane-openapi-client==23.0.5
     # via -r requirements.in
-faker==18.10.1
+faker==18.11.2
     # via -r requirements.in
-humanize==4.6.0
+humanize==4.7.0
     # via -r requirements.in
-joblib==1.2.0
+joblib==1.3.1
     # via scikit-learn
-markdown-it-py==2.2.0
+markdown-it-py==3.0.0
     # via rich
 mdurl==0.1.2
     # via markdown-it-py
 mmh3==4.0.0
     # via -r requirements.in
-numpy==1.24.3
+numpy==1.25.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
     #   pandas
     #   scikit-learn
     #   scipy
 packaging==23.1
     # via deprecation
-pandas==2.0.2
+pandas==2.0.3
     # via -r requirements.in
 pycparser==2.21
     # via cffi
 pygments==2.15.1
     # via rich
 pyjwt==2.7.0
     # via -r requirements.in
@@ -51,35 +51,35 @@
     #   diveplane-openapi-client
     #   faker
     #   pandas
 pytz==2023.3
     # via pandas
 pyyaml==6.0
     # via -r requirements.in
-rich==13.4.1
+rich==13.4.2
     # via -r requirements.in
-scikit-learn==1.2.2
+scikit-learn==1.3.0
     # via -r requirements.in
-scipy==1.10.1
+scipy==1.11.1
     # via scikit-learn
 semantic-version==2.10.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
 six==1.16.0
     # via
     #   diveplane-openapi-client
     #   python-dateutil
 threadpoolctl==3.1.0
     # via scikit-learn
-typing-extensions==4.6.3
+typing-extensions==4.7.1
     # via -r requirements.in
 tzdata==2023.3
     # via pandas
 urllib3==1.26.16
     # via
     #   -r requirements.in
     #   diveplane-openapi-client
 
 # The following packages are considered to be unsafe in a requirements file:
-setuptools==67.8.0
+setuptools==68.0.0
     # via diveplane-openapi-client
```

## diveplane/client/requirements-3.8.txt

```diff
@@ -9,40 +9,40 @@
     # via -r requirements.in
 cffi==1.15.1
     # via cryptography
 cryptography==41.0.1
     # via -r requirements.in
 deprecation==2.1.0
     # via -r requirements.in
-diveplane-amalgam-api==2.3.7
+diveplane-amalgam-api==2.3.9
     # via -r requirements.in
-diveplane-openapi-client==22.2.4
+diveplane-openapi-client==23.0.5
     # via -r requirements.in
-faker==18.10.1
+faker==18.11.2
     # via -r requirements.in
-humanize==4.6.0
+humanize==4.7.0
     # via -r requirements.in
-joblib==1.2.0
+joblib==1.3.1
     # via scikit-learn
-markdown-it-py==2.2.0
+markdown-it-py==3.0.0
     # via rich
 mdurl==0.1.2
     # via markdown-it-py
 mmh3==4.0.0
     # via -r requirements.in
-numpy==1.24.3
+numpy==1.24.4
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
     #   pandas
     #   scikit-learn
     #   scipy
 packaging==23.1
     # via deprecation
-pandas==2.0.2
+pandas==2.0.3
     # via -r requirements.in
 pycparser==2.21
     # via cffi
 pygments==2.15.1
     # via rich
 pyjwt==2.7.0
     # via -r requirements.in
@@ -51,37 +51,37 @@
     #   diveplane-openapi-client
     #   faker
     #   pandas
 pytz==2023.3
     # via pandas
 pyyaml==6.0
     # via -r requirements.in
-rich==13.4.1
+rich==13.4.2
     # via -r requirements.in
-scikit-learn==1.2.2
+scikit-learn==1.3.0
     # via -r requirements.in
 scipy==1.10.1
     # via scikit-learn
 semantic-version==2.10.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
 six==1.16.0
     # via
     #   diveplane-openapi-client
     #   python-dateutil
 threadpoolctl==3.1.0
     # via scikit-learn
-typing-extensions==4.6.3
+typing-extensions==4.7.1
     # via
     #   -r requirements.in
     #   rich
 tzdata==2023.3
     # via pandas
 urllib3==1.26.16
     # via
     #   -r requirements.in
     #   diveplane-openapi-client
 
 # The following packages are considered to be unsafe in a requirements file:
-setuptools==67.8.0
+setuptools==68.0.0
     # via diveplane-openapi-client
```

## diveplane/client/requirements-3.9.txt

```diff
@@ -9,40 +9,40 @@
     # via -r requirements.in
 cffi==1.15.1
     # via cryptography
 cryptography==41.0.1
     # via -r requirements.in
 deprecation==2.1.0
     # via -r requirements.in
-diveplane-amalgam-api==2.3.7
+diveplane-amalgam-api==2.3.9
     # via -r requirements.in
-diveplane-openapi-client==22.2.4
+diveplane-openapi-client==23.0.5
     # via -r requirements.in
-faker==18.10.1
+faker==18.11.2
     # via -r requirements.in
-humanize==4.6.0
+humanize==4.7.0
     # via -r requirements.in
-joblib==1.2.0
+joblib==1.3.1
     # via scikit-learn
-markdown-it-py==2.2.0
+markdown-it-py==3.0.0
     # via rich
 mdurl==0.1.2
     # via markdown-it-py
 mmh3==4.0.0
     # via -r requirements.in
-numpy==1.24.3
+numpy==1.25.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
     #   pandas
     #   scikit-learn
     #   scipy
 packaging==23.1
     # via deprecation
-pandas==2.0.2
+pandas==2.0.3
     # via -r requirements.in
 pycparser==2.21
     # via cffi
 pygments==2.15.1
     # via rich
 pyjwt==2.7.0
     # via -r requirements.in
@@ -51,35 +51,35 @@
     #   diveplane-openapi-client
     #   faker
     #   pandas
 pytz==2023.3
     # via pandas
 pyyaml==6.0
     # via -r requirements.in
-rich==13.4.1
+rich==13.4.2
     # via -r requirements.in
-scikit-learn==1.2.2
+scikit-learn==1.3.0
     # via -r requirements.in
-scipy==1.10.1
+scipy==1.11.1
     # via scikit-learn
 semantic-version==2.10.0
     # via
     #   -r requirements.in
     #   diveplane-amalgam-api
 six==1.16.0
     # via
     #   diveplane-openapi-client
     #   python-dateutil
 threadpoolctl==3.1.0
     # via scikit-learn
-typing-extensions==4.6.3
+typing-extensions==4.7.1
     # via -r requirements.in
 tzdata==2023.3
     # via pandas
 urllib3==1.26.16
     # via
     #   -r requirements.in
     #   diveplane-openapi-client
 
 # The following packages are considered to be unsafe in a requirements file:
-setuptools==67.8.0
+setuptools==68.0.0
     # via diveplane-openapi-client
```

## diveplane/client/requirements-dev-3.10.txt

```diff
@@ -21,17 +21,17 @@
     # via pytest-cov
 dill==0.3.6
     # via
     #   -r requirements-dev-3.10.in
     #   pylint
 diveplane-build-artifacts==1.0.10
     # via -r requirements-dev-3.10.in
-dohq-artifactory==0.8.4
+dohq-artifactory==0.9.0
     # via diveplane-build-artifacts
-exceptiongroup==1.1.1
+exceptiongroup==1.1.2
     # via pytest
 flake8==3.9.2
     # via
     #   -r requirements-dev-3.10.in
     #   flake8-docstrings
 flake8-docstrings==1.7.0
     # via -r requirements-dev-3.10.in
@@ -45,34 +45,34 @@
     # via pylint
 lazy-object-proxy==1.9.0
     # via astroid
 mccabe==0.6.1
     # via
     #   flake8
     #   pylint
-numpy==1.24.3
+numpy==1.25.0
     # via
     #   -c requirements-3.10.txt
     #   pandas
 packaging==23.1
     # via
     #   -c requirements-3.10.txt
     #   build
     #   pytest
-pandas==2.0.2
+pandas==2.0.3
     # via
     #   -c requirements-3.10.txt
     #   pmlb
-pip-tools==6.13.0
+pip-tools==6.14.0
     # via -r requirements-dev-3.10.in
-pipdeptree==2.9.0
+pipdeptree==2.9.3
     # via -r requirements-dev-3.10.in
-platformdirs==3.5.1
+platformdirs==3.8.0
     # via pylint
-pluggy==1.0.0
+pluggy==1.2.0
     # via pytest
 pmlb==1.0.1.post3
     # via -r requirements-dev-3.10.in
 pycodestyle==2.7.0
     # via
     #   -r requirements-dev-3.10.in
     #   flake8
@@ -85,15 +85,15 @@
     # via
     #   -c requirements-3.10.txt
     #   dohq-artifactory
 pylint==2.17.4
     # via -r requirements-dev-3.10.in
 pyproject-hooks==1.0.0
     # via build
-pytest==7.3.1
+pytest==7.4.0
     # via
     #   -r requirements-dev-3.10.in
     #   pytest-cov
     #   pytest-mock
 pytest-cov==4.1.0
     # via -r requirements-dev-3.10.in
 pytest-mock==1.13.0
@@ -126,20 +126,21 @@
     #   python-dateutil
 snowballstemmer==2.2.0
     # via pydocstyle
 tomli==2.0.1
     # via
     #   build
     #   coverage
+    #   pip-tools
     #   pylint
     #   pyproject-hooks
     #   pytest
 tomlkit==0.11.8
     # via pylint
-typing-extensions==4.6.3
+typing-extensions==4.7.1
     # via
     #   -c requirements-3.10.txt
     #   astroid
 tzdata==2023.3
     # via
     #   -c requirements-3.10.txt
     #   pandas
```

## diveplane/client/requirements-dev-3.11.txt

```diff
@@ -21,15 +21,15 @@
     # via pytest-cov
 dill==0.3.6
     # via
     #   -r requirements-dev-3.11.in
     #   pylint
 diveplane-build-artifacts==1.0.10
     # via -r requirements-dev-3.11.in
-dohq-artifactory==0.8.4
+dohq-artifactory==0.9.0
     # via diveplane-build-artifacts
 flake8==3.9.2
     # via
     #   -r requirements-dev-3.11.in
     #   flake8-docstrings
 flake8-docstrings==1.7.0
     # via -r requirements-dev-3.11.in
@@ -43,34 +43,34 @@
     # via pylint
 lazy-object-proxy==1.9.0
     # via astroid
 mccabe==0.6.1
     # via
     #   flake8
     #   pylint
-numpy==1.24.3
+numpy==1.25.0
     # via
     #   -c requirements-3.11.txt
     #   pandas
 packaging==23.1
     # via
     #   -c requirements-3.11.txt
     #   build
     #   pytest
-pandas==2.0.2
+pandas==2.0.3
     # via
     #   -c requirements-3.11.txt
     #   pmlb
-pip-tools==6.13.0
+pip-tools==6.14.0
     # via -r requirements-dev-3.11.in
-pipdeptree==2.9.0
+pipdeptree==2.9.3
     # via -r requirements-dev-3.11.in
-platformdirs==3.5.1
+platformdirs==3.8.0
     # via pylint
-pluggy==1.0.0
+pluggy==1.2.0
     # via pytest
 pmlb==1.0.1.post3
     # via -r requirements-dev-3.11.in
 pycodestyle==2.7.0
     # via
     #   -r requirements-dev-3.11.in
     #   flake8
@@ -83,15 +83,15 @@
     # via
     #   -c requirements-3.11.txt
     #   dohq-artifactory
 pylint==2.17.4
     # via -r requirements-dev-3.11.in
 pyproject-hooks==1.0.0
     # via build
-pytest==7.3.1
+pytest==7.4.0
     # via
     #   -r requirements-dev-3.11.in
     #   pytest-cov
     #   pytest-mock
 pytest-cov==4.1.0
     # via -r requirements-dev-3.11.in
 pytest-mock==1.13.0
```

## diveplane/client/requirements-dev-3.8.txt

```diff
@@ -21,17 +21,17 @@
     # via pytest-cov
 dill==0.3.6
     # via
     #   -r requirements-dev-3.8.in
     #   pylint
 diveplane-build-artifacts==1.0.10
     # via -r requirements-dev-3.8.in
-dohq-artifactory==0.8.4
+dohq-artifactory==0.9.0
     # via diveplane-build-artifacts
-exceptiongroup==1.1.1
+exceptiongroup==1.1.2
     # via pytest
 flake8==3.9.2
     # via
     #   -r requirements-dev-3.8.in
     #   flake8-docstrings
 flake8-docstrings==1.7.0
     # via -r requirements-dev-3.8.in
@@ -45,34 +45,34 @@
     # via pylint
 lazy-object-proxy==1.9.0
     # via astroid
 mccabe==0.6.1
     # via
     #   flake8
     #   pylint
-numpy==1.24.3
+numpy==1.24.4
     # via
     #   -c requirements-3.8.txt
     #   pandas
 packaging==23.1
     # via
     #   -c requirements-3.8.txt
     #   build
     #   pytest
-pandas==2.0.2
+pandas==2.0.3
     # via
     #   -c requirements-3.8.txt
     #   pmlb
-pip-tools==6.13.0
+pip-tools==6.14.0
     # via -r requirements-dev-3.8.in
-pipdeptree==2.9.0
+pipdeptree==2.9.3
     # via -r requirements-dev-3.8.in
-platformdirs==3.5.1
+platformdirs==3.8.0
     # via pylint
-pluggy==1.0.0
+pluggy==1.2.0
     # via pytest
 pmlb==1.0.1.post3
     # via -r requirements-dev-3.8.in
 pycodestyle==2.7.0
     # via
     #   -r requirements-dev-3.8.in
     #   flake8
@@ -85,15 +85,15 @@
     # via
     #   -c requirements-3.8.txt
     #   dohq-artifactory
 pylint==2.17.4
     # via -r requirements-dev-3.8.in
 pyproject-hooks==1.0.0
     # via build
-pytest==7.3.1
+pytest==7.4.0
     # via
     #   -r requirements-dev-3.8.in
     #   pytest-cov
     #   pytest-mock
 pytest-cov==4.1.0
     # via -r requirements-dev-3.8.in
 pytest-mock==1.13.0
@@ -126,20 +126,21 @@
     #   python-dateutil
 snowballstemmer==2.2.0
     # via pydocstyle
 tomli==2.0.1
     # via
     #   build
     #   coverage
+    #   pip-tools
     #   pylint
     #   pyproject-hooks
     #   pytest
 tomlkit==0.11.8
     # via pylint
-typing-extensions==4.6.3
+typing-extensions==4.7.1
     # via
     #   -c requirements-3.8.txt
     #   astroid
     #   pylint
 tzdata==2023.3
     # via
     #   -c requirements-3.8.txt
```

## diveplane/client/requirements-dev-3.9.txt

```diff
@@ -21,17 +21,17 @@
     # via pytest-cov
 dill==0.3.6
     # via
     #   -r requirements-dev-3.9.in
     #   pylint
 diveplane-build-artifacts==1.0.10
     # via -r requirements-dev-3.9.in
-dohq-artifactory==0.8.4
+dohq-artifactory==0.9.0
     # via diveplane-build-artifacts
-exceptiongroup==1.1.1
+exceptiongroup==1.1.2
     # via pytest
 flake8==3.9.2
     # via
     #   -r requirements-dev-3.9.in
     #   flake8-docstrings
 flake8-docstrings==1.7.0
     # via -r requirements-dev-3.9.in
@@ -45,34 +45,34 @@
     # via pylint
 lazy-object-proxy==1.9.0
     # via astroid
 mccabe==0.6.1
     # via
     #   flake8
     #   pylint
-numpy==1.24.3
+numpy==1.25.0
     # via
     #   -c requirements-3.9.txt
     #   pandas
 packaging==23.1
     # via
     #   -c requirements-3.9.txt
     #   build
     #   pytest
-pandas==2.0.2
+pandas==2.0.3
     # via
     #   -c requirements-3.9.txt
     #   pmlb
-pip-tools==6.13.0
+pip-tools==6.14.0
     # via -r requirements-dev-3.9.in
-pipdeptree==2.9.0
+pipdeptree==2.9.3
     # via -r requirements-dev-3.9.in
-platformdirs==3.5.1
+platformdirs==3.8.0
     # via pylint
-pluggy==1.0.0
+pluggy==1.2.0
     # via pytest
 pmlb==1.0.1.post3
     # via -r requirements-dev-3.9.in
 pycodestyle==2.7.0
     # via
     #   -r requirements-dev-3.9.in
     #   flake8
@@ -85,15 +85,15 @@
     # via
     #   -c requirements-3.9.txt
     #   dohq-artifactory
 pylint==2.17.4
     # via -r requirements-dev-3.9.in
 pyproject-hooks==1.0.0
     # via build
-pytest==7.3.1
+pytest==7.4.0
     # via
     #   -r requirements-dev-3.9.in
     #   pytest-cov
     #   pytest-mock
 pytest-cov==4.1.0
     # via -r requirements-dev-3.9.in
 pytest-mock==1.13.0
@@ -126,20 +126,21 @@
     #   python-dateutil
 snowballstemmer==2.2.0
     # via pydocstyle
 tomli==2.0.1
     # via
     #   build
     #   coverage
+    #   pip-tools
     #   pylint
     #   pyproject-hooks
     #   pytest
 tomlkit==0.11.8
     # via pylint
-typing-extensions==4.6.3
+typing-extensions==4.7.1
     # via
     #   -c requirements-3.9.txt
     #   astroid
     #   pylint
 tzdata==2023.3
     # via
     #   -c requirements-3.9.txt
```

## diveplane/client/requirements.in

```diff
@@ -9,9 +9,9 @@
 pyjwt>=2.6.0,<3
 pyyaml>=5.4.1,<7.0
 rich>=12.5.1
 scikit-learn
 semantic-version>=2.8.5,<3
 typing-extensions>=4.1.0,<5.0
 urllib3>=1.26.2,<=2
-diveplane-openapi-client==22.2.5 # Added by automation - please leave
-diveplane-amalgam-api==2.3.7 # Added by automation - please leave
+diveplane-openapi-client==23.0.6 # Added by automation - please leave
+diveplane-amalgam-api==2.3.9 # Added by automation - please leave
```

## diveplane/client/tests/test_infer_feature_attributes.py

```diff
@@ -30,30 +30,14 @@
         assert feature_attribs['datetime']['type'] == 'continuous'
         assert feature_attribs['datetime']['date_time_format'] == '%Y-%m-%dT%H:%M:%S'
 
         # values pre-epoch work and bounds are set to values in the dataset
         assert feature_attribs['datetime']['bounds']['min'] == '1920-10-11T11:11:11'
         assert feature_attribs['datetime']['bounds']['max'] == '2020-12-12T12:12:12'
 
-    def test_datetime_warn(self):
-        """Test that infer_feature_attributes infers dates correctly."""
-        df = pd.DataFrame(data=np.asarray([
-            ['a', 'b', 'c'],
-            ['20-10-12', '20-12-12', '20-10-11']
-        ]).transpose(), columns=['nom', 'datetime'])
-
-        with pytest.warns() as warn:
-            feature_attribs = infer_feature_attributes(df)
-            assert str(warn[0].message) == (
-                "Feature datetime detected as having datetime values, but are "
-                "not in an ISO 8601 format, such as '%Y-%m-%dT%H:%M:%S', for "
-                "example: '2020-10-02T12:43:39'")
-            assert feature_attribs['datetime']['type'] == 'continuous'
-            assert feature_attribs['datetime']['date_time_format'] == '%Y-%m-%dT%H:%M:%S'
-
     def test_not_datetime_column(self):
         """Test that infer_feature_attributes infers dates correctly."""
         df = pd.DataFrame(data=np.asarray([
             ["1.2", "3.4", "5.0"],
             ['2020-10-12T10:10:10', 'okay not really a date', 'that first value is a fluke']
         ]).transpose(), columns=['nom', 'not_actually_datetime'])
         feature_attribs = infer_feature_attributes(df)
```

## diveplane/direct/client.py

```diff
@@ -39,14 +39,15 @@
     num_list_dimensions,
     ProgressTimer,
     replace_doublemax_with_infinity,
     serialize_cases,
     validate_case_indices,
     validate_list_shape,
 )
+from diveplane.utilities.feature_attributes.base import SingleTableFeatureAttributes
 
 from ._utilities import license_check, model_from_dict
 from .core import DiveplaneCore
 
 # Configure diveplane base logger
 _logger = logging.getLogger('diveplane.direct')
 
@@ -1111,14 +1112,20 @@
             When true, and accumulate_weight_feature is provided,
             will accumulate all of the cases' neighbor weights instead of
             training the cases into the model.
         """
         self._auto_resolve_trainee(trainee_id)
         feature_attributes = self.trainee_cache.get(trainee_id).features
 
+        # Check to see if the feature attributes still generally describe
+        # the data, and warn the user if they do not
+        if isinstance(cases, DataFrame):
+            attrs_obj = SingleTableFeatureAttributes(feature_attributes, {})
+            attrs_obj.validate(cases)
+
         validate_list_shape(features, 1, "features", "str")
         if self.verbose:
             print(f'Training session(s) on trainee with id: {trainee_id}')
 
         validate_list_shape(cases, 2, "cases", "list", allow_none=False)
         if features is None:
             features = internals.get_features_from_data(cases)
@@ -3030,15 +3037,14 @@
         *,
         features: Iterable[str] = None,
         familiarity_conviction_addition: bool = False,
         familiarity_conviction_removal: bool = False,
         p_value_of_addition: bool = False,
         p_value_of_removal: bool = False,
         distance_contribution: bool = False,
-        high_distance_accuracy: bool = True,
         weight_feature: Optional[str] = None,
         use_case_weights: bool = False
     ):
         """
         Calculate and cache conviction and other statistics.
 
         Parameters
@@ -3063,17 +3069,14 @@
             The name of the feature to store p value of removal
             values. If set to True the values will be stored to the feature
             'p_value_of_removal'.
         distance_contribution : bool or str, default False
             The name of the feature to store distance contribution.
             If set to True the values will be stored to the
             feature 'distance_contribution'.
-        high_distance_accuracy : bool, default True
-             When true, uses high accuracy when computing distances.
-            Otherwise uses low accuracy.
         weight_feature : str, optional
             Name of feature whose values to use as case weights.
             When left unspecified uses the internally managed case weight.
         use_case_weights : bool, default False
             If set to True will scale influence weights by each
             case's weight_feature weight.
         """
@@ -3085,15 +3088,14 @@
             trainee_id,
             features=features,
             familiarity_conviction_addition=familiarity_conviction_addition,
             familiarity_conviction_removal=familiarity_conviction_removal,
             p_value_of_addition=p_value_of_addition,
             p_value_of_removal=p_value_of_removal,
             distance_contribution=distance_contribution,
-            high_distance_accuracy=high_distance_accuracy,
             weight_feature=weight_feature,
             use_case_weights=use_case_weights)
         self._auto_persist_trainee(trainee_id)
 
     def get_cases(
         self,
         trainee_id: str,
@@ -3230,15 +3232,14 @@
         distance_contributions: bool = False,
         familiarity_conviction_addition: bool = True,
         familiarity_conviction_removal: bool = False,
         kl_divergence_addition: bool = False,
         kl_divergence_removal: bool = False,
         p_value_of_addition: bool = False,
         p_value_of_removal: bool = False,
-        high_distance_accuracy: bool = True,
         weight_feature: Optional[str] = None,
         use_case_weights: bool = False
     ) -> client_models.ReactGroupResponse:
         """
         Computes specified data for a **set** of cases.
 
         Return the list of familiarity convictions (and optionally, distance
@@ -3276,17 +3277,14 @@
         kl_divergence_removal : bool, default False
             Calculate and output KL divergence of removing the
             specified cases.
         p_value_of_addition : bool, default False
             If true will output p value of addition.
         p_value_of_removal : bool, default False
             If true will output p value of removal.
-        high_distance_accuracy : bool, default True
-            When true, uses high accuracy when computing distances.
-            Otherwise uses low accuracy.
         weight_feature : str, optional
             Name of feature whose values to use as case weights.
             When left unspecified uses the internally managed case weight.
         use_case_weights : bool, default False
             If set to True will scale influence weights by each
             case's weight_feature weight.
 
@@ -3336,30 +3334,28 @@
             familiarity_conviction_addition=familiarity_conviction_addition,
             familiarity_conviction_removal=familiarity_conviction_removal,
             kl_divergence_addition=kl_divergence_addition,
             kl_divergence_removal=kl_divergence_removal,
             p_value_of_addition=p_value_of_addition,
             p_value_of_removal=p_value_of_removal,
             distance_contributions=distance_contributions,
-            high_distance_accuracy=high_distance_accuracy,
             weight_feature=weight_feature,
             use_case_weights=use_case_weights
         )
 
         return ret
 
     def get_feature_conviction(
         self,
         trainee_id,
         *,
         features: Optional[Iterable[str]] = None,
         action_features: Optional[Iterable[str]] = None,
         familiarity_conviction_addition: bool = True,
         familiarity_conviction_removal: bool = False,
-        high_distance_accuracy: bool = True,
         weight_feature: Optional[str] = None,
         use_case_weights: bool = False
     ) -> client_models.FeatureConviction:
         """
         Get familiarity conviction for features in the model.
 
         Parameters
@@ -3375,17 +3371,14 @@
             conviction is computed for each feature against the rest of the features as a whole.
         familiarity_conviction_addition : bool, default True
             Calculate and output familiarity conviction of adding the
             specified features in the output.
         familiarity_conviction_removal : bool, default False
             Calculate and output familiarity conviction of removing
             the specified features in the output.
-        high_distance_accuracy : bool, default True
-            When true, uses high accuracy when computing distances.
-            Otherwise uses low accuracy.
         weight_feature : str, optional
             Name of feature whose values to use as case weights.
             When left unspecified uses the internally managed case weight.
         use_case_weights : bool, default False
             If set to True will scale influence weights by each
             case's weight_feature weight.
 
@@ -3403,15 +3396,14 @@
                   f'{trainee_id}')
         return self.dp.compute_conviction_of_features(
             trainee_id,
             features=features,
             action_features=action_features,
             familiarity_conviction_addition=familiarity_conviction_addition,
             familiarity_conviction_removal=familiarity_conviction_removal,
-            high_distance_accuracy=high_distance_accuracy,
             weight_feature=weight_feature,
             use_case_weights=use_case_weights
         )
 
     def add_feature(
         self,
         trainee_id: str,
```

## diveplane/direct/core.py

```diff
@@ -891,27 +891,25 @@
 
     def react_into_features(self, trainee, features=None,
                             familiarity_conviction_addition=False,
                             familiarity_conviction_removal=False,
                             p_value_of_addition=False,
                             p_value_of_removal=False,
                             distance_contribution=False,
-                            high_distance_accuracy=True,
                             weight_feature=None,
                             use_case_weights=False):
         return self._execute("react_into_features",
                              {
                                  "trainee": trainee,
                                  "features": features,
                                  "familiarity_conviction_addition": familiarity_conviction_addition,
                                  "familiarity_conviction_removal": familiarity_conviction_removal,
                                  "p_value_of_addition": p_value_of_addition,
                                  "p_value_of_removal": p_value_of_removal,
                                  "distance_contribution": distance_contribution,
-                                 "high_distance_accuracy": high_distance_accuracy,
                                  "weight_feature": weight_feature,
                                  "use_case_weights": use_case_weights
                              })
 
     def batch_react_group(self, trainee, *,
                           new_cases=None,
                           features=None,
@@ -919,15 +917,14 @@
                           distance_contributions=False,
                           familiarity_conviction_addition=True,
                           familiarity_conviction_removal=False,
                           kl_divergence_addition=False,
                           kl_divergence_removal=False,
                           p_value_of_addition=False,
                           p_value_of_removal=False,
-                          high_distance_accuracy=True,
                           weight_feature=None,
                           use_case_weights=False):
         return self._execute("batch_react_group",
                              {
                                  "trainee": trainee,
                                  "features": features,
                                  "new_cases": new_cases,
@@ -935,35 +932,32 @@
                                  "distance_contributions": distance_contributions,
                                  "familiarity_conviction_addition": familiarity_conviction_addition,
                                  "familiarity_conviction_removal": familiarity_conviction_removal,
                                  "kl_divergence_addition": kl_divergence_addition,
                                  "kl_divergence_removal": kl_divergence_removal,
                                  "p_value_of_addition": p_value_of_addition,
                                  "p_value_of_removal": p_value_of_removal,
-                                 "high_distance_accuracy": high_distance_accuracy,
                                  "weight_feature": weight_feature,
                                  "use_case_weights": use_case_weights
                              })
 
     def compute_conviction_of_features(self, trainee, *,
                                        features=None,
                                        action_features=None,
                                        familiarity_conviction_addition=True,
                                        familiarity_conviction_removal=False,
-                                       high_distance_accuracy=True,
                                        weight_feature=None,
                                        use_case_weights=False):
         return self._execute("compute_conviction_of_features",
                              {
                                  "trainee": trainee,
                                  "features": features,
                                  "action_features": action_features,
                                  "familiarity_conviction_addition": familiarity_conviction_addition,
                                  "familiarity_conviction_removal": familiarity_conviction_removal,
-                                 "high_distance_accuracy": high_distance_accuracy,
                                  "weight_feature": weight_feature,
                                  "use_case_weights": use_case_weights
                              })
 
     def simplify_model(self, trainee, num_cases_to_remove, distribute_weight_feature):
         return self._execute("simplify_model",
                              {
```

## diveplane/reactor/trainee.py

```diff
@@ -2078,15 +2078,14 @@
     def react_group(
         self,
         *,
         distance_contributions: Optional[bool] = False,
         familiarity_conviction_addition: Optional[bool] = True,
         familiarity_conviction_removal: Optional[bool] = False,
         features: Optional[Iterable[str]] = None,
-        high_distance_accuracy: Optional[bool] = True,
         new_cases: Optional[
             Union[List["DataFrame"], List[List[List[object]]]]] = None,
         kl_divergence_addition: Optional[bool] = False,
         kl_divergence_removal: Optional[bool] = False,
         p_value_of_addition: Optional[bool] = False,
         p_value_of_removal: Optional[bool] = False,
         trainees_to_compare: Optional[Iterable[Union["Trainee", str]]] = None,
@@ -2108,17 +2107,14 @@
             (Optional) Calculate and output familiarity conviction of adding the
             specified cases.
         familiarity_conviction_removal : bool, default False
             (Optional) Calculate and output familiarity conviction of removing
             the specified cases.
         features : list of str or None, optional
             A list of feature names to consider while calculating convictions.
-        high_distance_accuracy : bool, default True
-            (Optional) When true, uses high accuracy when computing distances.
-            Otherwise uses low accuracy.
         kl_divergence_addition : bool, default False
             (Optional) Calculate and output KL divergence of adding the
             specified cases.
         kl_divergence_removal : bool, default False
             (Optional) Calculate and output KL divergence of removing the
             specified cases.
         new_cases : list of list of list of object or list of pandas.DataFrame, optional
@@ -2168,27 +2164,25 @@
             familiarity_conviction_addition=familiarity_conviction_addition,
             familiarity_conviction_removal=familiarity_conviction_removal,
             kl_divergence_addition=kl_divergence_addition,
             kl_divergence_removal=kl_divergence_removal,
             p_value_of_addition=p_value_of_addition,
             p_value_of_removal=p_value_of_removal,
             distance_contributions=distance_contributions,
-            high_distance_accuracy=high_distance_accuracy,
             use_case_weights=use_case_weights,
             weight_feature=weight_feature,
         )
 
     def get_feature_conviction(
         self,
         *,
         action_features: Optional[Iterable[str]] = None,
         familiarity_conviction_addition: Optional[bool] = True,
         familiarity_conviction_removal: Optional[bool] = False,
         features: Optional[Iterable[str]] = None,
-        high_distance_accuracy: Optional[bool] = True,
         use_case_weights: Optional[bool] = False,
         weight_feature: Optional[str] = None,
     ) -> "DataFrame":
         """
         Get familiarity conviction for features in the model.
 
         Parameters
@@ -2205,17 +2199,14 @@
         familiarity_conviction_removal : bool, default False
             (Optional) Calculate and output familiarity conviction of removing
             the specified cases.
         features : list of str, optional
             The feature names to calculate convictions for. At least 2 features
             are required to get familiarity conviction. If not specified all
             features will be used.
-        high_distance_accuracy : bool, default True
-            (Optional) When true, uses high accuracy when computing distances.
-            Otherwise uses low accuracy.
         use_case_weights : bool, default False
             When True, will scale influence weights by each case's
             `weight_feature` weight.
         weight_feature : str, optional
             Name of feature whose values to use as case weights.
             When left unspecified uses the internally managed case weight.
 
@@ -2227,15 +2218,14 @@
         """
         return self.client.get_feature_conviction(
             trainee_id=self.id,
             action_features=action_features,
             familiarity_conviction_addition=familiarity_conviction_addition,
             familiarity_conviction_removal=familiarity_conviction_removal,
             features=features,
-            high_distance_accuracy=high_distance_accuracy,
             use_case_weights=use_case_weights,
             weight_feature=weight_feature,
         )
 
     def get_feature_residuals(
         self,
         *,
@@ -2383,15 +2373,14 @@
     def react_into_features(
         self,
         *,
         distance_contribution: Optional[Union[str, bool]] = False,
         familiarity_conviction_addition: Optional[Union[str, bool]] = False,
         familiarity_conviction_removal: Optional[Union[str, bool]] = False,
         features: Optional[Iterable[str]] = None,
-        high_distance_accuracy: Optional[bool] = True,
         p_value_of_addition: Optional[Union[str, bool]] = False,
         p_value_of_removal: Optional[Union[str, bool]] = False,
         use_case_weights: Optional[bool] = False,
         weight_feature: Optional[str] = None,
     ) -> None:
         """
         Calculate conviction and other data and stores them into features.
@@ -2408,17 +2397,14 @@
             'familiarity_conviction_addition'.
         familiarity_conviction_removal : bool or str, default False
             (Optional) The name of the feature to store conviction of removal
             values. If set to True the values will be stored to the feature
             'familiarity_conviction_removal'.
         features : list of str, optional
             A list of features to calculate convictions.
-        high_distance_accuracy : bool, default True
-            (Optional) When true, uses high accuracy when computing distances.
-            Otherwise uses low accuracy.
         p_value_of_addition : bool or str, default False
             (Optional) The name of the feature to store p value of addition
             values. If set to True the values will be stored to the feature
             'p_value_of_addition'.
         p_value_of_removal : bool or str, default False
             (Optional) The name of the feature to store p value of removal
             values. If set to True the values will be stored to the feature
@@ -2437,15 +2423,14 @@
         self.client.react_into_features(
             trainee_id=self.id,
             distance_contribution=distance_contribution,
             familiarity_conviction_addition=familiarity_conviction_addition,
             familiarity_conviction_removal=familiarity_conviction_removal,
             p_value_of_addition=p_value_of_addition,
             p_value_of_removal=p_value_of_removal,
-            high_distance_accuracy=high_distance_accuracy,
             features=features,
             use_case_weights=use_case_weights,
             weight_feature=weight_feature,
         )
 
     def react_into_trainee(
         self,
```

## diveplane/scikit/scikit.py

```diff
@@ -501,15 +501,14 @@
     def react_into_features(
         self,
         features=None,
         *,
         distance_contribution=False,
         familiarity_conviction_addition=False,
         familiarity_conviction_removal=False,
-        high_distance_accuracy=True,
         p_value_of_addition=False,
         p_value_of_removal=False,
         use_case_weights=False,
         weight_feature=None,
     ):
         """
         Calculate conviction and other data and stores them into features.
@@ -526,17 +525,14 @@
             The name of the feature to store conviction of addition values. If
             set to True the values will be stored to the feature
             'familiarity_conviction_addition'.
         familiarity_conviction_removal : bool or str, default False
             The name of the feature to store conviction of removal values. If
             set to True the values will be stored to the feature
             'familiarity_conviction_removal'.
-        high_distance_accuracy : bool, default True
-            When true, uses high accuracy when computing distances. Otherwise
-            uses low accuracy.
         p_value_of_addition : bool or str, default False
             The name of the feature to store p value of addition values. If set
             to True the values will be stored to the feature
             'p_value_of_addition'.
         p_value_of_removal : bool or str, default False
             The name of the feature to store p value of removal values. If set
             to True the values will be stored to the feature
@@ -554,15 +550,14 @@
 
         # Call conviction store on the trainee.
         self.trainee.react_into_features(
             features=features,
             distance_contribution=distance_contribution,
             familiarity_conviction_addition=familiarity_conviction_addition,
             familiarity_conviction_removal=familiarity_conviction_removal,
-            high_distance_accuracy=high_distance_accuracy,
             p_value_of_addition=p_value_of_addition,
             p_value_of_removal=p_value_of_removal,
             use_case_weights=use_case_weights,
             weight_feature=weight_feature,
         )
 
     def describe_prediction(self, X, details=None) -> Dict:
```

## diveplane/utilities/__init__.py

```diff
@@ -16,15 +16,14 @@
     check_feature_names,
     date_format_is_iso,
     date_to_epoch,
     determine_iso_format,
     dprint,
     epoch_to_date,
     get_kwargs,
-    is_datetime,
     is_valid_uuid,
     ISO_8601_DATE_FORMAT,
     ISO_8601_FORMAT,
     LocaleOverride,
     num_list_dimensions,
     replace_doublemax_with_infinity,
     replace_nan_with_none,
```

## diveplane/utilities/eula_helper.py

```diff
@@ -1,35 +1,38 @@
 import os
-from pathlib import Path
 
-from diveplane.direct import DiveplaneDirectClient
 from diveplane.client.client import get_diveplane_client_class
+from diveplane.direct import DiveplaneDirectClient
 from diveplane.direct._utilities import (
     DIVEPLANE_EULA_ENV_VAR,
     DiveplaneLicenseAcceptanceException,
     get_eula_acceptance_file_path,
     license_check,
     LicenseType,
 )
 from rich import print
+from rich.console import Console
 
 DIVEPLANE_EULA_FILE = "LICENSE.TXT"
 
 
 def eula_not_accepted():
     """Walk end-user through accepting (or not) the license."""
     # ATTN: This is formatted for an 80 column terminal. Please do not make
     # whitespace (or other) changes without careful testing!
     eula_file = get_eula_acceptance_file_path()
 
-    response = input(r'''
+    console = Console()
+
+    response = console.input(r'''
 In order to continue, this agreement must be accepted. If you have read and
-accept the terms and conditions presented in this document, enter "I ACCEPT".
+accept the terms and conditions presented in this document, enter [green]"I ACCEPT"[/green]
+or [green]"TRUE"[/green].
 > ''')
-    if response.lower() == "i accept":
+    if response.lower() == "i accept" or response.lower() == "true":
         try:
             eula_file.parent.mkdir(parents=True, exist_ok=True)
             eula_file.touch(exist_ok=True)
         except Exception:  # noqa: Deliberately broad
             """Something went wrong, advise the alternative ENV VAR method."""
             print(rf'''
 This utilitiy unsuccessfully tried to create a file at "{eula_file}"
@@ -43,14 +46,21 @@
         else:
             print(rf'''
 This utility successfully created a file at:
 {eula_file}
 which indicates that the Diveplane Corporation Free Software License Terms was
 read and accepted. The software should now run without interruption.
 ''')
+    else:
+        print(rf'''
+[red][bold]ERROR:[/red][/bold] You did not accept the Diveplane Corporation Free Software
+License Terms. Please re-run this tool or set the environment variable
+"{DIVEPLANE_EULA_ENV_VAR}" to "True" before running the Diveplane software.
+''')
+        exit(1)
 
 
 def eula_already_accepted():
     """Show the end-user which artifact indicates the EULA was accepted."""
     env_var_value = os.environ.get(DIVEPLANE_EULA_ENV_VAR, '')
     env_var = env_var_value.lower() == 'true'
     eula_file = get_eula_acceptance_file_path()
```

## diveplane/utilities/installation_verification.py

```diff
@@ -20,15 +20,15 @@
 from diveplane.client import (
     AbstractDiveplaneClient, diveplane_banner, DiveplaneClient
 )
 from diveplane.client.exceptions import DiveplaneConfigurationError
 from diveplane.direct._utilities import DiveplaneLicenseAcceptanceException
 from diveplane.openapi.models import Trainee
 try:
-    from diveplane.dqt_enterprise import DataQuality  # noqa: might not be available
+    from data_quality_tool import DataQuality  # noqa: might not be available
 except OSError as e:
     DataQuality = e
 except ImportError:
     DataQuality = None
 from diveplane.utilities import infer_feature_attributes
 try:
     from diveplane.geminai import Geminai  # noqa: might not be available
@@ -894,16 +894,16 @@
         if source_df is None:
             source_df, _ = generate_dataframe(client=registry.client, num_samples=150)
 
         orig_df = source_df.sample(frac=0.5)
         gen_df = source_df[~source_df.index.isin(orig_df.index)]
         features = infer_feature_attributes(orig_df)
 
-        dqt = DataQuality(orig_df, gen_df, features=features, verbose=0)
-        desirability, _ = dqt.run_metrics(distance_ratio_type="avg")
+        dqt = DataQuality(orig_df, gen_df, features=features, verbose=-1)
+        desirability = dqt.run_metrics(distance_ratio_type="avg").overall_desirability
         if desirability < desirability_threshold:
             return (Status.WARNING, f"Desirability did not exceed the threshold of {desirability_threshold:,.1f}.")
     except Exception:
         traceback.print_exc(file=registry.logger)
         return (Status.CRITICAL,
                 "Could not complete operation. Check installation.")
     else:
```

## diveplane/utilities/utilities.py

```diff
@@ -72,15 +72,15 @@
         "always", "allow", "never".
 
     trainee_metadata : Mapping, optional
         (Optional) mapping of key/value pairs of metadata for trainee.
 
     Returns
     -------
-    Trainee
+    diveplane.openapi.models.Trainee
         A trainee object
     """
     # Place this here to avoid circular imports
     from diveplane.utilities.feature_attributes import infer_feature_attributes
     action_features = [] if action_features is None else list(action_features)
 
     if features is None:
@@ -583,88 +583,14 @@
         else:
             f_type = f_desc.get("type")
         if f_type not in valid_feature_types:
             raise ValueError(f"The feature name '{f_name}' has invalid "
                              f"feature type - '{f_type}'")
 
 
-def is_iso8601_datetime_column(column_values, name):
-    """
-    Return true if the passed in column value contain datetimes.
-
-    Note: The passed in values must have all None values filtered out.
-
-    Parameters
-    ----------
-    column_values : Pandas dataframe column
-        A Pandas dataframe column, must not contain 'None' values.
-    name : str
-        name of feature
-
-    Returns
-    -------
-    True if the column values can be parsed into a datetime
-    """
-    if len(column_values) > 0:
-        first_non_none = column_values.iloc[0]
-        if is_datetime(first_non_none):
-            validate_datetime_iso8061(first_non_none, name)
-
-            # pick another value and test if it's also a date to make sure
-            # that the first one wasn't a date by accident, for example
-            # if the column is 'miscellaneous notes' or 'comment', it's
-            # possible for it to have a datetime as the sole value, but the
-            # column itself is not actually a datetime type
-            num_values = len(column_values)
-            if num_values > 1:
-                # pick a different (not 0) index at random
-                rand_val = column_values.iloc[1 + np.random.randint(num_values - 1)]
-                if is_datetime(rand_val):
-                    return True
-
-                # the randomly chosen value isn't a datetime,
-                return False
-
-            # in the scenario  where there is only one value in the column and
-            # it was a datetime, assume the entire column is datetime
-            return True
-
-    return False
-
-
-def is_datetime(string):
-    """
-    Return True if string can be interpreted as a date.
-
-    Parameters
-    ----------
-    string : str
-        The string to check.
-
-    Returns
-    -------
-    True if string is a date, False if not.
-    """
-    # Return False if string contains only letters.
-    if isinstance(string, str) and string.isalpha():
-        return False
-    try:
-        # if the string is a number, it's not a datetime
-        float(string)
-        return False
-    except (TypeError, ValueError):
-        pass
-
-    try:
-        dt_parse(string)
-        return True
-    except Exception:  # noqa: Intentionally broad
-        return False
-
-
 def validate_datetime_iso8061(datetime_value, feature):
     """
     Check that the passed in datetime value adheres to the ISO 8601 format.
 
     Warn the user if it doesn't check out.
 
     Parameters
```

## diveplane/utilities/feature_attributes/base.py

```diff
@@ -1,22 +1,25 @@
 from abc import ABC, abstractmethod
 from collections.abc import Container
 from copy import deepcopy
+from functools import singledispatchmethod
 import json
 import logging
 import numbers
 from typing import (
     Any, Collection, Dict, Iterable, List, Mapping, Optional, Tuple, Union
 )
 import warnings
 
+from dateutil.parser import isoparse
+from dateutil.parser import parse as dt_parse
 from diveplane.utilities.features import FeatureType
 from diveplane.utilities.internals import serialize_openapi_models
 import numpy as np
-
+import pandas as pd
 
 logger = logging.getLogger(__name__)
 
 
 class FeatureAttributesBase(dict):
     """Provides accessor methods for and dict-like access to inferred feature attributes."""
 
@@ -91,54 +94,295 @@
             ]
 
         return [
             key for key in names
             if without is None or key not in without
         ]
 
+    def _validate_bounds(self, data: pd.DataFrame, feature: str, attributes: Dict) -> List[str]:  # noqa: C901
+        """Validate the feature bounds of the provided DataFrame."""
+        # Import here to avoid circular import
+        from diveplane.utilities import date_to_epoch
+
+        errors = []
+
+        # Ensure that there are bounds to validate
+        if not isinstance(attributes.get('bounds'), Mapping):
+            return errors
+
+        # Gather some data to use for validation
+        series = data[feature]
+        bounds = attributes['bounds']
+        min_bound = bounds.get('min')
+        max_bound = bounds.get('max')
+        # Get unique values but exclude NoneTypes
+        unique_values = series.dropna().unique()
+
+        if bounds.get('allowed'):
+            # Check nominal bounds
+            allowed_values = attributes['bounds']['allowed']
+            out_of_band_values = set(unique_values) - set(allowed_values)
+            if pd.isna(list(out_of_band_values)).all():
+                # Placeholder for behavior when columns contain nans
+                pass
+            elif out_of_band_values:
+                errors.append(f'{feature} contains out-of-band values: {out_of_band_values}')
+        elif attributes.get('date_time_format'):
+            # Since this is a datetime feature, convert dates to epoch time for bounds comparison
+            try:
+                if min_bound:
+                    min_bound = date_to_epoch(min_bound, time_format=attributes['date_time_format'])
+                if max_bound:
+                    max_bound = date_to_epoch(max_bound, time_format=attributes['date_time_format'])
+                for value in unique_values:
+                    epoch = date_to_epoch(value, time_format=attributes['date_time_format'])
+                    if (max_bound and epoch > max_bound) or (min_bound and epoch < min_bound):
+                        errors.append(f'{feature} has a value outside of bounds (min: {min_bound}, '
+                                      f'max: {max_bound}): {value}')
+            except ValueError as err:
+                errors.append(f'Could not validate datetime bounds due to the following error: {err}')
+        elif min_bound or max_bound:
+            # Check int/float bounds
+            for value in unique_values:
+                if (max_bound and float(value) > float(max_bound)) or (min_bound and float(value) < float(min_bound)):
+                    errors.append(f'{feature} has a value outside of bounds (min: {min_bound}, '
+                                  f'max: {max_bound}): {value}')
+
+        return errors
+
+    def _validate_dtype(self, data: pd.DataFrame, feature: str,
+                        expected_dtype: str, coerced_df: pd.DataFrame,
+                        coerce: bool = False) -> List[str]:
+        """Validate the data type of a feature and optionally attempt to coerce."""
+        errors = []
+        series = coerced_df[feature]
+        is_valid = False
+
+        if isinstance(data[feature].dtype, pd.CategoricalDtype):
+            # If the feature is a Categorical dtype, check the dtype of the categories
+            if data[feature].cat.categories.dtype.name == expected_dtype:
+                is_valid = True
+        else:
+            # Else, compare the dtype directly
+            if data[feature].dtype.name == expected_dtype:
+                is_valid = True
+            # If the feature can be converted, consider it valid (slightly differing numeric types, etc.)
+            else:
+                try:
+                    series = series.astype(expected_dtype)
+                    if coerce:
+                        coerced_df[feature] = series
+                    is_valid = True
+                except Exception: # noqa: Intentionally broad
+                    pass
+
+        # Don't strictly enforce pandas datetime subtype (datetime64[ns], datetime64[ms], etc.)
+        if not is_valid and expected_dtype == 'datetime64':
+            if pd.api.types.is_datetime64_any_dtype(data[feature]):
+                is_valid = True
+            # If the feature is detected as an object, consider it valid if it could be converted
+            elif data[feature].dtype.name == 'object':
+                try:
+                    series = pd.to_datetime(series)
+                    if coerce:
+                        coerced_df[feature] = series
+                    is_valid = True
+                except Exception: # noqa: Intentionally broad
+                    pass
+
+        # Raise warnings and/or try to coerce if the types do not match
+        if not is_valid:
+            if coerce:
+                errors.append(f'Expected dtype {expected_dtype} for feature {feature} '
+                              'but could not coerce.')
+            else:
+                errors.append(f"Feature {feature} should be '{expected_dtype}' dtype, but found "
+                              f"'{data[feature].dtype}'")
+
+        return errors
+
+    @staticmethod
+    def _allows_null(attributes: Dict) -> bool:
+        """Return whether the given attributes indicates the allowance of null values."""
+        return 'bounds' in attributes and attributes['bounds'].get('allow_null', False)
+
+    def _validate_df(self, data: pd.DataFrame, coerce: bool = False,  # noqa: C901
+                     raise_errors: bool = False, table_name: str = None):
+        errors = []
+        coerced_df = data.copy(deep=True)
+        features = self[table_name] if table_name else self
+
+        for feature, attributes in features.items():
+            if feature not in data.columns:
+                # Column is missing
+                if not feature.startswith('.'):
+                    errors.append(f'{feature} is missing from the dataframe')
+                # OK if it's an internal feature
+                continue
+
+            # Check ordinal types
+            if attributes['type'] == 'ordinal':
+                if 'bounds' in attributes and 'allowed' in attributes['bounds']:
+                    # Check type (should be object)
+                    errors.extend(self._validate_dtype(data, feature, 'object',
+                                                       coerced_df, coerce=coerce))
+                elif 'bounds' in attributes:
+                    # Check type (should be float)
+                    if attributes.get('decimal_places', 0) > 0 or self._allows_null(attributes):
+                        errors.extend(self._validate_dtype(data, feature, 'float64',
+                                                           coerced_df, coerce=coerce))
+                    # Check type (should be int)
+                    else:
+                        errors.extend(self._validate_dtype(data, feature, 'int64',
+                                                           coerced_df, coerce=coerce))
+
+            # Check nominal types
+            elif attributes['type'] == 'nominal':
+                if attributes.get('data_type') == 'number':
+                    # Check type (should be float)
+                    if attributes.get('decimal_places', 0) > 0 or self._allows_null(attributes):
+                        errors.extend(self._validate_dtype(data, feature, 'float64',
+                                                           coerced_df, coerce=coerce))
+                    # Check type (should be int)
+                    else:
+                        errors.extend(self._validate_dtype(data, feature, 'int64',
+                                                           coerced_df, coerce=coerce))
+                elif attributes.get('data_type') == 'boolean':
+                    # Check type (should be bool)
+                    errors.extend(self._validate_dtype(data, feature, 'bool',
+                                                       coerced_df, coerce=coerce))
+                else:
+                    # Else, should be object
+                    errors.extend(self._validate_dtype(data, feature, 'object',
+                                                       coerced_df, coerce=coerce))
+
+            # Check continuous types
+            elif attributes['type'] == 'continuous':
+                if 'date_time_format' in attributes:
+                    # Check type (should be datetime64)
+                    errors.extend(self._validate_dtype(data, feature, 'datetime64',
+                                                       coerced_df, coerce=coerce))
+                elif attributes.get('decimal_places', -1) > 0 or self._allows_null(attributes):
+                    # Check type (should be float64)
+                    errors.extend(self._validate_dtype(data, feature, 'float64',
+                                                       coerced_df, coerce=coerce))
+                elif attributes.get('decimal_places', -1) == 0:
+                    # Check type (should be int64)
+                    errors.extend(self._validate_dtype(data, feature, 'int64',
+                                                       coerced_df, coerce=coerce))
+            # Check feature bounds
+            errors.extend(self._validate_bounds(data, feature, attributes))
+
+        if errors:
+            msg = ('Failed to validate DataFrame against feature attributes due to the '
+                   'following errors:\n')
+            for error in errors:
+                msg = msg + f'{error}\n'
+            if raise_errors:
+                raise ValueError(msg)
+            else:
+                warnings.warn(msg)
+
+        if coerce:
+            return coerced_df
+
+    @abstractmethod
+    def validate(data: Any, coerce=False, raise_errors=False):
+        """
+        Validate the given data against this FeatureAttributes object.
+
+        Check that feature bounds and data types loosely describe the data. Optionally
+        attempt to coerce the data into conformity.
+        Parameters
+        ----------
+        data : Any
+            The data to validate
+        coerce : bool
+            Whether to attempt to coerce DataFrame columns into correct data types
+        raise_errors : bool
+            If True, raises a ValueError if nonconforming columns are found; else issue a warning
+        Returns
+        -------
+        None | DataFrame
+            None or the coerced DataFrame if 'coerce' is True and there were no errors.
+        """
+        raise NotImplementedError()
+
 
 class MultiTableFeatureAttributes(FeatureAttributesBase):
     """A dict-like object containing feature attributes for multiple tables."""
 
     pass
 
 
 class SingleTableFeatureAttributes(FeatureAttributesBase):
     """A dict-like object containing feature attributes for a single table or DataFrame."""
 
-    pass
+    @singledispatchmethod
+    def validate(data: Any, **kwargs):
+        """
+        Validate the given single table data against this FeatureAttributes object.
+
+        Check that feature bounds and data types loosely describe the data. Optionally
+        attempt to coerce the data into conformity.
+        Parameters
+        ----------
+        data : Any
+            The data to validate (single table only)
+        coerce : bool
+            Whether to attempt to coerce DataFrame columns into correct data types
+        raise_errors : bool
+            If True, raises a ValueError if nonconforming columns are found; else issue a warning
+        strict_numeric_types : bool
+            Whether to strictly enforce numeric dtypes if what is inferred does not match the
+            feature attributes.
+        Returns
+        -------
+        None | DataFrame
+            None or the coerced DataFrame if 'coerce' is True and there were no errors.
+        """
+        raise NotImplementedError("'data' is an unsupported type")
+
+    @validate.register
+    def _(self, data: pd.DataFrame, coerce=False, raise_errors=False):
+        return self._validate_df(data, coerce=coerce, raise_errors=raise_errors)
 
 
 class InferFeatureAttributesBase(ABC):
     """
     This is an abstract Feature Attributes inferrer base class.
 
     It is agnostic to the type of data being inspected.
     """
 
     def _process(self,  # noqa: C901
                  features: Optional[Dict[str, Dict]] = None,
                  infer_bounds: bool = True,
                  dropna: bool = False,
-                 tight_bounds: bool = False,
                  tight_bound_features: Optional[Iterable[str]] = None,
+                 tight_bounds: Optional[Union[bool, Iterable[str]]] = False,
                  mode_bound_features: Optional[Iterable[str]] = None,
                  id_feature_name: Optional[Union[str, Iterable[str]]] = None,
                  attempt_infer_extended_nominals: bool = False,
                  nominal_substitution_config: Optional[Dict[str, Dict]] = None,
                  include_extended_nominal_probabilities: Optional[bool] = False,
                  datetime_feature_formats: Optional[Dict] = None,
                  ordinal_feature_values: Optional[Dict[str, List[str]]] = None,
                  dependent_features: Optional[Dict[str, List[str]]] = None,
                  ) -> Dict:
         """
         Get inferred feature attributes for the parameters.
 
         See `infer_feature_attributes` for full docstring.
         """
+        if tight_bound_features:
+            warnings.warn("Argument 'tight_bound_features' is deprecated. Please set "
+                          "'tight_bounds' to the feature list.")
+            tight_bounds = tight_bound_features
+
         if features and not isinstance(features, dict):
             raise ValueError(
                 f"The parameter `features` needs to be a `dict` and not of "
                 f"type {type(features)}."
             )
 
         if features:
@@ -304,15 +548,14 @@
                              f'not {type(id_feature_name)}.')
 
         if infer_bounds:
             for feature_name in feature_attributes:
                 bounds = self._infer_feature_bounds(
                     feature_attributes, feature_name,
                     tight_bounds=tight_bounds,
-                    tight_bound_features=tight_bound_features,
                     mode_bound_features=mode_bound_features,
                 )
                 if bounds:
                     feature_attributes[feature_name]['bounds'] = bounds  # noqa
 
         if dropna:
             for feature_name in feature_attributes:
@@ -410,16 +653,15 @@
         """Get inferred attributes for the given unknown-type column."""
 
     @abstractmethod
     def _infer_feature_bounds(
         self,
         feature_attributes: Mapping[str, Mapping],
         feature_name: str,
-        tight_bounds: bool = False,
-        tight_bound_features: Optional[Iterable[str]] = None,
+        tight_bounds: Optional[Union[bool, Iterable[str]]] = False,
         mode_bound_features: Optional[Iterable[str]] = None,
     ) -> Optional[Dict]:
         """
         Return inferred bounds for the given column.
 
         Features with datetimes are converted to seconds since epoch and their
         bounds are calculated accordingly. Features with timedeltas are
@@ -429,19 +671,18 @@
         ----------
         data : Any
             Input data
         feature_attributes : dict
             A dictionary of feature names to a dictionary of parameters.
         feature_name : str
             The name of feature to infer bounds for.
-        tight_bounds: bool, default False
-            If True, will set tight min and max bounds on all continuous
-            features.
-        tight_bound_features : list of str, optional
-            Explicit list of feature names that should have tight bounds.
+        tight_bounds: bool or Iterable of str, default False
+            Set tight min and max bounds on either the features specified in
+            the Iterable, or on all continuous features if True, or none if
+            False.
         mode_bound_features : list of str, optional
             Explicit list of feature names that should use mode bounds. When
             None, uses all features.
 
         Returns
         -------
         dict or None
@@ -490,14 +731,83 @@
         elif max_bound > 0:
             # for positive max_bound boundary values: e^ceil(ln(num))
             max_bound = np.exp(np.ceil(np.log(max_bound)))
 
         return min_bound, max_bound
 
     @staticmethod
+    def _is_datetime(string):
+        """
+        Return True if string can be interpreted as a date.
+
+        Parameters
+        ----------
+        string : str
+            The string to check.
+
+        Returns
+        -------
+        True if string is a date, False if not.
+        """
+        # Return False if string contains only letters.
+        if isinstance(string, str) and string.isalpha():
+            return False
+        try:
+            # if the string is a number, it's not a datetime
+            float(string)
+            return False
+        except (TypeError, ValueError):
+            pass
+
+        try:
+            dt_parse(string)
+            return True
+        except Exception:  # noqa: Intentionally broad
+            return False
+
+    def _is_iso8601_datetime_column(self, feature: str) -> bool:
+        """
+        Return whether the given feature contains ISO 8601 datetimes.
+
+        Parameters
+        ----------
+        feature : string
+            The feature to check the values of.
+
+        Returns
+        -------
+        True if the column values can be parsed into an ISO 8601 datetime
+        """
+        first_non_none = self._get_first_non_null(feature)
+        if first_non_none is None:
+            return False
+
+        # Pick another value and test if it's also a date to make sure
+        # that the first one wasn't a date by accident, for example
+        # if the column is 'miscellaneous notes' or 'comment', it's
+        # possible for it to have a datetime as the sole value, but the
+        # column itself is not actually a datetime type
+        rand_val = self._get_random_value(feature, no_nulls=True)
+
+        # Try to parse one or both values as strictly iso8601
+        try:
+            if not self._is_datetime(first_non_none):
+                return False
+            isoparse(first_non_none)
+            if rand_val is not None:
+                if not self._is_datetime(rand_val):
+                    return False
+                isoparse(rand_val)
+        except Exception:  # noqa: Intentionally broad
+            return False
+
+        # No issues; it's valid
+        return True
+
+    @staticmethod
     def _add_id_attribute(feature_attributes: Mapping,
                           id_feature_name: str
                           ) -> None:
         """Update the given feature_attributes in-place for id_features."""
         if id_feature_name in feature_attributes:
             feature_attributes[id_feature_name]['id_feature'] = True
             # If id feature was inferred to be continuous, change it to nominal
@@ -592,14 +902,18 @@
             The feature type or None if the column could not be found.
         Dict or None
             Additional typing information about the feature or None if the
             column could not be found.
         """
 
     @abstractmethod
+    def _get_random_value(self, feature_name: str, no_nulls: bool = False) -> Any:
+        """Retrieve a random value from the data."""
+
+    @abstractmethod
     def _has_unique_constraint(self, feature_name: str) -> bool:
         """Return whether this feature has a unique constraint."""
 
     @abstractmethod
     def _get_first_non_null(self, feature_name: str) -> Optional[Any]:
         """
         Get the first non-null value in the given column.
```

## diveplane/utilities/feature_attributes/infer_feature_attributes.py

```diff
@@ -199,21 +199,22 @@
                         '0' : 0.178,
                         '1': 3.4582e-3,
                         '2': None
                     }
                 }
             }
 
-    tight_bounds : bool, default False
-        (optional) If True, will set tight min and max bounds on all
-        continuous features.
-
     tight_bound_features : list of str, default None
-        (Optional) Explicit list of feature names that should have tight
-        bounds.
+        (Deprecated) Explicit list of feature names that should have tight
+        bounds. This argument is deprecated; please use `tight_bounds` instead.
+
+    tight_bounds: bool or Iterable of str, default False
+        (Optional) Set tight min and max bounds on either the features
+        specified in the Iterable, or on all continuous features if True,
+        or none if False.
 
     tight_time_bounds : bool, default False
         (optional) If True, will set tight_bounds flag on time_feature to True.
         The True flag will cause the bounds for the start and end times set
         to the same bounds as observed in the original data.
 
     time_series_type_default : str, default 'rate'
```

## diveplane/utilities/feature_attributes/pandas.py

```diff
@@ -1,13 +1,13 @@
 import datetime
 import decimal
 import logging
 from math import isnan
 from typing import (
-    Any, Dict, Iterable, List, Mapping, Optional, Tuple
+    Any, Dict, Iterable, List, Mapping, Optional, Tuple, Union
 )
 import warnings
 
 from dateutil.parser import parse as dt_parse
 import numpy as np
 import pandas as pd
 from pandas.core.dtypes.common import (
@@ -23,15 +23,14 @@
 
 from .base import InferFeatureAttributesBase, SingleTableFeatureAttributes
 from ..features import FeatureType
 from ..utilities import (
     date_to_epoch,
     determine_iso_format,
     epoch_to_date,
-    is_iso8601_datetime_column,
     ISO_8601_DATE_FORMAT,
     ISO_8601_FORMAT,
     time_to_seconds,
 )
 
 logger = logging.getLogger(__name__)
 
@@ -168,20 +167,35 @@
 
     def _get_first_non_null(self, feature_name: str) -> Optional[Any]:
         index = self.data[feature_name].first_valid_index()
         if index is None:
             return None
         return self.data[feature_name][index]
 
+    def _get_random_value(self, feature_name: str, no_nulls: bool = False) -> Optional[Any]:
+        """
+        Return a random sample from the given DataFrame column.
+
+        The return type is determined by the column type.
+
+        if `no_nulls` is set, select a random value from the set of non-null
+        values, if any. If there are no such non-nulls, this will return None.
+        """
+        cases = self.data[feature_name]
+        if no_nulls:
+            cases = cases.loc[~self.data[feature_name].isnull()]
+        if len(cases) <= 1:
+            return None
+        return cases.iloc[1 + np.random.randint(len(cases) - 1)]
+
     def _infer_feature_bounds(  # noqa: C901
         self,
         feature_attributes: Mapping[str, Mapping],
         feature_name: str,
-        tight_bounds: bool = False,
-        tight_bound_features: Optional[Iterable[str]] = None,
+        tight_bounds: Optional[Union[bool, Iterable[str]]] = False,
         mode_bound_features: Optional[Iterable[str]] = None,
     ) -> Optional[Dict]:
         output = None
         allow_null = True
         column = self.data[feature_name]
         # only integers by default do not allow nulls
         if is_integer_dtype(column.dtype):
@@ -249,16 +263,16 @@
                     max_date_tz = max_date.tzinfo
 
                 try:
                     actual_min_f = min_f = date_to_epoch(min_date, format_dt)
                     actual_max_f = max_f = date_to_epoch(max_date, format_dt)
                     if (
                         not tight_bounds
-                        and (tight_bound_features is None or
-                             feature_name not in tight_bound_features)
+                        or (isinstance(tight_bounds, Iterable) and
+                            feature_name not in tight_bounds)
                     ):
                         # Calculate loose bounds
                         min_f, max_f = self.infer_loose_feature_bounds(min_f, max_f)
                         # Check for mode bounds
                         if (
                             mode_bound_features is None or
                             feature_name in mode_bound_features
@@ -318,16 +332,16 @@
             if not (isnan(min_f) or isnan(max_f)):
                 actual_min_f = min_f
                 actual_max_f = max_f
                 # set loose bounds if no tight bounds for all and this
                 # feature isn't on the tight bounds list
                 if (
                     not tight_bounds
-                    and (tight_bound_features is None or
-                         feature_name not in tight_bound_features)
+                    or (isinstance(tight_bounds, Iterable) and
+                        feature_name not in tight_bounds)
                 ):
                     min_f, max_f = self.infer_loose_feature_bounds(actual_min_f,
                                                                    actual_max_f)
                     # Check for mode bounds
                     if (
                         mode_bound_features is None or
                         feature_name in mode_bound_features
@@ -506,18 +520,18 @@
             }
 
         return attributes
 
     def _infer_string_attributes(self, feature_name: str) -> Dict:
         # Column has arbitrary string values, first check if they
         # are ISO8601 datetimes.
-        non_null_values = self.data[feature_name].loc[~self.data[feature_name].isnull()]
-        if is_iso8601_datetime_column(non_null_values, feature_name):
+        if self._is_iso8601_datetime_column(feature_name):
             # if datetime, determine the iso8601 format it's using
-            fmt = determine_iso_format(non_null_values.iloc[0], feature_name)
+            first_non_null = self._get_first_non_null(feature_name)
+            fmt = determine_iso_format(first_non_null, feature_name)
             return {
                 "type": "continuous",
                 "date_time_format": fmt
             }
         else:
             return self._infer_unknown_attributes(feature_name)
```

## diveplane/utilities/feature_attributes/relational.py

```diff
@@ -2,15 +2,15 @@
 import datetime
 from datetime import time, timedelta
 import decimal
 import logging
 from math import ceil, isnan, log
 import re
 from typing import (
-    Any, Dict, Iterable, List, Mapping, Optional, Tuple
+    Any, Dict, Iterable, List, Mapping, Optional, Tuple, Union
 )
 import warnings
 
 import numpy as np
 import pandas as pd
 
 from .base import InferFeatureAttributesBase, MultiTableFeatureAttributes, SingleTableFeatureAttributes
@@ -20,19 +20,17 @@
     SQLTableProtocol,
     TableNameProtocol,
 )
 from ..features import FeatureType
 from ..utilities import (
     date_to_epoch,
     determine_iso_format,
-    is_datetime,
     ISO_8601_DATE_FORMAT,
     ISO_8601_FORMAT,
     time_to_seconds,
-    validate_datetime_iso8061,
 )
 
 logger = logging.getLogger(__name__)
 
 # Import sqlalchemy only if it's available. Not all users of this package will have or need
 # this installed.
 try:
@@ -305,29 +303,29 @@
 
         if `no_nulls` is set, select a random value from the set of non-null
         values, if any. If there are no such non-nulls, this will return None.
         """
         with session_scope(self.session_cls) as session:
             # Get the total number of non-null rows in the given table
             if no_nulls:
-                num_samples = session.db_table.c[feature_name].filter(
+                num_samples = session.query(self.data.c[feature_name]).filter(
                     self.data.c[feature_name].is_not(None)).count()
             else:
                 num_samples = session.query(self.data.c[feature_name]).count()
 
             if num_samples == 0:
                 random_value = None
             else:
                 # Select a pseudo-random one by offset. This should be DB
                 # agnostic and reasonably performant since we only want a
                 # single row/value.
                 idx = np.random.randint(num_samples)
                 if no_nulls:
                     random_value = (
-                        session.query
+                        session.query(self.data.c[feature_name])
                                .filter(self.data.c[feature_name].is_not(None))
                                .offset(idx)
                                .first()
                     )
                 else:
                     random_value = (
                         session.query(self.data.c[feature_name])
@@ -434,51 +432,14 @@
 
         if result is None:
             return []
 
         # value, count
         return [(r[0], r[1]) for r in result]
 
-    def _is_iso8601_datetime_column(self, feature_name: str) -> bool:
-        """
-        Return true if the passed in column value contain datetimes.
-
-        NOTE: This function is meant to mirror the existing function of the
-        same name in diveplane.utilities. The purpose is to detect if a column
-        contains dates formatted as ISO-8601 strings.
-        """
-        first_non_none = self._get_first_non_null(feature_name)
-        if first_non_none is None:
-            return False
-
-        if is_datetime(first_non_none):
-            # Simply warns if the feature is a non-ISO-8601-formatted date.
-            # TODO: Consider if issuing a user-warning even makes sense in the
-            #       context of geminai-data-services.
-            validate_datetime_iso8061(first_non_none, feature_name)
-
-            # Pick another value and test if it's also a date to make sure
-            # that the first one wasn't a date just coincidentally.
-            # For example, if the column is 'miscellaneous notes' or 'comment',
-            # it's possible for it to have a string datetime as the sole value,
-            # but the column itself is not actually a datetime type.
-            num_values = self._get_num_cases()
-            if num_values > 1:
-                # pick a different (not 0) index at random
-                rand_val = self._get_random_value(feature_name, no_nulls=True)
-                if is_datetime(rand_val):
-                    return True
-
-                # the randomly chosen value isn't a datetime,
-                return False
-
-            # In the scenario where there is only one value in the column and
-            # it was a datetime, assume the entire column is datetime
-            return True
-
     def _get_num_features(self) -> int:
         return len(self._get_feature_names())
 
     def _get_num_cases(self) -> int:
         with session_scope(self.session_cls) as session:
             num_rows = session.query(self.data).count()
         return num_rows
@@ -782,16 +743,15 @@
         return {
             'type': 'nominal'
         }
 
     def _infer_feature_bounds(self,  # noqa: C901
                               feature_attributes: Mapping[str, Mapping],
                               feature_name: str,
-                              tight_bounds: bool = False,
-                              tight_bound_features: Optional[List[str]] = None,
+                              tight_bounds: Optional[Union[bool, Iterable[str]]] = False,
                               mode_bound_features: Optional[List[str]] = None,
                               ) -> Optional[Dict]:
         output = None
         allow_null = True
         original_type = feature_attributes[feature_name]['original_type']
 
         # Only integers by default do no allow nulls.
@@ -861,17 +821,18 @@
             if (
                 min_value is not None and max_value is not None and
                 not isnan(min_value) and
                 not isnan(max_value)
             ):
                 actual_min_value = min_value
                 actual_max_value = max_value
-                if not tight_bounds and (
-                    tight_bound_features is None or
-                    feature_name not in tight_bound_features
+                if (
+                    not tight_bounds
+                    or (isinstance(tight_bounds, Iterable) and
+                        feature_name not in tight_bounds)
                 ):
                     min_value, max_value = (
                         self.infer_loose_feature_bounds(actual_min_value,
                                                         actual_max_value))
 
                     if (
                         mode_bound_features is None or
```

## diveplane/utilities/feature_attributes/time_series.py

```diff
@@ -542,15 +542,15 @@
             infer_bounds=infer_bounds,
             dropna=dropna,
             datetime_feature_formats=datetime_feature_formats,
             attempt_infer_extended_nominals=attempt_infer_extended_nominals,
             nominal_substitution_config=nominal_substitution_config,
             include_extended_nominal_probabilities=include_extended_nominal_probabilities,
             id_feature_name=id_feature_name,
-            tight_bound_features=tight_bound_features,
+            tight_bounds=tight_bound_features,
             mode_bound_features=mode_bound_features,
         )
 
         if isinstance(time_invariant_features, str):
             time_invariant_features = [time_invariant_features]
         elif isinstance(time_invariant_features, Iterable):
             time_invariant_features = time_invariant_features
```

## diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py

```diff
@@ -15,14 +15,15 @@
 import pytz
 
 
 cwd = Path(__file__).parent.parent.parent.parent
 iris_path = Path(cwd, 'utilities', 'tests', 'data', 'iris.csv')
 int_path = Path(cwd, 'utilities', 'tests', 'data', 'integers.csv')
 stock_path = Path(cwd, 'utilities', 'tests', 'data', 'mini_stock_data.csv')
+ts_path = Path(cwd, 'utilities', 'tests', 'data', 'example_timeseries.csv')
 
 # Partially defined dictionary-1
 features_1 = {
     "sepal_length": {
         "type": "continuous",
         'bounds': {
             'min': 2.72,
@@ -496,7 +497,124 @@
     feature_attributes = infer_feature_attributes(df, time_feature_name='DATE', features=features)
 
     # Ensure that the partial features remain
     for feature in features.keys():
         for k, v in features[feature].items():
             assert k in feature_attributes[feature]
             assert feature_attributes[feature][k] == v
+
+
+@pytest.mark.parametrize("tight_bounds", [
+    (['DATE', 'TURNOVER', '%DELIVERABLE']),
+    (['DATE', '%DELIVERABLE']),
+    (['DATE', 'TURNOVER']),
+    (['%DELIVERABLE', 'TURNOVER']),
+    (['DATE']),
+    (['TURNOVER']),
+    (['%DELIVERABLE']),
+    ([''])
+])
+def test_tight_bounds(tight_bounds):
+    """Test the tight_bounds argument with a features list."""
+    df = pd.read_csv(stock_path)
+    all_tight_bounds = infer_feature_attributes(df, tight_bounds=True)
+    no_tight_bounds = infer_feature_attributes(df, tight_bounds=False)
+
+    features = infer_feature_attributes(df, tight_bounds=tight_bounds)
+    for feature in features.keys():
+        if 'bounds' not in features[feature]:
+            continue
+        if feature in tight_bounds:
+            assert features[feature]['bounds'] == all_tight_bounds[feature]['bounds']
+        else:
+            assert features[feature]['bounds'] == no_tight_bounds[feature]['bounds']
+
+
+def test_validate_dataframe():
+    """Test the validate method with a DataFrame."""
+    # Test valid feature attributes against their original datasets
+    # (should not raise any exceptions!)
+    # Iris dataset
+    df = pd.read_csv(iris_path)
+    features = infer_feature_attributes(df)
+    assert features.validate(df, raise_errors=True) is None
+    # Integers dataset
+    df = pd.read_csv(int_path)
+    features = infer_feature_attributes(df)
+    assert features.validate(df, raise_errors=True) is None
+    # Example timeseries dataset
+    df = pd.read_csv(ts_path)
+    features = infer_feature_attributes(df, time_feature_name='date')
+    assert features.validate(df, raise_errors=True) is None
+    # Mini stock data dataset
+    df = pd.read_csv(stock_path)
+    features = infer_feature_attributes(df, time_feature_name='DATE')
+    assert features.validate(df, raise_errors=True) is None
+    # Also try this one with a non-ts infer
+    df = pd.read_csv(stock_path)
+    features = infer_feature_attributes(df)
+    assert features.validate(df, raise_errors=True) is None
+    # Should not raise any exceptions and return a "coerced" dataframe
+    df = pd.read_csv(iris_path)
+    features = infer_feature_attributes(df)
+    df['sepal_length'] = df['sepal_length'].astype('int64')
+    df = features.validate(df, coerce=True, raise_errors=True)
+    assert df is not None
+    assert pd.api.types.is_float_dtype(df['sepal_length'])
+    # Try validating a categorical feature
+    df = pd.read_csv(iris_path)
+    features['class']['type'] = 'ordinal'
+    features['class']['bounds'] = {}
+    unique = list(df['class'].unique())
+    features['class']['bounds']['allowed'] = unique
+    df['class'] = df['class'].astype(pd.CategoricalDtype(categories=unique))
+    df = features.validate(df, coerce=True, raise_errors=True)
+    assert df is not None
+    assert pd.api.types.is_categorical_dtype(df['class'])
+
+
+@pytest.mark.parametrize("ftype, data_type, decimal_places, bounds, date_time_format, expected_dtype", [
+    ("continuous", "number", 0, {'allow_null': False}, None, "int64"),
+    ("continuous", "number", 1, {'allow_null': False}, None, "float64"),
+    ("continuous", "number", 0, {'allow_null': False}, "%Y-%m-%d", "datetime64"),
+    ("ordinal", "number", 0, {'allow_null': False}, None, "int64"),
+    ("ordinal", "number", 2, {'allow_null': True}, None, "float64"),
+    ("ordinal", "string", 0, {'allowed': ['SBIN'], 'allow_null': False}, None, "object"),
+    ("nominal", "number", 0, {'allow_null': False}, None, "int64"),
+    ("nominal", "number", 9, {'allow_null': False}, None, "float64"),
+    ("nominal", "boolean", 0, {'allow_null': False}, None, "bool"),
+])
+def test_validate_df_multiple_dtypes(ftype, data_type, decimal_places, bounds, date_time_format,
+                                     expected_dtype):
+    """Test the validate() method with all possible inferred dtypes."""
+    # First, read in the mini_stock_series dataset as it has a variety of data types
+    df = pd.read_csv(stock_path)
+    # Based on the expected_dtype, choose the feature in the dataset that is loosely described by the given parameters
+    if expected_dtype == 'int64':
+        feature = 'VOLUME'
+    elif expected_dtype == 'float64':
+        feature = 'PREV CLOSE'
+    elif expected_dtype == 'datetime64':
+        feature = 'DATE'
+    elif expected_dtype == 'bool':
+        # Make a new column of a bool dtype since there are none in the dataset
+        df['NEW'] = True
+        feature = 'NEW'
+    else:
+        feature = 'SYMBOL'
+    # Infer the feature attributes like normal, but replace the attributes for the chosen feature
+    # with our parameter attributes, which should also be considered valid.
+    attrs = infer_feature_attributes(df, time_feature_name='DATE')
+    attrs[feature] = {
+        'type': ftype,
+        'data_type': data_type,
+        'decimal_places': decimal_places,
+        'bounds': bounds,
+        'date_time_format': date_time_format,
+    }
+    if not date_time_format:
+        del attrs[feature]['date_time_format']
+    # validate() should not raise any errors
+    coerced_df = attrs.validate(df, raise_errors=True, coerce=True)
+    assert coerced_df is not None
+    # coerced_df should also contain a coerced DATE column, as it is originally detected as a string
+    assert coerced_df['DATE'].dtype.name == 'datetime64[ns]'
```

## Comparing `diveplane_reactor_api-2.5.3.dist-info/LICENSE.txt` & `diveplane_reactor_api-5.0.18.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `diveplane_reactor_api-2.5.3.dist-info/RECORD` & `diveplane_reactor_api-5.0.18.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,70 +1,70 @@
 diveplane/client/LICENSE-3RD-PARTY.txt,sha256=QEmcu1Zg-r4WfRD4AIwQKN5UeegTiq5f8a1WYk5ytII,171737
-diveplane/client/__init__.py,sha256=6LdRtjajHEHFSuBhCIpbgyviOkFqVV28lV3kSCX4T6Q,2145
-diveplane/client/base.py,sha256=KV2vr6Ez8BwGhVT7o4-ZNFjcfrPKtkRRkdT6yur2Kro,14544
+diveplane/client/__init__.py,sha256=4GNjZRT1LoQB3o9MqHazav_aqMEzz8-2hUu2m4eoQoU,2146
+diveplane/client/base.py,sha256=vOihpEjiqXiK9k4h_nu6705WOVPv8_u0eenNIMf3FSs,14410
 diveplane/client/cache.py,sha256=Kmaz31Br2aS9BiDCStmGFxpkiRY71iPwvetzyamON3U,3080
 diveplane/client/client.py,sha256=hhdv48zGkpHO6jN3dvq8KJixE-28tTzi_0fGU8dVCN8,16092
 diveplane/client/configuration.py,sha256=oqPT4a4oBWS-jMMOSqlUL8AiQtSlg7bvBai96JZhBmo,4594
 diveplane/client/exceptions.py,sha256=2rSJ0BGw5wsBZI6iQ4V3G8mXL0ZQeK7ypQpPeKUVSNY,4329
 diveplane/client/protocols.py,sha256=m0X5RHkPyG5F3oqs7WbleEqo7Pv-nrnTuZc0o7_-K1U,958
-diveplane/client/requirements-3.10.txt,sha256=s2kULz12dbDABr3lSAFI0kd7PcOWiJdnWbyFC6CTDZ4,1900
-diveplane/client/requirements-3.11.txt,sha256=ndoE8Ym3qeons_kIWaFQmrlVvXFQutpCr8AIqvLtoq0,1900
-diveplane/client/requirements-3.8.txt,sha256=DRGLJMzjrWDCjyc_sAfmTP8x4syBf78F4V8NeFZYqCE,1919
-diveplane/client/requirements-3.9.txt,sha256=zPmaGMnmTKug6p7zV679cbBL10E80DJvNcgIys-ugkw,1898
-diveplane/client/requirements-dev-3.10.txt,sha256=CDc90kawICcvel2uajB2Lp340gph-xktlErPyPSA9Iw,3291
-diveplane/client/requirements-dev-3.11.txt,sha256=tpknjsQVRJk5s7L5BDcO3_mmY6nHOVI6gKUEclE6r_k,3060
-diveplane/client/requirements-dev-3.8.txt,sha256=PQ712rnW37UcXjUEBYLjvCzGNXwP--5PBWSBzjti-5w,3278
-diveplane/client/requirements-dev-3.9.txt,sha256=0U_g9MzCeB9VnRZlF8YQbNWnWv-gYbLQcy7JQM6ECXY,3278
+diveplane/client/requirements-3.10.txt,sha256=9y_rCjwIIq-Y_kV8WPW3Jrs3XUlYqZDr7hfzp1jo7HI,1900
+diveplane/client/requirements-3.11.txt,sha256=IhGsFvoDh1e2L99yZJeIZAn8Fdt2Smd4zoXH0hlobPg,1900
+diveplane/client/requirements-3.8.txt,sha256=wHrH4_WR2YICIeAVgWvkwC6zQU9WCVt7sHsamQwtpzQ,1919
+diveplane/client/requirements-3.9.txt,sha256=dvT_0eHsUJtM6VtqiVggGSE_3yj4C1-3dnO-1WHpw6M,1898
+diveplane/client/requirements-dev-3.10.txt,sha256=_3mn7zpznzV2SftC7hF5WhlaNa_bKJa66firu3EM0qA,3309
+diveplane/client/requirements-dev-3.11.txt,sha256=NwqkMNIWTq3XlE9-x3ApZyiGa5VnDQw-0sXdkt7Olvs,3060
+diveplane/client/requirements-dev-3.8.txt,sha256=r0rfvYDBCJW468Iv4Q8X4tg21t4R1JbahosocDrmygg,3296
+diveplane/client/requirements-dev-3.9.txt,sha256=mL61kfUnkiQysbKy3_9wiDb3CMk_R9OHFZMTw3yoY7s,3296
 diveplane/client/requirements-dev.in,sha256=JBgU9DPrMnbzV5xteJ0AQSFH5JIUQF_qlukMZ_Rrzsc,210
-diveplane/client/requirements.in,sha256=WPuMILuKgpH5Wxtw_eT71HZ7eK_IhgpyqudLil-5Y6M,371
+diveplane/client/requirements.in,sha256=uLHJTjnOM9IwWMpFvI1OwMZPDYq-8041fA2ELDVAzmc,371
 diveplane/client/pandas/__init__.py,sha256=ZdaK7e_ovjv158JIde8kVt9XSWGbhhzDtGOLouqCqOo,175
 diveplane/client/pandas/client.py,sha256=Xqoy3hlIueLEdOkjxc5tK1uREz3J7xtk5hpP5BEsaME,10935
 diveplane/client/tests/__init__.py,sha256=LnuLdAce6XZRk49LY3lMNcuOplDB3pe8L2OLAnyYMNk,845
 diveplane/client/tests/test_client.py,sha256=ZmEjl_N8Xbi5P7XbkqbwYQsTHx8h1KU6Juyf6Bj0JS8,50937
-diveplane/client/tests/test_infer_feature_attributes.py,sha256=xPZIJphJTisO5nHQD--HIFjADNyjiAU7sae0ZKFopq8,14329
+diveplane/client/tests/test_infer_feature_attributes.py,sha256=JNuxenY4rUxb7ONCZF9Yz0j1sT9aOlMAzARzvukYtyk,13534
 diveplane/client/tests/test_scikit.py,sha256=MtPxO1AFP9l_MYoGKThQuN7_emOY-zpYvwbarr7yl58,8783
 diveplane/direct/__init__.py,sha256=Yv7hQCFhrEXpMud0DTn-XxRggERJGvsoCBv8ywqcsE8,193
 diveplane/direct/_utilities.py,sha256=Bkh5xDDL4SQudN8vhdUXzizXc5uVoGYvG4iMoV8IQgY,4538
-diveplane/direct/client.py,sha256=eeSOSPK_gBJ8AsZMtxj8sBUWGZGNCs37uKqkl8pspf4,214199
-diveplane/direct/core.py,sha256=0vZ-U06rCw8sh5zVWw92Fa55iMmdPk0nZDOPR06A574,66688
+diveplane/direct/client.py,sha256=5e9xPJ1XxqEOKigj--qRYLk96pJK-wgEnHLMWLA_cFs,213772
+diveplane/direct/core.py,sha256=ZpJlOpTSN67A-FjA2mhS5wVGETSiSYM2abFeOno8Zn4,66259
 diveplane/direct/tests/test_standalone.py,sha256=JnScTGnewhw0EdkiMsCt-HYJ0b931Frd9fN5QrfG4Eg,923
 diveplane/reactor/__init__.py,sha256=mio87Im_AHb24labU6xx8w7K_rjHU4hGHxTIraZ4YPY,768
 diveplane/reactor/client.py,sha256=LUNTU571VyLCYpnlh2kNsR06LeRXInctLlRsw2aml1w,1282
 diveplane/reactor/project.py,sha256=EfD-TQZ0vcJDliiqpT2HZXhKi0giC4J3YORwyaHhuoQ,10613
 diveplane/reactor/session.py,sha256=ZZ5NoP_jYdeFo20HXboYVMX7_crP6mdTibHgis9wMAE,9637
-diveplane/reactor/trainee.py,sha256=yw-Eay1Ct8wXb1exA7isvws9JCx86l-9FOMhvJooDxw,127203
+diveplane/reactor/trainee.py,sha256=-sypPpkJ4JyLSs5HoDKGtEQisH5u6ogylwrE3_j-Umc,126345
 diveplane/reactor/tests/conftest.py,sha256=ZIgUWK98WXhI9wOMrZ7BaQsYElaW8iqAqpQa-25kOBU,302
 diveplane/reactor/tests/test_reactor.py,sha256=eLvrzb07ei9uJUw-XdOvSNZ1wo46R0wkVpxQQc44F8I,3243
 diveplane/scikit/__init__.py,sha256=3c8FbYnCRuRBHiv9anNmDR0rHM61RQwKekzarBkuH2w,426
-diveplane/scikit/scikit.py,sha256=BoY-TT7cLguKWHxAzAyFNVZHNXSjI7Wy2eRVLxgM13w,54916
-diveplane/utilities/__init__.py,sha256=b9fmFNtovDCqsJcsiAyXQfcuNBzd_SPovH9qKpTp5Ng,1758
-diveplane/utilities/eula_helper.py,sha256=gmnv2x5IOUN6LC4xvlPaW6NlDPYuCK0iMqTT1GsNP_U,3256
+diveplane/scikit/scikit.py,sha256=P4EoPWltdO3Xg22KhrJuLuqTuV7qUocnc6V17-Y8Ag8,54659
+diveplane/utilities/__init__.py,sha256=SEeEY7-IYV03B2XMCnrEf4O8tGKUKZenN3_xgJNGaig,1741
+diveplane/utilities/eula_helper.py,sha256=DqfHstMAX2_eM2GMW_Qd_IEd_12hGXj-et7mCjvlJRY,3655
 diveplane/utilities/features.py,sha256=Wd3vibeFa6ICcQsDNQgfuRChquHN5iLQmMBhO_JyqGo,22403
 diveplane/utilities/guess_feature_attributes.py,sha256=hJhmat3zHg9KeJ0Gt-2mm9iBw54YnqlHTQH8YPSS2QI,698
-diveplane/utilities/installation_verification.py,sha256=hY8gRh8iCm9Q7lGaFex_XG6egJgojpnqJue3tWkqqQ4,39646
+diveplane/utilities/installation_verification.py,sha256=ok1br_JBzHzZXA9eHpuOF2lVMJ2nOabOJCDrQxXLLGI,39658
 diveplane/utilities/internals.py,sha256=z3MQGBuzbtTQzDfaLJNLiptvUEqyf6d705KX2kXnu-Q,25873
 diveplane/utilities/json_wrapper.py,sha256=rGv60a185rJRNLvk0vYPfWchHAoXBPLYz7fZuZV8uIE,2126
 diveplane/utilities/locale.py,sha256=EREn4KKN9RxAnT8ysgi8qiiv9-4P7kXE1UKIOKWoj0w,2424
 diveplane/utilities/monitors.py,sha256=lVszyiU8sVflcJ-UXlbqZW3YBrNzDSkjYSbg6mT16Jo,4697
 diveplane/utilities/posix.py,sha256=94LaB9gwtVJKC8LBdoE8E7HudE0BIHRmHf-my7jzwFU,2341
 diveplane/utilities/random.py,sha256=5rTYgJXwgcCTtGmOcy7la7MyGl1npwHYdK-qgNa1sx0,380
-diveplane/utilities/utilities.py,sha256=Ibf0qvAMSPJfOxq2ngxmk8L_LNcouQo6k4jwD0W3XEY,42853
+diveplane/utilities/utilities.py,sha256=Swyr8SH2rWbrX94Bb0jOCrKDJZcmRBi-PQ-f4PK7I30,40637
 diveplane/utilities/feature_attributes/__init__.py,sha256=sVfvYYLhM7zqilIg94vEYmCHLrgf6r_YYnDmU-k5dMg,194
-diveplane/utilities/feature_attributes/base.py,sha256=NICbfTg3dhGfgHvY82IHeyxx3ppHH41JMFecaPvuUXA,25413
-diveplane/utilities/feature_attributes/infer_feature_attributes.py,sha256=9TbqYoqSG_Ygfv8f1EHlUKBe43pvlwiTfFYVPQnzrbM,16249
-diveplane/utilities/feature_attributes/pandas.py,sha256=AyFBOVcMj-WmouhSc5ywlCaNKvLf2E3lyIXD86XgQIo,22625
+diveplane/utilities/feature_attributes/base.py,sha256=qV7S2YAt5Umk6Sk8uYKriPyEMQheD-Tl3UpWO-lUVuM,39499
+diveplane/utilities/feature_attributes/infer_feature_attributes.py,sha256=0-wQ-MhIPvfkVxxz5PXWd-I9TGKPPR96li1wAUgVDI8,16407
+diveplane/utilities/feature_attributes/pandas.py,sha256=fYzbh2JecKBbZQX2Plh0c7B6wcHeyMelnkGDZ97_g_U,23164
 diveplane/utilities/feature_attributes/protocols.py,sha256=bZFdT0AeQiKXsoPGTQVQQ7fs4wle1wghZox-EVTHhYY,6359
-diveplane/utilities/feature_attributes/relational.py,sha256=9mIUUgtywfiRpuTrtdzT54GxWTcB1JrKb_IkA9HAt0I,39180
-diveplane/utilities/feature_attributes/time_series.py,sha256=S4HXKCuh8OXfDRkrlYiUdQ9Dwf94BjZIsdm_MYpm5dE,31817
-diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py,sha256=Yx8xcDfVIYLYkm-Ok2ZyvbSbv22h7IOoZQFZW3HHB6k,19545
+diveplane/utilities/feature_attributes/relational.py,sha256=Fthe9UUSd53NcK9PIgl2Nl491s1VF3REMhoz49YND8E,37406
+diveplane/utilities/feature_attributes/time_series.py,sha256=JBSAV7p1LIks0z07iv88GV2zqdu9P-Ohu_Y9MUZD2-U,31809
+diveplane/utilities/feature_attributes/tests/test_infer_feature_attributes.py,sha256=WcpdSo_foGDWR4AmOhfRrjQq9KCDp9-2SnUotKLFYtA,24901
 diveplane/utilities/feature_attributes/tests/test_infer_time_series_attributes.py,sha256=VuvQJrRveKKUbaWkqWJ5N3Yp1q495gMrwtHvYvpPEAs,6442
 diveplane/utilities/tests/__init__.py,sha256=PDWNKsI7BxQHj3t6j9JzGsIe2PtMNV97QNjmHLuGseQ,1321
 diveplane/utilities/tests/conftest.py,sha256=UPuIMzLSn76ug_bh8uRzVtZAw1HCB_fGDS-nmuhjnZE,51
 diveplane/utilities/tests/test_features.py,sha256=MFdnGZd5MUJEw6GL-vS7LYaN5GTRj0DcI1qLL-Z-dSk,13611
 diveplane/utilities/tests/test_internals.py,sha256=E2Qy5L-00E2kzhjz9sk8zolXebom_dVHiFsAMRWhDhM,8189
 diveplane/utilities/tests/test_utilities.py,sha256=9lWVm3_iR7YZh3OAXT0Hjb6tnj-2X7l4uYDnISmGANM,11237
-diveplane_reactor_api-2.5.3.dist-info/LICENSE.txt,sha256=4UTGY81GVApDEB2E0QKXiQuyEUVTWXJvrR46vpD9oNM,12104
-diveplane_reactor_api-2.5.3.dist-info/METADATA,sha256=-5_9h7atK4uXlnEOHQw0GqbQgEjHVrNw8CMw0DoXCoU,1356
-diveplane_reactor_api-2.5.3.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-diveplane_reactor_api-2.5.3.dist-info/entry_points.txt,sha256=qY25dCLu2kebBfJ5CmFkRoaNlc_nEYI3LW_D2RgXPM0,157
-diveplane_reactor_api-2.5.3.dist-info/top_level.txt,sha256=tZqkTmi8MrypFXe_oDtXVHM7ePHFgma5zJaXO5OpwfA,10
-diveplane_reactor_api-2.5.3.dist-info/RECORD,,
+diveplane_reactor_api-5.0.18.dist-info/LICENSE.txt,sha256=4UTGY81GVApDEB2E0QKXiQuyEUVTWXJvrR46vpD9oNM,12104
+diveplane_reactor_api-5.0.18.dist-info/METADATA,sha256=9XbSptNS4gGrHQGfYH18ENiF3Dygj1uP4EJiTgP2jbI,3192
+diveplane_reactor_api-5.0.18.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+diveplane_reactor_api-5.0.18.dist-info/entry_points.txt,sha256=qY25dCLu2kebBfJ5CmFkRoaNlc_nEYI3LW_D2RgXPM0,157
+diveplane_reactor_api-5.0.18.dist-info/top_level.txt,sha256=tZqkTmi8MrypFXe_oDtXVHM7ePHFgma5zJaXO5OpwfA,10
+diveplane_reactor_api-5.0.18.dist-info/RECORD,,
```

