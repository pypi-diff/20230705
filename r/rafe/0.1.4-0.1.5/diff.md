# Comparing `tmp/rafe-0.1.4-py3-none-any.whl.zip` & `tmp/rafe-0.1.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,21 +1,21 @@
-Zip file size: 23015 bytes, number of entries: 19
--rw-r--r--  2.0 unx       71 b- defN 23-Jun-16 02:51 rafe/__init__.py
--rw-r--r--  2.0 unx       61 b- defN 23-Jun-16 02:51 rafe/__main__.py
--rw-r--r--  2.0 unx     2014 b- defN 23-Jun-16 02:51 rafe/build.py
--rw-r--r--  2.0 unx    22950 b- defN 23-Jun-16 02:51 rafe/cfgraph.py
--rw-r--r--  2.0 unx    13115 b- defN 23-Jun-16 02:51 rafe/cli.py
--rw-r--r--  2.0 unx     8138 b- defN 23-Jun-16 02:51 rafe/cli_api.py
--rw-r--r--  2.0 unx     2366 b- defN 23-Jun-16 02:51 rafe/config.py
--rw-r--r--  2.0 unx     1241 b- defN 23-Jun-16 02:51 rafe/logger.py
--rw-r--r--  2.0 unx      281 b- defN 23-Jun-16 02:51 rafe/main.py
--rw-r--r--  2.0 unx     2087 b- defN 23-Jun-16 02:51 rafe/metadata.py
--rw-r--r--  2.0 unx     4014 b- defN 23-Jun-16 02:51 rafe/plugin.py
--rw-r--r--  2.0 unx     2162 b- defN 23-Jun-16 02:51 rafe/source.py
--rw-r--r--  2.0 unx     4064 b- defN 23-Jun-16 02:51 rafe/utils.py
--rw-r--r--  2.0 unx     1455 b- defN 23-Jun-16 02:52 rafe-0.1.4.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx     1125 b- defN 23-Jun-16 02:52 rafe-0.1.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-16 02:52 rafe-0.1.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       40 b- defN 23-Jun-16 02:52 rafe-0.1.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        5 b- defN 23-Jun-16 02:52 rafe-0.1.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1387 b- defN 23-Jun-16 02:52 rafe-0.1.4.dist-info/RECORD
-19 files, 66668 bytes uncompressed, 20813 bytes compressed:  68.8%
+Zip file size: 23624 bytes, number of entries: 19
+-rw-r--r--  2.0 unx       71 b- defN 23-Jul-05 19:23 rafe/__init__.py
+-rw-r--r--  2.0 unx       63 b- defN 23-Jul-05 19:23 rafe/__main__.py
+-rw-r--r--  2.0 unx     2415 b- defN 23-Jul-05 19:23 rafe/build.py
+-rw-r--r--  2.0 unx    26009 b- defN 23-Jul-05 19:23 rafe/cfgraph.py
+-rw-r--r--  2.0 unx    13854 b- defN 23-Jul-05 19:23 rafe/cli.py
+-rw-r--r--  2.0 unx     9039 b- defN 23-Jul-05 19:23 rafe/cli_api.py
+-rw-r--r--  2.0 unx     2639 b- defN 23-Jul-05 19:23 rafe/config.py
+-rw-r--r--  2.0 unx     1286 b- defN 23-Jul-05 19:23 rafe/logger.py
+-rw-r--r--  2.0 unx      302 b- defN 23-Jul-05 19:23 rafe/main.py
+-rw-r--r--  2.0 unx     2068 b- defN 23-Jul-05 19:23 rafe/metadata.py
+-rw-r--r--  2.0 unx     3998 b- defN 23-Jul-05 19:23 rafe/plugin.py
+-rw-r--r--  2.0 unx     2217 b- defN 23-Jul-05 19:23 rafe/source.py
+-rw-r--r--  2.0 unx     4044 b- defN 23-Jul-05 19:23 rafe/utils.py
+-rw-r--r--  2.0 unx     1455 b- defN 23-Jul-05 19:24 rafe-0.1.5.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx     1878 b- defN 23-Jul-05 19:24 rafe-0.1.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-05 19:24 rafe-0.1.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx       40 b- defN 23-Jul-05 19:24 rafe-0.1.5.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        5 b- defN 23-Jul-05 19:24 rafe-0.1.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1387 b- defN 23-Jul-05 19:24 rafe-0.1.5.dist-info/RECORD
+19 files, 72862 bytes uncompressed, 21422 bytes compressed:  70.6%
```

## zipnote {}

```diff
@@ -33,26 +33,26 @@
 
 Filename: rafe/source.py
 Comment: 
 
 Filename: rafe/utils.py
 Comment: 
 
-Filename: rafe-0.1.4.dist-info/LICENSE.txt
+Filename: rafe-0.1.5.dist-info/LICENSE.txt
 Comment: 
 
-Filename: rafe-0.1.4.dist-info/METADATA
+Filename: rafe-0.1.5.dist-info/METADATA
 Comment: 
 
-Filename: rafe-0.1.4.dist-info/WHEEL
+Filename: rafe-0.1.5.dist-info/WHEEL
 Comment: 
 
-Filename: rafe-0.1.4.dist-info/entry_points.txt
+Filename: rafe-0.1.5.dist-info/entry_points.txt
 Comment: 
 
-Filename: rafe-0.1.4.dist-info/top_level.txt
+Filename: rafe-0.1.5.dist-info/top_level.txt
 Comment: 
 
-Filename: rafe-0.1.4.dist-info/RECORD
+Filename: rafe-0.1.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## rafe/__init__.py

```diff
@@ -1,3 +1,3 @@
 # rafe: Reproducible Artifacts for Environments
 
-__version__ = '0.1.4'
+__version__ = "0.1.5"
```

## rafe/__main__.py

```diff
@@ -1,4 +1,4 @@
 from rafe.cli import run
 
 if __name__ == "__main__":
-		run()
+    run()
```

## rafe/build.py

```diff
@@ -1,75 +1,87 @@
 import os
 import sys
-import subprocess 
-from os.path import isfile, join
+import subprocess
 
-from rafe.logger import logger 
+from rafe.logger import logger
 import rafe.source as source
 
 
 def get_environ():
     d = dict(os.environ)
-    d['PREFIX'] = sys.prefix
-    d['PYTHON'] = sys.executable
+    d["PREFIX"] = sys.prefix
+    d["PYTHON"] = sys.executable
     return d
 
+
 def ensure_build_dir_exists(recipe_dir, package):
-	"""
-	If the build directory exists, return true. Otherwise, raise an error.
-	"""
-	pass
+    """
+    If the build directory exists, return true. Otherwise, raise an error.
+    """
+    pass
 
 
 def build_package(recipe_dir):
-	"""
-	Builds a package for the platform that is being used to build. 
-	"""
-	
-	source.provide(recipe_dir)
-	src_dir = source.get_dir()
+    """
+    Builds a package for the platform that is being used to build.
+    """
+
+    recipe_dir = recipe_dir.resolve()
+    source.provide(recipe_dir)
+    src_dir = source.get_dir()
 
-	logger.info(f"""
+    logger.info(
+        f"""
 	Building Pacakge: {recipe_dir}
 	Source Tree: 	  {src_dir}
-	""")
-
-	env = get_environ()
-
-	if sys.platform == 'win32':
-		vcvarsall = (r'C:\Program Files (x86)\Microsoft Visual Studio 14.0'
-                     r'\VC\vcvarsall.bat')
-		assert isfile(vcvarsall)
+	"""
+    )
 
-		with open(join(recipe_dir, 'bld.bat')) as fi:
-			data = fi.read()
-			logger.debug(f"Read File {join(recipe_dir, 'bld.bat')} [green] SUCCESS [/green]")
+    env = get_environ()
 
-		with open(join(src_dir, 'bld.bat'), 'w') as fo:
+    if sys.platform == "win32":
+        vcvarsall = (
+            r"C:\Program Files (x86)\Microsoft Visual Studio 14.0" r"\VC\vcvarsall.bat"
+        )
+        assert os.path.isfile(vcvarsall)
+
+        batfile = recipe_dir.joinpath("bld.bat")
+        with open(batfile) as fi:
+            data = fi.read()
+            logger.debug(f"Read File {batfile} [green] SUCCESS [/green]")
+        srcdir = src_dir.joinpath("bld.bat")
+        with open(srcdir, "w") as fo:
             # more debuggable with echo on
-			fo.write('@echo on\n')
-			for kv in env.items():
-				fo.write('set %s=%s\n' % kv)
-				fo.write('call "%s" amd64\n' % vcvarsall)
-				fo.write(":: --- end generated header ---\n")
-				fo.write(data)
-		
-		cmd = [os.environ['COMSPEC'], '/c', 'bld.bat']
-		subprocess.check_call(cmd, cwd=src_dir)
-
-	else:
-
-		cmd = ['/bin/bash', '-x', '-e', recipe_dir.joinpath('build.sh')]
-		
-		build_process = subprocess.Popen(cmd, env=env, cwd=src_dir, stdout=subprocess.PIPE, bufsize=1)
-		log_relative_filename = f"[bold cyan] {recipe_dir.name}" + "/build.sh" + "[/bold cyan] | " 
-		for line in build_process.stdout:
-			logger.info(log_relative_filename + line.decode('UTF-8').strip('\n'), extra={"markup": True})
-
-		build_process.wait()
-		if build_process.returncode != 0:
-			raise OSError(build_process.returncode) 	
-
-		logger.info(f'Build of {recipe_dir.name} [bold green] SUCCESS [/bold green] :boom:', extra={"markup": True})
-		return 0
-
-		
+            fo.write("@echo on\n")
+            for kv in env.items():
+                fo.write("set %s=%s\n" % kv)
+                fo.write('call "%s" amd64\n' % vcvarsall)
+                fo.write(":: --- end generated header ---\n")
+                fo.write(data)
+
+        cmd = [os.environ["COMSPEC"], "/c", "bld.bat"]
+        subprocess.check_call(cmd, cwd=src_dir)
+
+    else:
+        cmd = ["/bin/bash", "-x", "-e", recipe_dir.joinpath("build.sh")]
+
+        build_process = subprocess.Popen(
+            cmd, env=env, cwd=src_dir, stdout=subprocess.PIPE, bufsize=1
+        )
+        log_relative_filename = (
+            f"[bold cyan] {recipe_dir.name}" + "/build.sh" + "[/bold cyan] | "
+        )
+        for line in build_process.stdout:
+            logger.info(
+                log_relative_filename + line.decode("UTF-8").strip("\n"),
+                extra={"markup": True},
+            )
+
+        build_process.wait()
+        if build_process.returncode != 0:
+            raise OSError(build_process.returncode)
+
+        logger.info(
+            f"Build of {recipe_dir.name} [bold green] SUCCESS [/bold green] :boom:",
+            extra={"markup": True},
+        )
+        return 0
```

## rafe/cfgraph.py

```diff
@@ -6,506 +6,711 @@
 import re
 import logging
 
 from typing import Union, Dict, List
 from datetime import datetime, timedelta
 from dataclasses import dataclass
 
+
 @dataclass
-class CFGraphPaths():
+class CFGraphPaths:
     """
     Given a default home or base path, create and check for these files.
     """
 
     home: pathlib.Path = pathlib.Path.home()
     cache_age_minutes: int = 120
 
     def __post_init__(self):
+        self.cache: pathlib.Path = self.home.joinpath(".rafe", ".cfcache/")
+
+        self.package_reports: pathlib.Path = self.cache.joinpath("package_reports")
+        self.package_jsons: pathlib.Path = self.cache.joinpath("jsons")
+        self.match_cache: pathlib.Path = self.cache.joinpath("cache", "matches")
+
+        self.noarch_json: pathlib.Path = self.cache.joinpath(
+            pathlib.Path("noarch_repodata.json")
+        )
+        self.linux64_json: pathlib.Path = self.cache.joinpath(
+            pathlib.Path("linux-64_repodata.json")
+        )
+        self.win64_json: pathlib.Path = self.cache.joinpath(
+            pathlib.Path("win64_repodata.json")
+        )
+        self.grayskull_map: pathlib.Path = self.cache.joinpath(
+            pathlib.Path("grayskull_pypi_mapping.yaml")
+        )
 
-        self.cache: pathlib.Path = self.home.joinpath('.rafe', '.cfcache/')
 
-        self.package_reports: pathlib.Path = self.cache.joinpath('package_reports')
-        self.package_jsons: pathlib.Path = self.cache.joinpath('jsons')
-        self.match_cache: pathlib.Path = self.cache.joinpath('cache', 'matches')
-
-    
-        self.noarch_json: pathlib.Path = self.cache.joinpath(pathlib.Path("noarch_repodata.json"))
-        self.linux64_json: pathlib.Path = self.cache.joinpath(pathlib.Path("linux-64_repodata.json"))
-        self.win64_json: pathlib.Path = self.cache.joinpath(pathlib.Path("win64_repodata.json"))
-        self.grayskull_map: pathlib.Path = self.cache.joinpath(pathlib.Path("grayskull_pypi_mapping.yaml"))
-    
 @dataclass
-class CFGraphURLs():
+class CFGraphURLs:
     """
     By deafult, these are the URLs that the application will check against.
     """
+
     noarch_repodata: str = "https://conda.anaconda.org/conda-forge/noarch/repodata.json"
-    linux64_repodata: str = "https://conda.anaconda.org/conda-forge/linux-64/repodata.json"
+    linux64_repodata: str = (
+        "https://conda.anaconda.org/conda-forge/linux-64/repodata.json"
+    )
     grayskull_map: str = "https://raw.githubusercontent.com/regro/cf-graph-countyfair/master/mappings/pypi/grayskull_pypi_mapping.yaml"
 
     def __post_init__(self):
         # TODO Update this URL to compile on load
         self.package_root: str = "https://github.com/regro/libcfgraph/raw/master/artifacts/{package_requested}/conda-forge/{package_arch}/"
 
-class CFGraph():
+
+class CFGraph:
     """
     This class manages the local repodata for _where_ packages come from. It can be initalized with any filepath,
     and will look for the filepaths that it needs in order to work.
     """
-    def __init__(self, 
-                arch: str = "linux-64", 
-                configuration: CFGraphPaths = CFGraphPaths(), 
-                urls: CFGraphURLs = CFGraphURLs(),
-                logger: logging.Logger = None, 
-        ):
 
-        self.config = configuration 
+    def __init__(
+        self,
+        arch: str = "linux-64",
+        configuration: CFGraphPaths = CFGraphPaths(),
+        urls: CFGraphURLs = CFGraphURLs(),
+        logger: logging.Logger = None,
+    ):
+        self.config = configuration
         self.urls = urls
 
         # Initialize the logging mechanism
         if isinstance(logger, logging.Logger):
-                self.logger = logger
+            self.logger = logger
         else:
-            logger = logging.getLogger(__name__)    
+            logger = logging.getLogger(__name__)
             self.logger = logger
 
     ###### REPODATA MANAGEMENT ######
 
-    def get_file_age(self, filepath: pathlib.Path)-> timedelta:
+    def get_file_age(self, filepath: pathlib.Path) -> timedelta:
         """
         Get the age of a file and when it was last modified. Returns a datetime.timedelta
         """
         if filepath.exists():
             last_modified_time = datetime.fromtimestamp(os.path.getmtime(filepath))
             current_time = datetime.now()
 
             file_age = current_time - last_modified_time
         else:
             file_age = timedelta(seconds=9999999)
 
         return file_age
-    
+
     def convert_pypi_conda_name(self, package_name: str, gs_map: Dict) -> str:
         """
         This function converts a pypi package name to a conda name using an initialized grayskull map if it's in there. Else,
         it just returns the input.
         """
 
         if package_name in gs_map.keys():
             conda_name = gs_map[package_name]
-            self.logger.info("[bold purple] Converted Package Name: [/bold purple]" +  "[bold blue]Py[/bold blue][bold yellow]Pi[/bold yellow] | " + f'{package_name} ->' \
-                    + '[bold Green] Conda[/bold green] |' + f' {conda_name}', extra = {"markup": True})
+            self.logger.info(
+                "[bold purple] Converted Package Name: [/bold purple]"
+                + "[bold blue]Py[/bold blue][bold yellow]Pi[/bold yellow] | "
+                + f"{package_name} ->"
+                + "[bold Green] Conda[/bold green] |"
+                + f" {conda_name}",
+                extra={"markup": True},
+            )
             return conda_name
         return package_name
-    
+
     def fetch_repodata_file(self, url: str, destination: pathlib.Path):
         """
         DEPRECATED
 
         Fetches the repodata files with special streaming with the URL. Saves it to the
-        filepath defined by destination. 
+        filepath defined by destination.
         response = requests.get(url, stream=True)
         response.raise_for_status()
-    
+
         total_size = response.headers.get("Content-Length", 0)
         if total_size == 0:
             site = urlopen(url)
             meta = site.info()
             total_size = int(meta["Content-Length"])
 
         with open(destination, "wb") as file:
             for chunk in response.iter_content(chunk_size=8192):
                 file.write(chunk)
         """
 
     def check_update_repodata(self, filepath: pathlib.Path) -> bool:
         """
         Given a filepath (to a JSON or YAML formatted repodata file) and a URL, check and see
-        whether an update is needed. 
+        whether an update is needed.
 
         If return is True, you'll want to call self.update_repodata()
         """
-        age = self.get_file_age(filepath).total_seconds() / 60 
+        age = self.get_file_age(filepath).total_seconds() / 60
 
         if age >= self.config.cache_age_minutes:
             return True
         else:
             return False
-    
-    def update_repodata(self, arch : str) -> None:
+
+    def update_repodata(self, arch: str) -> None:
         """
         Runs the "update repodata" method on all filepaths and URLS as described in the
-        input objects. 
+        input objects.
         """
         grayskull_url = self.urls.grayskull_map
 
         if arch == "linux-64":
             path = self.config.linux64_json
             url = self.urls.linux64_repodata
 
         elif arch == "noarch":
             path = self.config.noarch_json
             url = self.urls.noarch_repodata
 
-        #elif arch == "win64"
+        # elif arch == "win64"
         #   path = self.config.win64_json
         #   url = self.urls.linux64_repodata
         #   noarch_path = self.config.noarch_json
         #   noarch_url = self.urls.noarch_repodata
         return
 
-
     def load_repodata(self, arch: str = "noarch") -> Union[Dict, None]:
         """
         Given an architecture, return a json object. Defaults to noarch.
         Possible arch arguments: "noarch", "linux-64", "win64"
         """
         if arch == "linux-64":
             path = self.config.linux64_json
         elif arch == "noarch":
             path = self.config.noarch_json
         elif arch == "win64":
             path = self.config.win64_json
 
         if path.exists():
-            with open(path, 'r') as read_file:
+            with open(path, "r") as read_file:
                 json_object = json.load(read_file)
 
-        missing_timestamp = [i for i in json_object['packages'] if 'timestamp' not in json_object['packages'][i].keys()]
+        missing_timestamp = [
+            i
+            for i in json_object["packages"]
+            if "timestamp" not in json_object["packages"][i].keys()
+        ]
         for i in missing_timestamp:
-            json_object['packages'][i]['timestamp'] = 0
+            json_object["packages"][i]["timestamp"] = 0
 
-        self.logger.info(f'Load Repodata | from {path} | Success')
+        self.logger.info(f"Load Repodata | from {path} | Success")
         return json_object
 
-        
     ##### PACKAGE VERSION MATCHING ######
 
     @staticmethod
     def get_package_name(s: str) -> str:
         """
         Gets the human-readable name of the package from the uploaded package tarball name.
         """
-        pattern = r'^(.*?)(?=-\d+\.\d+\.\d+)'
+        pattern = r"^(.*?)(?=-\d+\.\d+\.\d+)"
         match = re.match(pattern, s)
-        return match.group(0) if match else ''
+        return match.group(0) if match else ""
 
     @staticmethod
     def get_package_version(filename: str) -> Union[str, None]:
         """
         Gets the package version given a filename string.
         """
-        pattern = r'(?<=-)\d+\.\d+\.\d+'
+        pattern = r"(?<=-)\d+\.\d+\.\d+"
         match = re.search(pattern, filename)
         return match.group(0) if match else None
 
     @staticmethod
     def get_package_build(filename: str) -> Union[str, None]:
         """
         Gets the package build identifier from filename string.
         """
-        pattern = r'(?<=\d-).*(?=.tar.bz2)'
+        pattern = r"(?<=\d-).*(?=.tar.bz2)"
         match = re.search(pattern, filename)
-        return match.group(0) if match else ''
+        return match.group(0) if match else ""
 
-    def fetch_package_json(self, requested_package, matched_package, arch = "linux-64"):
+    def fetch_package_json(self, requested_package, matched_package, arch="linux-64"):
         """
         Fetches the metadata for a given single package.
         """
-        
+
         root_url = f"https://github.com/regro/libcfgraph/raw/master/artifacts/{requested_package}/conda-forge/{arch}/"
 
         if len(matched_package) > 0:
-            url = root_url+matched_package.replace(".tar.bz2",".json")
+            url = root_url + matched_package.replace(".tar.bz2", ".json")
         else:
-            url = ""    
-    
-        self.logger.info(f'Fetching from {url}')
-        pkg_metadata_json_filepath = self.config.package_jsons.joinpath(arch).joinpath(url.split("/")[-1])
+            url = ""
+
+        self.logger.info(f"Fetching from {url}")
+        pkg_metadata_json_filepath = self.config.package_jsons.joinpath(arch).joinpath(
+            url.split("/")[-1]
+        )
 
         if not pkg_metadata_json_filepath.exists():
-            with open(pkg_metadata_json_filepath,'wb') as write_file:
-                self.logger.info("[bold blue] Downloading,.. [/bold blue]"+ f"{url.split('/')[-1]}", extra={"markup": True}) 
+            with open(pkg_metadata_json_filepath, "wb") as write_file:
+                self.logger.info(
+                    "[bold blue] Downloading,.. [/bold blue]" + f"{url.split('/')[-1]}",
+                    extra={"markup": True},
+                )
                 r = requests.get(url)
-                if len(r.content)>0:
+                if len(r.content) > 0:
                     write_file.write(r.content)
 
-
-    def format_package_json(self, requested_package, matched_package, arch = "linux-64"):
+    def format_package_json(self, requested_package, matched_package, arch="linux-64"):
         """
         Fetches the metadata for a given package.
         """
-        
+
         root_url = f"https://github.com/regro/libcfgraph/raw/master/artifacts/{requested_package}/conda-forge/{arch}/"
 
         if len(matched_package) > 0:
-            url = root_url+matched_package.replace(".tar.bz2",".json")
+            url = root_url + matched_package.replace(".tar.bz2", ".json")
         else:
-            url = ""    
+            url = ""
 
-        pkg_metadata_json_filepath = self.config.package_jsons.joinpath(arch).joinpath(url.split("/")[-1])
+        pkg_metadata_json_filepath = self.config.package_jsons.joinpath(arch).joinpath(
+            url.split("/")[-1]
+        )
         return (url, pkg_metadata_json_filepath)
 
-            
     def read_missed_and_matched_from_cache(self):
         """
-        Reads a cached manifest file and then uses that if a search was just done. 
+        Reads a cached manifest file and then uses that if a search was just done.
         """
-        fp = list(self.config.package_reports.glob('cfgraph-to-fetch-*.json'))[-1]
-        with open(fp, 'r') as read_file:
+        fp = list(self.config.package_reports.glob("cfgraph-to-fetch-*.json"))[-1]
+        with open(fp, "r") as read_file:
             json_object = json.load(read_file)
-        
+
         return json_object
 
     def read_manifest(self, manifest_file_path):
         """
         Reads a JSON formatted manifest. You can do this using the metaconvert functon.
         """
-        with open(manifest_file_path, 'r') as read_file:
+        with open(manifest_file_path, "r") as read_file:
             json_object = json.load(read_file)
-        
+
         package_arch = json_object["package_arch"]
         python_version_requested = json_object["python_version_requested"]
         manifest_packages = json_object["packages"]
-        
-        return package_arch, python_version_requested, manifest_packages
 
+        return package_arch, python_version_requested, manifest_packages
 
-    def match_packages(self, package_requested, version_requested, python_version_requested, package_arch, json_object):
+    def match_packages(
+        self,
+        package_requested,
+        version_requested,
+        python_version_requested,
+        package_arch,
+        json_object,
+    ):
+        self.logger.info(f"Matching | {package_requested} | {version_requested} ")
+
+        all_matches = [
+            i
+            for i in json_object["packages"]
+            if self.get_package_name(i) == package_requested
+        ]
 
-        self.logger.info(f'Matching | {package_requested} | {version_requested} ')
-        
-        all_matches = [i for i in json_object["packages"] if self.get_package_name(i) == package_requested]
-        
         if version_requested != None:
-            version_matches = [i for i in all_matches if self.get_package_version(i) == version_requested]
+            version_matches = [
+                i
+                for i in all_matches
+                if self.get_package_version(i) == version_requested
+            ]
 
         else:
             version_matches = all_matches
-    
+
         if python_version_requested != "":
-            version_matches = [i for i in version_matches if python_version_requested in i]
-            
+            version_matches = [
+                i for i in version_matches if python_version_requested in i
+            ]
+
         return version_matches
 
     def choose_build(self, version_requested, matched_packages, json_object):
         if len(matched_packages) == 1:
             return matched_packages[0]
         if (version_requested == "") or (version_requested is None):
-            repo_matched_subset = {i:json_object[i] for i in matched_packages}
-            return [i[0] for i in sorted(repo_matched_subset.items(), key=lambda x:x[1]['timestamp'])][-1]
+            repo_matched_subset = {i: json_object[i] for i in matched_packages}
+            return [
+                i[0]
+                for i in sorted(
+                    repo_matched_subset.items(), key=lambda x: x[1]["timestamp"]
+                )
+            ][-1]
         else:
-            versions = [json_object[i]['version'] for i in matched_packages]
+            versions = [json_object[i]["version"] for i in matched_packages]
             if version_requested in versions:
                 version_exact = version_requested
             else:
                 versions2 = versions.copy()
                 versions2.append(version_requested)
                 versions2.sort(key=StrictVersion)
                 version_index = versions2.index(version_requested)
                 if version_index == len(versions2) - 1:
                     version_index = version_index - 1
                 else:
                     version_index = version_index + 1
                 version_exact = versions2[version_index]
-            repo_matched_subset = {i:json_object[i] for i in matched_packages}
-            versioned_subset = {i:repo_matched_subset[i] for i in repo_matched_subset if repo_matched_subset[i]['version'] == version_exact}
+            repo_matched_subset = {i: json_object[i] for i in matched_packages}
+            versioned_subset = {
+                i: repo_matched_subset[i]
+                for i in repo_matched_subset
+                if repo_matched_subset[i]["version"] == version_exact
+            }
             if len(versioned_subset) == 1:
                 return versioned_subset[0]
             else:
-                return [i[0] for i in sorted(versioned_subset.items(), key=lambda x:x[1]['timestamp'])][-1]
+                return [
+                    i[0]
+                    for i in sorted(
+                        versioned_subset.items(), key=lambda x: x[1]["timestamp"]
+                    )
+                ][-1]
         return []
 
-    def match_versioned_packages(self, package_requested, version_requested, json_object):
-        
+    def match_versioned_packages(
+        self, package_requested, version_requested, json_object
+    ):
         if version_requested == "":
             version_requested = None
-    
+
         if version_requested != None:
-            version_matches = [i for i in json_object if json_object[i]["name"] == package_requested and json_object[i]["version"] == version_requested]
-        else:
-            version_matches = [i for i in json_object if json_object[i]["name"] == package_requested]
-            
+            version_matches = [
+                i
+                for i in json_object
+                if json_object[i]["name"] == package_requested
+                and json_object[i]["version"] == version_requested
+            ]
+        else:
+            version_matches = [
+                i for i in json_object if json_object[i]["name"] == package_requested
+            ]
+
         return version_matches
 
     def check_build_host_depends(self, package_requested: str, json_object: str):
-        if 'build' in json_object['rendered_recipe']['requirements'].keys():
-            build_match = [i for i in json_object['rendered_recipe']['requirements']['build'] if package_requested in i]
+        if "build" in json_object["rendered_recipe"]["requirements"].keys():
+            build_match = [
+                i
+                for i in json_object["rendered_recipe"]["requirements"]["build"]
+                if package_requested in i
+            ]
         else:
             build_match = []
-        if 'host' in json_object['rendered_recipe']['requirements'].keys():
-            host_match = [i for i in json_object['rendered_recipe']['requirements']['host'] if package_requested in i]
+        if "host" in json_object["rendered_recipe"]["requirements"].keys():
+            host_match = [
+                i
+                for i in json_object["rendered_recipe"]["requirements"]["host"]
+                if package_requested in i
+            ]
         else:
             host_match = []
 
         return build_match, host_match
 
-    def check_package_build(self, package_requested, lin64_json_object, noarch_json_object, mode, debug):
+    def check_package_build(
+        self, package_requested, lin64_json_object, noarch_json_object, mode, debug
+    ):
         package_requested = package_requested.lower()
         all_jsons = list(self.config.package_jsons.joinpath("linux-64").glob("*.json"))
-        picked_json = [i for i in all_jsons if package_requested == self.get_package_name(str(i.name))]
-        if len(picked_json)==0:
-            all_jsons = list(self.config.package_jsons.joinpath("noarch").glob("*.json"))
-            picked_json = [i for i in all_jsons if package_requested == self.get_package_name(str(i.name))]
+        picked_json = [
+            i
+            for i in all_jsons
+            if package_requested == self.get_package_name(str(i.name))
+        ]
+        if len(picked_json) == 0:
+            all_jsons = list(
+                self.config.package_jsons.joinpath("noarch").glob("*.json")
+            )
+            picked_json = [
+                i
+                for i in all_jsons
+                if package_requested == self.get_package_name(str(i.name))
+            ]
 
-        if len(picked_json)>0:
+        if len(picked_json) > 0:
             picked_json = picked_json[0]
         else:
-            self.logger.info(f"Unable to find local json for requested '{package_requested}'")
+            self.logger.info(
+                f"Unable to find local json for requested '{package_requested}'"
+            )
             return
-        with open(picked_json, 'r') as f:
+        with open(picked_json, "r") as f:
             package_json = json.load(f)
 
-        if mode == 'run':
-            if 'run' in package_json['rendered_recipe']['requirements'].keys():
-                depends = [i.split(" ")[0] for i in package_json['rendered_recipe']['requirements']['run']]
+        if mode == "run":
+            if "run" in package_json["rendered_recipe"]["requirements"].keys():
+                depends = [
+                    i.split(" ")[0]
+                    for i in package_json["rendered_recipe"]["requirements"]["run"]
+                ]
             self.logger.info(f"Run-time dependencies for {package_requested}:")
-        elif mode == 'build':
-            if 'build' in package_json['rendered_recipe']['requirements'].keys():
-                depends_build = [i.split(" ")[0] for i in package_json['rendered_recipe']['requirements']['build']]
+        elif mode == "build":
+            if "build" in package_json["rendered_recipe"]["requirements"].keys():
+                depends_build = [
+                    i.split(" ")[0]
+                    for i in package_json["rendered_recipe"]["requirements"]["build"]
+                ]
             else:
                 depends_build = []
-            if 'host' in package_json['rendered_recipe']['requirements'].keys():
-                depends_host = [i.split(" ")[0] for i in package_json['rendered_recipe']['requirements']['host']]
+            if "host" in package_json["rendered_recipe"]["requirements"].keys():
+                depends_host = [
+                    i.split(" ")[0]
+                    for i in package_json["rendered_recipe"]["requirements"]["host"]
+                ]
             else:
                 depends_host = []
             depends = depends_host + depends_build
             self.logger.info(f"Build-time dependencies for {package_requested}:")
         else:
-            #TODO unhandled, but open to extension
+            # TODO unhandled, but open to extension
             ...
-        name_matches = [i for i in lin64_json_object["packages"] if self.get_package_name(i) in depends]
+        name_matches = [
+            i
+            for i in lin64_json_object["packages"]
+            if self.get_package_name(i) in depends
+        ]
 
         for j in depends:
             basic_matches = [i for i in name_matches if j in i]
-            is_python = [self.get_package_build(i) for i in basic_matches if "py" in self.get_package_build(i).split("cpython")[0].split("pypy")[0]]
-            if debug: self.logger.info(f"{j}: {is_python}")
-            if len(basic_matches)==0:
-                name_matches_noarch = [i for i in noarch_json_object["packages"] if self.get_package_name(i) in depends]
-                if len(name_matches_noarch)>0:
-                    self.logger.info(f"[bold green] {j} : noarch [/bold green]", extra={"markup": True})
-                else: 
-                    self.logger.info(f"[bold yellow] {j} : not found in linux-64 or noarch[/bold yellow]", extra={"markup": True})
-
-            elif len(is_python)>0:
-                self.logger.info(f"[bold green] {j} [/bold green]", extra={"markup": True})
+            is_python = [
+                self.get_package_build(i)
+                for i in basic_matches
+                if "py"
+                in self.get_package_build(i).split("cpython")[0].split("pypy")[0]
+            ]
+            if debug:
+                self.logger.info(f"{j}: {is_python}")
+            if len(basic_matches) == 0:
+                name_matches_noarch = [
+                    i
+                    for i in noarch_json_object["packages"]
+                    if self.get_package_name(i) in depends
+                ]
+                if len(name_matches_noarch) > 0:
+                    self.logger.info(
+                        f"[bold green] {j} : noarch [/bold green]",
+                        extra={"markup": True},
+                    )
+                else:
+                    self.logger.info(
+                        f"[bold yellow] {j} : not found in linux-64 or noarch[/bold yellow]",
+                        extra={"markup": True},
+                    )
+
+            elif len(is_python) > 0:
+                self.logger.info(
+                    f"[bold green] {j} [/bold green]", extra={"markup": True}
+                )
             else:
-                self.logger.info(f"[bold blue] {j} : non-python [/bold blue]", extra={"markup": True})
+                self.logger.info(
+                    f"[bold blue] {j} : non-python [/bold blue]", extra={"markup": True}
+                )
 
     def check_all_package_jsons(self, arch: str, package_requested: str):
         all_jsons = list(self.config.package_jsons.joinpath(arch).glob("*.json"))
         matches = []
-        
+
         for i in all_jsons:
-            with open(i, 'r') as read_file:
+            with open(i, "r") as read_file:
                 json_object = json.load(read_file)
-            build_match, host_match = self.check_build_host_depends(package_requested, json_object)
+            build_match, host_match = self.check_build_host_depends(
+                package_requested, json_object
+            )
             if len(build_match) > 0 or len(host_match) > 0:
                 matches.append(f"{i.name}: {set(build_match + host_match)}")
         return matches
 
     def convert_grayskull_map(self) -> Dict:
         """
         Converts a given grayskull map from pypi -> conda.
         """
         gs_mapping = {}
 
         if self.config.grayskull_map.exists():
-            with open(self.config.grayskull_map, 'r') as file:
+            with open(self.config.grayskull_map, "r") as file:
                 gs_map = yaml.safe_load(file)
 
             for i in gs_map:
-                if gs_map[i]['conda_name'] != gs_map[i]['pypi_name']:
-                    #self.logger.info(f"GRAYSKULL SWAP | ADD MORE DETAILED LOGS HERE")
-                    gs_mapping[gs_map[i]['pypi_name']] = gs_map[i]['conda_name']
+                if gs_map[i]["conda_name"] != gs_map[i]["pypi_name"]:
+                    # self.logger.info(f"GRAYSKULL SWAP | ADD MORE DETAILED LOGS HERE")
+                    gs_mapping[gs_map[i]["pypi_name"]] = gs_map[i]["conda_name"]
         else:
             self.logger.warn("LOAD ERROR | gs_mapping yaml did not load properly")
 
         return gs_mapping
 
-    def missed_and_matched_from_manifest(self, package_arch, python_version_requested, manifest_packages, drop_versions, json_object):
+    def missed_and_matched_from_manifest(
+        self,
+        package_arch,
+        python_version_requested,
+        manifest_packages,
+        drop_versions,
+        json_object,
+    ):
         """
         A gigantic loop to be moved outside of here, and to the cli.
         """
 
         if python_version_requested != None:
-            python_version_matches = {i:json_object["packages"][i] for i in json_object["packages"] if python_version_requested in i}
+            python_version_matches = {
+                i: json_object["packages"][i]
+                for i in json_object["packages"]
+                if python_version_requested in i
+            }
         else:
             python_version_matches = json_object["packages"]
-        
-        package_report = {
-                'matched_packages' : [],
-                'missed_packages' : []
-        }
+
+        package_report = {"matched_packages": [], "missed_packages": []}
 
         to_fetch: List[Tuple] = []
 
         converted_grayskull_map = self.convert_grayskull_map()
 
         for i in manifest_packages:
-
-            if ('pyVer' in i.keys()) and (i['pyVer'] == '2.7'):
-                self.logger.info("[bold yellow] Skipping [/bold yellow]" + f"{i['name']}=={i['version']}, pyVer = {i['pyVer']}", extra={"markup": True})
+            if ("pyVer" in i.keys()) and (i["pyVer"] == "2.7"):
+                self.logger.info(
+                    "[bold yellow] Skipping [/bold yellow]"
+                    + f"{i['name']}=={i['version']}, pyVer = {i['pyVer']}",
+                    extra={"markup": True},
+                )
                 continue
 
             i["name"] = self.convert_pypi_conda_name(i["name"], converted_grayskull_map)
-            i["name"] = i["name"].lower() #conda packages are all lower
+            i["name"] = i["name"].lower()  # conda packages are all lower
             i["version"] = i["version"].split("+")[0]
 
-            if i["version"] == "0.0.0" or len(i["version"].split("."))>3 or drop_versions:
+            if (
+                i["version"] == "0.0.0"
+                or len(i["version"].split(".")) > 3
+                or drop_versions
+            ):
                 i["version"] = ""
 
-            matched_package = self.match_versioned_packages(i["name"], i["version"], python_version_matches)
+            matched_package = self.match_versioned_packages(
+                i["name"], i["version"], python_version_matches
+            )
 
             if len(matched_package) > 0:
-                matched_package = self.choose_build(i["version"], matched_package, python_version_matches)
-                self.logger.info("[bold green] Match Found [/bold green]" + f"{i['name']}=={python_version_matches[matched_package]['version']}", extra={"markup": True})
-                payload = self.format_package_json(i["name"], matched_package, package_arch)
+                matched_package = self.choose_build(
+                    i["version"], matched_package, python_version_matches
+                )
+                self.logger.info(
+                    "[bold green] Match Found [/bold green]"
+                    + f"{i['name']}=={python_version_matches[matched_package]['version']}",
+                    extra={"markup": True},
+                )
+                payload = self.format_package_json(
+                    i["name"], matched_package, package_arch
+                )
                 to_fetch.append(payload)
 
-                package_report["matched_packages"].append({"name" : i['name'],"version" : i['version'] })
-
-            elif 'eggName' in i:
-
-                i["eggName"] = self.convert_pypi_conda_name(i["eggName"], converted_grayskull_map)
+                package_report["matched_packages"].append(
+                    {"name": i["name"], "version": i["version"]}
+                )
+
+            elif "eggName" in i:
+                i["eggName"] = self.convert_pypi_conda_name(
+                    i["eggName"], converted_grayskull_map
+                )
                 i["eggName"] = i["eggName"].lower()
 
-                matched_package = self.match_versioned_packages(i["eggName"], i["version"], python_version_matches)
+                matched_package = self.match_versioned_packages(
+                    i["eggName"], i["version"], python_version_matches
+                )
 
                 if len(matched_package) > 0:
-                    matched_package = self.choose_build(i["version"], matched_package, python_version_matches)
-                    self.logger.info("[bold green] Match Found [/bold green]" + f"{i['name']}/{i['eggName']}=={python_version_matches[matched_package]['version']}", extra={"markup": True})
-                    payload = self.format_package_json(i["eggName"], matched_package, package_arch)
+                    matched_package = self.choose_build(
+                        i["version"], matched_package, python_version_matches
+                    )
+                    self.logger.info(
+                        "[bold green] Match Found [/bold green]"
+                        + f"{i['name']}/{i['eggName']}=={python_version_matches[matched_package]['version']}",
+                        extra={"markup": True},
+                    )
+                    payload = self.format_package_json(
+                        i["eggName"], matched_package, package_arch
+                    )
                     to_fetch.append(payload)
 
-                    package_report["matched_packages"].append({"name" : i['eggName'],"version" : i['version'] })
+                    package_report["matched_packages"].append(
+                        {"name": i["eggName"], "version": i["version"]}
+                    )
 
                 else:
-                    package_report["missed_packages"].append({"name" : i['eggName'],"version" : i['version'] })
-                    self.logger.info("[bold red] No Matches [/bold red]" + f"{i['name']}/{i['eggName']}=={i['version']}", extra={"markup": True})
+                    package_report["missed_packages"].append(
+                        {"name": i["eggName"], "version": i["version"]}
+                    )
+                    self.logger.info(
+                        "[bold red] No Matches [/bold red]"
+                        + f"{i['name']}/{i['eggName']}=={i['version']}",
+                        extra={"markup": True},
+                    )
             else:
-                package_report["missed_packages"].append({"name" : i['name'],"version" : i['version'] })
-                self.logger.info("[bold red] No Matches [/bold red]"  + f"{i['name']}=={i['version']}", extra={"markup": True})
-        
-        #remove duplicates by converting list of dicts to set of dicts after serialization, then back to list. 
-        package_report["missed_packages"] = [json.loads(i) for i in set(json.dumps(i, sort_keys=True) for i in package_report["missed_packages"])]
-        package_report["matched_packages"] = [json.loads(i) for i in set(json.dumps(i, sort_keys=True) for i in package_report["matched_packages"])]
-
-        self.logger.info("[bold green] Total Matched: [/bold green]" + f"{len(package_report['matched_packages'])}", extra={"markup": True})
-        self.logger.info("[bold red] Total Missed: [/bold red]" + f"{len(package_report['missed_packages'])}", extra={"markup": True})
+                package_report["missed_packages"].append(
+                    {"name": i["name"], "version": i["version"]}
+                )
+                self.logger.info(
+                    "[bold red] No Matches [/bold red]"
+                    + f"{i['name']}=={i['version']}",
+                    extra={"markup": True},
+                )
+
+        # remove duplicates by converting list of dicts to set of dicts after serialization, then back to list.
+        package_report["missed_packages"] = [
+            json.loads(i)
+            for i in set(
+                json.dumps(i, sort_keys=True) for i in package_report["missed_packages"]
+            )
+        ]
+        package_report["matched_packages"] = [
+            json.loads(i)
+            for i in set(
+                json.dumps(i, sort_keys=True)
+                for i in package_report["matched_packages"]
+            )
+        ]
+
+        self.logger.info(
+            "[bold green] Total Matched: [/bold green]"
+            + f"{len(package_report['matched_packages'])}",
+            extra={"markup": True},
+        )
+        self.logger.info(
+            "[bold red] Total Missed: [/bold red]"
+            + f"{len(package_report['missed_packages'])}",
+            extra={"markup": True},
+        )
         unique_missed = len(set([i["name"] for i in package_report["missed_packages"]]))
-        self.logger.info("[bold red] Unique Missed: [/bold red]" + f"{unique_missed}", extra={"markup": True})
-
-        output = self.config.package_reports.joinpath(pathlib.Path(f'cfgraph-package-report-{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'))
-        to_fetch_output = self.config.package_reports.joinpath(pathlib.Path(f'cfgraph-to-fetch-{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'))
+        self.logger.info(
+            "[bold red] Unique Missed: [/bold red]" + f"{unique_missed}",
+            extra={"markup": True},
+        )
+
+        output = self.config.package_reports.joinpath(
+            pathlib.Path(
+                f'cfgraph-package-report-{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
+            )
+        )
+        to_fetch_output = self.config.package_reports.joinpath(
+            pathlib.Path(
+                f'cfgraph-to-fetch-{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
+            )
+        )
 
         with open(output, "w") as write_file:
-           json.dump(package_report, write_file)
-           self.logger.info(f'[bold pink] WROTE SUMMARY FILE: ' + '[/bold pink]' + str(output), extra={"markup": True})
+            json.dump(package_report, write_file)
+            self.logger.info(
+                f"[bold pink] WROTE SUMMARY FILE: " + "[/bold pink]" + str(output),
+                extra={"markup": True},
+            )
 
-        #with open(to_fetch_output, "w") as write_file:
+        # with open(to_fetch_output, "w") as write_file:
         #   json.dump(d, write_file)
         #   self.logger.info(f'[bold pink] WROTE MATCH CACHE: ' + '[/bold pink]' + str(to_fetch_output), extra={"markup": True})
 
         ##need a test to verify downloads succeed - config_folder_var.joinpath("jsons").joinpath(package_arch).joinpath(matched_package.replace(".tar.bz2",".json"))
-        
+
         return set(to_fetch)
```

## rafe/cli.py

```diff
@@ -1,272 +1,364 @@
 import typing
 import pathlib
-import sys
 import json
 import typer
 
-import requests
-from urllib.request import urlopen
 from datetime import datetime
 
 from rich import print
 from rich.console import Console
 from rich.table import Table
 from rich.progress import Progress
-from rich.json import JSON
 from rich.prompt import Prompt
 
 from rafe import __version__
 from rafe.cfgraph import CFGraph
 from rafe.build import build_package
 from rafe.config import create_app_dirs
 from rafe.logger import logger, setFileHandle
 
-from rafe.plugin import RafePlugin, RafePluginManager
+from rafe.plugin import RafePluginManager
 
-from rafe.cli_api import load_repodata, update_all_repodata, fetch_repodata_file, fetch_all_package_jsons
+from rafe.cli_api import load_repodata, update_all_repodata, fetch_all_package_jsons
 
 app = typer.Typer(rich_markup_mode="rich", add_completion=False)
 
-if pathlib.Path.home().joinpath('.rafe', 'plugins_conf.json').exists():
-    rpm = RafePluginManager(app, pathlib.Path.home().joinpath('.rafe', 'plugins_conf.json') ,logger)
- 
+if pathlib.Path.home().joinpath(".rafe", "plugins_conf.json").exists():
+    rpm = RafePluginManager(
+        app, pathlib.Path.home().joinpath(".rafe", "plugins_conf.json"), logger
+    )
+
 
 def callback_get_version(value: bool):
     """
     prints the current verson of rafe used
     """
     if value:
-        print(f':rocket: Rafe Version: [bold green]{__version__}[/bold green]')
+        print(f":rocket: Rafe Version: [bold green]{__version__}[/bold green]")
         raise typer.Exit()
 
+
 def callback_config_init(init: bool):
     """
     initializes the folders that are required for rafe to run
     """
     if init:
         create_app_dirs()
         raise typer.Exit()
 
+
 search_options = {
     "python_version": typer.Option("py37", "--pyV", help="specify a python version"),
-    "package_name": typer.Option(..., "--package", help="search for existance of conda-forge package name"),
-    "package_arch": typer.Option("linux-64", "--arch", help="specify an architecture type"),
+    "package_name": typer.Option(
+        ..., "--package", help="search for existance of conda-forge package name"
+    ),
+    "package_arch": typer.Option(
+        "linux-64", "--arch", help="specify an architecture type"
+    ),
     "package_version": typer.Option(None, "--version", help="specify package version"),
     "fetch": typer.Option(None, "--fetch", help="specify package version"),
 }
 
+
 @app.command(rich_help_panel="Inspect")
-def search(python_version: str = search_options["python_version"], package_name: str = search_options["package_name"], package_version: str = search_options["package_version"], package_arch: str = search_options["package_arch"],fetch: typing.Optional[bool] = search_options["fetch"]):
+def search(
+    python_version: str = search_options["python_version"],
+    package_name: str = search_options["package_name"],
+    package_version: str = search_options["package_version"],
+    package_arch: str = search_options["package_arch"],
+    fetch: typing.Optional[bool] = search_options["fetch"],
+):
     """
     Given the arguments, this searches for packages across conda-forge, matches them with any given input, and returns them.
     """
-    def match_packages(cfgraph: CFGraph, repodata_object: str, progress: Progress) -> typing.List:
+
+    def match_packages(
+        cfgraph: CFGraph, repodata_object: str, progress: Progress
+    ) -> typing.List:
         """
         Matches Packages, and returns the set of matched possibilities.
         """
-        matched_packages = cfgraph.match_packages(package_name, package_version, python_version, package_arch, repodata_object)
+        matched_packages = cfgraph.match_packages(
+            package_name, package_version, python_version, package_arch, repodata_object
+        )
         progress.update(task_id, advance=1)
         return matched_packages
-    
 
     cfgraph = CFGraph(logger=logger)
 
     total_steps = 8
-    # Add conditional step to fetch 
+    # Add conditional step to fetch
     if fetch is True:
         total_steps += 1
 
     with Progress() as progress:
-        task_id = progress.add_task("[cyan] Total Search progress: [/cyan]", total=total_steps)
+        task_id = progress.add_task(
+            "[cyan] Total Search progress: [/cyan]", total=total_steps
+        )
 
         # Update Repodata; Total progress advances 6
         update_all_repodata(cfgraph, task_id, progress)
 
         # Load Repodata; Total progress advance 1
         json_object = load_repodata(cfgraph, task_id, progress, package_arch)
-        
+
         # Search conda-forge for all possible package matches; Total progress advance 1
-        matched_packages = match_packages(cfgraph, json_object, progress)   
+        matched_packages = match_packages(cfgraph, json_object, progress)
 
         # TODO: Implement package filtering here
         logger.info(matched_packages)
 
         if fetch is True:
             cfgraph.fetch_package_json(package_name, matched_packages[0], package_arch)
-            progress.update(task_id,advance = 1)
+            progress.update(task_id, advance=1)
 
     return
 
+
 depends_options = {
     "package": typer.Option(..., "--package", help="target package to check"),
-	"debug": typer.Option(None, "--debug", help="print matched build tags"),
+    "debug": typer.Option(None, "--debug", help="print matched build tags"),
     "run": typer.Option(None, "--run", help="run check for runtime depends"),
-    "build": typer.Option(None, "--build", help="run check for buildtime depends")
+    "build": typer.Option(None, "--build", help="run check for buildtime depends"),
 }
 
+
 @app.command(rich_help_panel="Inspect")
-def depends(package: str = depends_options["package"], debug: typing.Optional[bool] = depends_options["debug"], run_flag: typing.Optional[bool] = depends_options["run"], build_flag: typing.Optional[bool] = depends_options["build"]):
+def depends(
+    package: str = depends_options["package"],
+    debug: typing.Optional[bool] = depends_options["debug"],
+    run_flag: typing.Optional[bool] = depends_options["run"],
+    build_flag: typing.Optional[bool] = depends_options["build"],
+):
     """
     Verify runtime dependencies for a supplied package
     """
     if not (run_flag or build_flag):
-        logger.info("Please supply either --run or --build to specify which dependencies to check")
+        logger.info(
+            "Please supply either --run or --build to specify which dependencies to check"
+        )
         return
     cfgraph = CFGraph(logger=logger)
     lin64_json_object = cfgraph.load_repodata(arch="linux-64")
     noarch_json_object = cfgraph.load_repodata(arch="noarch")
     if run_flag:
-        cfgraph.check_package_build(package, lin64_json_object, noarch_json_object, 'run', debug)
+        cfgraph.check_package_build(
+            package, lin64_json_object, noarch_json_object, "run", debug
+        )
     elif build_flag:
-        cfgraph.check_package_build(package, lin64_json_object, noarch_json_object, 'build', debug)
+        cfgraph.check_package_build(
+            package, lin64_json_object, noarch_json_object, "build", debug
+        )
     else:
-        logger.info("Please supply either --run or --build to specify which dependencies to check")
+        logger.info(
+            "Please supply either --run or --build to specify which dependencies to check"
+        )
     return
 
+
 manifest_fetch_options = {
     "manifest": typer.Option(..., "--manifest", help="path to manifest json file"),
     "arch": typer.Option("linux-64", "--arch", help="specific architecture to target"),
-    "drop_versions": typer.Option(None, "--drop-versions", help="drop package version requirements in match"),
-    "from_cache": typer.Option(None, "--from-cache", help="load the last used matches from missed and matched"),
+    "drop_versions": typer.Option(
+        None, "--drop-versions", help="drop package version requirements in match"
+    ),
+    "from_cache": typer.Option(
+        None, "--from-cache", help="load the last used matches from missed and matched"
+    ),
 }
 
+
 @app.command(rich_help_panel="Inspect")
-def manifest_fetch(manifest: pathlib.Path = manifest_fetch_options["manifest"],
-                   arch: str = manifest_fetch_options["arch"],
-                   from_cache: typing.Optional[bool] = manifest_fetch_options["from_cache"],
-                   drop_versions: typing.Optional[bool] = manifest_fetch_options["drop_versions"] ):
+def manifest_fetch(
+    manifest: pathlib.Path = manifest_fetch_options["manifest"],
+    arch: str = manifest_fetch_options["arch"],
+    from_cache: typing.Optional[bool] = manifest_fetch_options["from_cache"],
+    drop_versions: typing.Optional[bool] = manifest_fetch_options["drop_versions"],
+):
     """
     Takes formatted JSON manifest and caches matched libcfgraph artifacts
     """
     cfgraph = CFGraph(logger=logger)
-    total_steps = 9 
+    total_steps = 9
 
     with Progress() as progress:
-        task_id = progress.add_task("[cyan] Total Package Match Progress: [/cyan]", total=total_steps)
+        task_id = progress.add_task(
+            "[cyan] Total Package Match Progress: [/cyan]", total=total_steps
+        )
 
         if manifest.exists():
-            package_arch, python_version_requested, manifest_packages = cfgraph.read_manifest(manifest)
-            progress.update(task_id, advance = 1)
+            (
+                package_arch,
+                python_version_requested,
+                manifest_packages,
+            ) = cfgraph.read_manifest(manifest)
+            progress.update(task_id, advance=1)
 
             update_all_repodata(cfgraph, task_id, progress)
 
             json_object = load_repodata(cfgraph, task_id, progress, arch)
 
             if arch != "linux-64":
                 package_arch = arch
             if package_arch == "noarch":
                 python_version_requested = None
 
             if from_cache is True:
                 to_fetch = cfgraph.read_missed_and_matched_from_cache()
             else:
-                to_fetch = cfgraph.missed_and_matched_from_manifest(package_arch, python_version_requested, manifest_packages, drop_versions, json_object)
-            
-            progress.update(task_id, advance = 1)
+                to_fetch = cfgraph.missed_and_matched_from_manifest(
+                    package_arch,
+                    python_version_requested,
+                    manifest_packages,
+                    drop_versions,
+                    json_object,
+                )
 
-            logger.info(f'Fetching package jsons for matched {len(to_fetch)}')
+            progress.update(task_id, advance=1)
 
-            fetch_all_package_jsons(to_fetch, progress)
+            logger.info(f"Fetching package jsons for matched {len(to_fetch)}")
 
+            fetch_all_package_jsons(to_fetch, progress)
 
         else:
-            logger.error('The provided manifest file does not exist or could not be found.') 
+            logger.error(
+                "The provided manifest file does not exist or could not be found."
+            )
             raise typer.Exit()
 
 
 depends_on_options = {
-    "package_name": typer.Option(..., "--package", help="check all cached json files for inclusion of given package in build dependency"),
-    "arch" : typer.Option("linux-64", "--arch", help="specified architecture to generate dependencies for"), 
-    "output": typer.Option(..., "-o", help="Path of where to place the resulting query's output file")
+    "package_name": typer.Option(
+        ...,
+        "--package",
+        help="check all cached json files for inclusion of given package in build dependency",
+    ),
+    "arch": typer.Option(
+        "linux-64", "--arch", help="specified architecture to generate dependencies for"
+    ),
+    "output": typer.Option(
+        ..., "-o", help="Path of where to place the resulting query's output file"
+    ),
 }
 
+
 @app.command(rich_help_panel="Inspect")
 def depends_on(package_name: str = depends_on_options["package_name"]):
     """
     Searches cached jsons for build-time dependency on given package
     """
     cfgraph = CFGraph(logger=logger)
     matches = cfgraph.check_all_package_jsons("linux-64", package_name)
     print(matches)
     return
 
+
 convert_missed_options = {
     "path": typer.Option(..., "--path", help="path to json produced by rafe"),
-	"arch": typer.Option("linux-64", "--arch", help="specified architecture to set in new manifest"),
-	"pyVer": typer.Option(None, "--pyVer", help="specified python version to set in new manifest"),
-	"output": typer.Option(".", "--output", help="directory location to write output file")
+    "arch": typer.Option(
+        "linux-64", "--arch", help="specified architecture to set in new manifest"
+    ),
+    "pyVer": typer.Option(
+        None, "--pyVer", help="specified python version to set in new manifest"
+    ),
+    "output": typer.Option(
+        ".", "--output", help="directory location to write output file"
+    ),
 }
 
+
 @app.command(rich_help_panel="Inspect")
-def convert_missed(json_path: pathlib.Path = convert_missed_options["path"], arch: str = convert_missed_options["arch"], pyVer: str = convert_missed_options["pyVer"]):
+def convert_missed(
+    json_path: pathlib.Path = convert_missed_options["path"],
+    arch: str = convert_missed_options["arch"],
+    pyVer: str = convert_missed_options["pyVer"],
+):
     """
     Creates a new manifest based on the missed_packages from a previous manifest match json
     """
     if arch == "noarch":
         pyVer = ""
-	
+
     if (pyVer != None) and (pyVer != ""):
         pyVer = "".join(pyVer.split("."))
         if pyVer[:2] != "py":
-            pyver = "py"+pyVer
-    if len(pyVer)>4:
+            pyver = "py" + pyVer
+    if len(pyVer) > 4:
         logger.error("PyVer format should be 'py37' or '3.7' or '37'")
         raise typer.Exit()
 
     if json_path.exists():
-        with open(json_path, 'r') as f:
+        with open(json_path, "r") as f:
             manifest_json = json.load(f)
     else:
         raise FileNotFound()
 
-    converted = {"python_version_requested": pyVer, "package_arch": arch, "packages": []}
+    converted = {
+        "python_version_requested": pyVer,
+        "package_arch": arch,
+        "packages": [],
+    }
     converted["packages"] = manifest_json["missed_packages"]
     if json_path.parent.is_dir():
-        file_name = f"converted_missed_{arch}_{pyVer}_{datetime.now().strftime('%H%M%S')}.json"
+        file_name = (
+            f"converted_missed_{arch}_{pyVer}_{datetime.now().strftime('%H%M%S')}.json"
+        )
         output = json_path.parent.joinpath(file_name)
         try:
-            with open(output, 'w') as write_file:
+            with open(output, "w") as write_file:
                 json.dump(converted, write_file)
             logger.info(f"Converted json written to: {str(output)}")
         except:
             logger.error("Error writing to file")
             raise typer.Exit()
     else:
-        logger.error("Parent path of input file not a directory, aborting without specified output dir.")
+        logger.error(
+            "Parent path of input file not a directory, aborting without specified output dir."
+        )
         raise typer.Exit()
     return
 
+
 build_options = {
-    "recipe_dir": typer.Option(..., "--recipe-dir", help="path to build recipies for packages"),
-    "package": typer.Option(..., "--package", help="name of package to build")
+    "recipe_dir": typer.Option(
+        ..., "--recipe-dir", help="path to build recipies for packages"
+    ),
+    "package": typer.Option(..., "--package", help="name of package to build"),
 }
 
+
 @app.command(rich_help_panel="Build")
-def build(recipe_dir: pathlib.Path = build_options["recipe_dir"], package: pathlib.Path = build_options["package"]):
+def build(
+    recipe_dir: pathlib.Path = build_options["recipe_dir"],
+    package: pathlib.Path = build_options["package"],
+):
     """
-    Builds a python wheel given a directory and recipe 
+    Builds a python wheel given a directory and recipe
     """
     package_path = pathlib.Path.joinpath(recipe_dir, package)
 
     if package_path.exists():
         result = build_package(package_path)
         if result == 0:
             raise typer.Exit()
     else:
         raise FileNotFound()
 
+
 def callback_plugin_add(path):
     if isinstance(path, pathlib.Path) and path.exists():
         absolute_path = path.resolve()
         name = path.name
-        entrypoint = Prompt.ask("Please Define an entrypoint; This is the function that will be used like rafe <function>")
+        entrypoint = Prompt.ask(
+            "Please Define an entrypoint; This is the function that will be used like rafe <function>"
+        )
         rpm.add_plugin(name, absolute_path, entrypoint)
     else:
-        #TODO Investigate why this gets called back.
+        # TODO Investigate why this gets called back.
         ...
 
 
 def callback_plugin_list():
     p = rpm.list_plugins()
     table = Table(title="Rafe Plugins")
 
@@ -277,55 +369,85 @@
     for r in p:
         table.add_row(r["name"], r["entrypoint"], r["path"])
 
     console = Console()
     console.print(table)
     return
 
+
 plugin_configuration_options = {
-    "add": typer.Option(None, "--add", help="Add a plugin given a folder", callback=callback_plugin_add),
-    "list": typer.Option(None, "--list", help="Print the current plugin configuration", callback=callback_plugin_list),
+    "add": typer.Option(
+        None, "--add", help="Add a plugin given a folder", callback=callback_plugin_add
+    ),
+    "list": typer.Option(
+        None,
+        "--list",
+        help="Print the current plugin configuration",
+        callback=callback_plugin_list,
+    ),
     "generate-cli-template": "",
-    "clean-cache": typer.Option(None, "--clean-cache", help="Cleans the plugin cache", callback=None),
-    "show-cache": typer.Option(None, "--show-cache", help="Look at the currently cached objects", callback=None),
+    "clean-cache": typer.Option(
+        None, "--clean-cache", help="Cleans the plugin cache", callback=None
+    ),
+    "show-cache": typer.Option(
+        None, "--show-cache", help="Look at the currently cached objects", callback=None
+    ),
 }
 
+
 @app.command(rich_help_panel="Utilities")
-def plugin(add: typing.Optional[pathlib.Path] = plugin_configuration_options["add"],
-           lst: typing.Optional[bool] = plugin_configuration_options["list"],
-           clean_cache: typing.Optional[bool] = plugin_configuration_options["clean-cache"],
-           show_cache: typing.Optional[bool] = plugin_configuration_options["show-cache"]):
+def plugin(
+    add: typing.Optional[pathlib.Path] = plugin_configuration_options["add"],
+    lst: typing.Optional[bool] = plugin_configuration_options["list"],
+    clean_cache: typing.Optional[bool] = plugin_configuration_options["clean-cache"],
+    show_cache: typing.Optional[bool] = plugin_configuration_options["show-cache"],
+):
     """
     Manages rafe plugins and configuration.
     """
     return
 
+
 configuration_options = {
-    "init": typer.Option(None, "--init", help="initializes a configuration with sane defaults", callback=callback_config_init, is_eager=True),
+    "init": typer.Option(
+        None,
+        "--init",
+        help="initializes a configuration with sane defaults",
+        callback=callback_config_init,
+        is_eager=True,
+    ),
 }
 
+
 @app.command(rich_help_panel="Utilities")
 def config(init: typing.Optional[bool] = configuration_options["init"]):
     """
     Manages the rafe configuration. Run `--init` to get started!
     """
     return
 
+
 main_options = {
-    "version": typer.Option(None, "--version", help="prints the current version", callback=callback_get_version, is_eager=True),
-    }
+    "version": typer.Option(
+        None,
+        "--version",
+        help="prints the current version",
+        callback=callback_get_version,
+        is_eager=True,
+    ),
+}
+
 
 @app.callback()
-def main(
-    version: typing.Optional[bool] = main_options["version"] 
-    ):
+def main(version: typing.Optional[bool] = main_options["version"]):
     """
-    Rafe is a build tool for python 
+    Rafe is a build tool for python
     """
     return
 
+
 def run():
-    if pathlib.Path.home().joinpath('.rafe').exists():
-        setFileHandle(logger, pathlib.Path.home().joinpath('.rafe', '.cfcache', 'package_reports'))
+    if pathlib.Path.home().joinpath(".rafe").exists():
+        setFileHandle(
+            logger, pathlib.Path.home().joinpath(".rafe", ".cfcache", "package_reports")
+        )
     app(prog_name="rafe")
-
-
```

## rafe/cli_api.py

```diff
@@ -7,188 +7,265 @@
 import aiofiles
 
 from urllib.request import urlopen
 from rich import print
 from rich.progress import Progress
 from rafe.cfgraph import CFGraph
 
-log_color = lambda color, hl_string, string: f'[bold {color}]' + hl_string + '[/bold' + f' {color}] | ' + str(string)
+log_color = (
+    lambda color, hl_string, string: f"[bold {color}]"
+    + hl_string
+    + "[/bold"
+    + f" {color}] | "
+    + str(string)
+)
 
-def load_repodata(cfgraph: CFGraph, task_id, progress: Progress, arch: str) -> typing.Union[str, bool]:
+
+def load_repodata(
+    cfgraph: CFGraph, task_id, progress: Progress, arch: str
+) -> typing.Union[str, bool]:
     """
     A wrapper function to load repodata objects into a collection of JSON formatted strings.
     """
 
     # win64_json_object = cfgraph.load_repodata_jsons(arch="win64")
 
     if arch == "noarch":
         noarch_json_object = cfgraph.load_repodata(arch="noarch")
         progress.update(task_id, advance=1)
         return noarch_json_object
 
     if arch == "linux-64":
-        linux64_json_object = cfgraph.load_repodata(arch="linux-64") 
+        linux64_json_object = cfgraph.load_repodata(arch="linux-64")
         progress.update(task_id, advance=1)
         return linux64_json_object
 
     if arch == "win64":
-        cfgraph.logger.error("Windows 64-Bit is unsupported. Please raise an issue with the developers if you would like to see this.")
+        cfgraph.logger.error(
+            "Windows 64-Bit is unsupported. Please raise an issue with the developers if you would like to see this."
+        )
 
     if arch == "win32":
-        cfgraph.logger.error("Windows 64-Bit is unsupported. Please raise an issue with the developers if you would like to see this.")
-    
+        cfgraph.logger.error(
+            "Windows 64-Bit is unsupported. Please raise an issue with the developers if you would like to see this."
+        )
+
     return False
 
+
 def update_all_repodata(cfgraph: CFGraph, task_id, progress: Progress):
     """
     A wrapper function which calls the update_repodata_arch function in cfgraph.py
     Refactor this later, pressed on time lol.
     """
     # grayskull
     if not cfgraph.check_update_repodata(cfgraph.config.grayskull_map):
-        cfgraph.logger.info(log_color('green', 'CACHED', cfgraph.config.grayskull_map), extra = {"markup": True})
+        cfgraph.logger.info(
+            log_color("green", "CACHED", cfgraph.config.grayskull_map),
+            extra={"markup": True},
+        )
         progress.update(task_id, advance=2)
     else:
-        fetch_repodata_file(cfgraph.urls.grayskull_map, cfgraph.config.grayskull_map, progress)
+        fetch_repodata_file(
+            cfgraph.urls.grayskull_map, cfgraph.config.grayskull_map, progress
+        )
         progress.update(task_id, advance=2)
 
     # linux-64
     if not cfgraph.check_update_repodata(cfgraph.config.linux64_json):
-        cfgraph.logger.info(log_color('green', 'CACHED', cfgraph.config.linux64_json), extra={"markup": True}) 
+        cfgraph.logger.info(
+            log_color("green", "CACHED", cfgraph.config.linux64_json),
+            extra={"markup": True},
+        )
         progress.update(task_id, advance=2)
     else:
-        fetch_repodata_file(cfgraph.urls.linux64_repodata, cfgraph.config.linux64_json, progress)
+        fetch_repodata_file(
+            cfgraph.urls.linux64_repodata, cfgraph.config.linux64_json, progress
+        )
         progress.update(task_id, advance=2)
 
     # noarch
     if not cfgraph.check_update_repodata(cfgraph.config.noarch_json):
-        cfgraph.logger.info(log_color('green', 'CACHED', cfgraph.config.noarch_json), extra = {"markup": True}) 
+        cfgraph.logger.info(
+            log_color("green", "CACHED", cfgraph.config.noarch_json),
+            extra={"markup": True},
+        )
         progress.update(task_id, advance=2)
     else:
-        fetch_repodata_file(cfgraph.urls.noarch_repodata, cfgraph.config.noarch_json, progress)
+        fetch_repodata_file(
+            cfgraph.urls.noarch_repodata, cfgraph.config.noarch_json, progress
+        )
         progress.update(task_id, advance=2)
 
+
 def fetch_all_package_jsons(to_fetch: typing.List[typing.Tuple], progress: Progress):
     sem = asyncio.BoundedSemaphore(10)
-    #session = aiohttp.ClientSession(trust_env=True)
+    # session = aiohttp.ClientSession(trust_env=True)
 
     async def fetch_file(url: str, destination: pathlib.Path):
         async with sem, aiohttp.ClientSession(trust_env=True) as session:
             async with session.get(url) as response:
-                print(f'Get {url}')
+                print(f"Get {url}")
                 """
                 total_size = int(response.headers.get("Content-Length", 0))
                 if total_size == 0:
                     site = urlopen(url)
                     meta = site.info()
                     total_size = int(meta["Content-Length"])
                 """
                 assert response.status == 200
                 data = await response.read()
-            #print(type(data), type(response))
-
+            # print(type(data), type(response))
 
         async with aiofiles.open(destination, "wb") as out:
-            print(f'Writing | {destination.name}')
+            print(f"Writing | {destination.name}")
             await out.write(data)
 
     loop = asyncio.get_event_loop()
     tasks = [loop.create_task(fetch_file(i[0], i[1])) for i in to_fetch]
     loop.run_until_complete(asyncio.wait(tasks))
     loop.close()
     return
 
+
 def fetch_repodata_file(url: str, destination: pathlib.Path, progress: Progress):
     """
     Fetches the repodata files with special streaming with the URL. Saves it to the
-    filepath defined by destination. 
+    filepath defined by destination.
     """
     response = requests.get(url, stream=True)
     response.raise_for_status()
 
     total_size = int(response.headers.get("Content-Length", 0))
     if total_size == 0:
         site = urlopen(url)
         meta = site.info()
         total_size = int(meta["Content-Length"])
 
-
-    task_id = progress.add_task(f"[cyan] Get {destination.name} ({total_size/(1024*1024):.2f}Mb):" + " [/cyan]", total=total_size)
+    task_id = progress.add_task(
+        f"[cyan] Get {destination.name} ({total_size/(1024*1024):.2f}Mb):" + " [/cyan]",
+        total=total_size,
+    )
 
     with open(destination, "wb") as file:
         for chunk in response.iter_content(chunk_size=8192):
             file.write(chunk)
             progress.update(task_id, advance=len(chunk))
 
+
 def match_nearest_version(how: str = "package") -> None:
     """
     Matches the nearest version found for a missed package depending on the specified parameter.
 
     @param how -> str either set to "package" or "pyVer"
     """
 
     pass
 
-def missed_and_matched_from_manifest(cfgraph, package_arch, python_version_requested, manifest_packages, drop_versions, json_object):
+
+def missed_and_matched_from_manifest(
+    cfgraph,
+    package_arch,
+    python_version_requested,
+    manifest_packages,
+    drop_versions,
+    json_object,
+):
     """
     A gigantic loop to be moved outside of here, and to the cli.
     """
 
     if python_version_requested != None:
-        python_version_matches = [i for i in json_object["packages"] if python_version_requested in i]
+        python_version_matches = [
+            i for i in json_object["packages"] if python_version_requested in i
+        ]
 
     else:
         python_version_matches = json_object["packages"]
-    
-    package_report = {
-            'matched_packages' : [],
-            'missed_packages' : []
-    }
+
+    package_report = {"matched_packages": [], "missed_packages": []}
 
     converted_grayskull_map = cfgraph.convert_grayskull_map()
 
     for i in manifest_packages:
-
-        if ('pyVer' in i.keys()) and (i['pyVer'] == '2.7'):
-            cfgraph.logger.info("[bold yellow] Skipping [/bold yellow]" + f"{i['name']}=={i['version']}, pyVer = {i['pyVer']}", extra={"markup": True})
+        if ("pyVer" in i.keys()) and (i["pyVer"] == "2.7"):
+            cfgraph.logger.info(
+                "[bold yellow] Skipping [/bold yellow]"
+                + f"{i['name']}=={i['version']}, pyVer = {i['pyVer']}",
+                extra={"markup": True},
+            )
             continue
 
         i["name"] = cfgraph.convert_pypi_conda_name(i["name"], converted_grayskull_map)
-        i["name"] = i["name"].lower() #conda packages are all lower
+        i["name"] = i["name"].lower()  # conda packages are all lower
         i["version"] = i["version"].split("+")[0]
 
-        if i["version"] == "0.0.0" or len(i["version"].split("."))>3 or drop_versions:
+        if i["version"] == "0.0.0" or len(i["version"].split(".")) > 3 or drop_versions:
             i["version"] = ""
 
-        matched_package = cfgraph.match_versioned_packages(i["name"], i["version"], python_version_matches)
+        matched_package = cfgraph.match_versioned_packages(
+            i["name"], i["version"], python_version_matches
+        )
 
         if len(matched_package) > 0:
             breakpoint()
             cfgraph.fetch_package_json(i["name"], matched_package[0], package_arch)
-            package_report["matched_packages"].append({"name" : i['name'],"version" : i['version'] })
-            cfgraph.logger.info("[bold green] Match Found [/bold green]" + f"{i['name']}=={i['version']}", extra={"markup": True})
-
-        elif 'eggName' in i:
-
-            i["eggName"] = cfgraph.convert_pypi_conda_name(i["eggName"], converted_grayskull_map)
+            package_report["matched_packages"].append(
+                {"name": i["name"], "version": i["version"]}
+            )
+            cfgraph.logger.info(
+                "[bold green] Match Found [/bold green]"
+                + f"{i['name']}=={i['version']}",
+                extra={"markup": True},
+            )
+
+        elif "eggName" in i:
+            i["eggName"] = cfgraph.convert_pypi_conda_name(
+                i["eggName"], converted_grayskull_map
+            )
             i["eggName"] = i["eggName"].lower()
 
-            matched_package = cfgraph.match_versioned_packages(i["eggName"], i["version"], python_version_matches)
+            matched_package = cfgraph.match_versioned_packages(
+                i["eggName"], i["version"], python_version_matches
+            )
 
             if len(matched_package) > 0:
-                cfgraph.fetch_package_json(i["eggName"], matched_package[0], package_arch)
-                package_report["matched_packages"].append({"name" : i['name'],"version" : i['version'] })
-                cfgraph.logger.info("[bold green] Match Found [/bold green]" + f"{i['name']}/{i['eggName']}=={i['version']}", extra={"markup": True})
+                cfgraph.fetch_package_json(
+                    i["eggName"], matched_package[0], package_arch
+                )
+                package_report["matched_packages"].append(
+                    {"name": i["name"], "version": i["version"]}
+                )
+                cfgraph.logger.info(
+                    "[bold green] Match Found [/bold green]"
+                    + f"{i['name']}/{i['eggName']}=={i['version']}",
+                    extra={"markup": True},
+                )
 
             else:
-                package_report["missed_packages"].append({"name" : i['name'],"version" : i['version'] })
-                cfgraph.logger.info("[bold red] No Matches [/bold red]" + f"{i['name']}/{i['eggName']}=={i['version']}", extra={"markup": True})
+                package_report["missed_packages"].append(
+                    {"name": i["name"], "version": i["version"]}
+                )
+                cfgraph.logger.info(
+                    "[bold red] No Matches [/bold red]"
+                    + f"{i['name']}/{i['eggName']}=={i['version']}",
+                    extra={"markup": True},
+                )
         else:
-            package_report["missed_packages"].append({"name" : i['name'],"version" : i['version'] })
-            cfgraph.logger.info("[bold red] No Matches [/bold red]"  + f"{i['name']}=={i['version']}", extra={"markup": True})
-    
-    output = cfgraph.config.package_reports.joinpath(pathlib.Path(f'cfgraph-package-report-{datetime.now()}.json'))
+            package_report["missed_packages"].append(
+                {"name": i["name"], "version": i["version"]}
+            )
+            cfgraph.logger.info(
+                "[bold red] No Matches [/bold red]" + f"{i['name']}=={i['version']}",
+                extra={"markup": True},
+            )
+
+    output = cfgraph.config.package_reports.joinpath(
+        pathlib.Path(f"cfgraph-package-report-{datetime.now()}.json")
+    )
 
     with open(output, "w") as write_file:
-       json.dump(package_report, write_file)
-       cfgraph.logger.info(f'[bold pink] WROTE SUMMARY FILE: ' + '[/bold pink]' + str(output), extra={"markup": True})
-
+        json.dump(package_report, write_file)
+        cfgraph.logger.info(
+            f"[bold pink] WROTE SUMMARY FILE: " + "[/bold pink]" + str(output),
+            extra={"markup": True},
+        )
```

## rafe/config.py

```diff
@@ -1,66 +1,96 @@
 import pydantic
 import pathlib
 import json
 
 from rafe.logger import logger
 from os.path import dirname, join
 
+
 class RafeConfig(pydantic.BaseModel):
     pass
 
+
 def create_rafe_rootdir(rootdir: pathlib.Path) -> None:
-    """
-    """
+    """ """
     if not rootdir.exists():
         rootdir.mkdir(parents=True, exist_ok=True)
-        logger.info('[bold green] CREATE DIRECTORY: [/bold green]'  + str(rootdir), extra={"markup": True}) 
+        logger.info(
+            "[bold green] CREATE DIRECTORY: [/bold green]" + str(rootdir),
+            extra={"markup": True},
+        )
     else:
-        logger.warn('[bold yellow] DIRECOTRY EXISTS: [/bold yellow]'  + str(rootdir), extra={"markup": True}) 
-    return 
+        logger.warn(
+            "[bold yellow] DIRECOTRY EXISTS: [/bold yellow]" + str(rootdir),
+            extra={"markup": True},
+        )
+    return
+
 
 def create_cfcache_subfolder(rootdir: pathlib.Path) -> None:
     """
     Given a root directory, setup the folder structure for cfgraph
     """
-    folders = ['.cfcache/', '.cfcache/jsons/', '.cfcache/package_reports', '.cfcache/jsons/linux-64', '.cfcache/jsons/noarch']
+    folders = [
+        ".cfcache/",
+        ".cfcache/jsons/",
+        ".cfcache/package_reports",
+        ".cfcache/jsons/linux-64",
+        ".cfcache/jsons/noarch",
+    ]
     for sf in folders:
         folder = rootdir.joinpath(sf)
         if not folder.exists():
             folder.mkdir(parents=True, exist_ok=True)
-            logger.info('[bold green] CREATE DIRECTORY: [/bold green]'  + str(folder), extra={"markup": True}) 
+            logger.info(
+                "[bold green] CREATE DIRECTORY: [/bold green]" + str(folder),
+                extra={"markup": True},
+            )
         else:
-            logger.warn('[bold yellow] DIRECOTRY EXISTS: [/bold yellow]'  + str(folder), extra={"markup": True}) 
-    return 
-    
+            logger.warn(
+                "[bold yellow] DIRECOTRY EXISTS: [/bold yellow]" + str(folder),
+                extra={"markup": True},
+            )
+    return
+
+
 def create_plugin_config(rootdir: pathlib.Path) -> None:
     """
     Given the root directory, create a plugin json.
     """
-    plugin_configuration_json: pathlib.Path = rootdir.joinpath('plugins_conf.json')
+    plugin_configuration_json: pathlib.Path = rootdir.joinpath("plugins_conf.json")
 
     if not plugin_configuration_json.exists():
-        logger.info('[bold green] CREATE PLUGIN CONFIGURATION [/bold green]'  + str(plugin_configuration_json), extra={"markup": True}) 
-        
-        empty_plugins = {"plugins" : []}
-        with open(plugin_configuration_json, 'w') as f:
+        logger.info(
+            "[bold green] CREATE PLUGIN CONFIGURATION [/bold green]"
+            + str(plugin_configuration_json),
+            extra={"markup": True},
+        )
+
+        empty_plugins = {"plugins": []}
+        with open(plugin_configuration_json, "w") as f:
             f.write(json.dumps(empty_plugins))
 
     else:
-        logger.warn('[bold yellow] FILE EXISTS: [/bold yellow]'  + str(plugin_configuration_json), extra={"markup": True}) 
+        logger.warn(
+            "[bold yellow] FILE EXISTS: [/bold yellow]"
+            + str(plugin_configuration_json),
+            extra={"markup": True},
+        )
 
     return
 
-def create_app_dirs(rootdir = pathlib.Path.home().joinpath('.rafe/')) -> int:
+
+def create_app_dirs(rootdir=pathlib.Path.home().joinpath(".rafe/")) -> int:
     """
     Given a root directory, create it inside the homedir and then create
     subfolders for neccesary functions.
     """
     create_rafe_rootdir(rootdir)
     create_cfcache_subfolder(rootdir)
     create_plugin_config(rootdir)
-    return 0    
+    return 0
 
 
 root_dir = dirname(dirname(__file__))
-src_cache = join(root_dir, 'src_cache')
-work_dir = join(root_dir, 'work')
+src_cache = join(root_dir, "src_cache")
+work_dir = join(root_dir, "work")
```

## rafe/logger.py

```diff
@@ -2,36 +2,40 @@
 import logging
 import pathlib
 
 from datetime import datetime, timedelta
 from rich.logging import RichHandler
 
 # ------ FORMATTERS ------
-shell_format = '%(message)s'
+shell_format = "%(message)s"
 shell_formatter = logging.Formatter(shell_format)
 
 # ------ HANDLERS ------
 shell_handler = RichHandler(show_path=False)
 shell_handler.setFormatter(shell_formatter)
 
 # ------ LOGGERS ------
 logger = logging.getLogger(__name__)
 logger.setLevel(logging.INFO)
 logger.addHandler(shell_handler)
 
+
 def setFileHandle(logger, filepath: pathlib.Path):
-    if len([i for i in filepath.glob('*.log')])>0:
-        last_modified_time = datetime.fromtimestamp(os.path.getmtime([i for i in filepath.glob('*.log')][-1])) 
+    if len([i for i in filepath.glob("*.log")]) > 0:
+        last_modified_time = datetime.fromtimestamp(
+            os.path.getmtime([i for i in filepath.glob("*.log")][-1])
+        )
         diff = datetime.now() - last_modified_time
     else:
         diff = timedelta(seconds=99999)
     if 30 >= (diff.total_seconds() / 60):
-        fh = logging.FileHandler(list(filepath.glob('*.log'))[-1])
+        fh = logging.FileHandler(list(filepath.glob("*.log"))[-1])
         logger.addHandler(fh)
         logger.debug(f'Using logfile: {list(filepath.glob("*.log"))[-1]}')
     else:
-        filepath = filepath.joinpath(f"rafe_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
-        logger.info(f'Creating logfile: {filepath}')
+        filepath = filepath.joinpath(
+            f"rafe_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
+        )
+        logger.info(f"Creating logfile: {filepath}")
         fh = logging.FileHandler(filepath)
         logger.addHandler(fh)
         return
-
```

## rafe/main.py

```diff
@@ -1,10 +1,11 @@
 import pathlib
 from rafe.cli import app
 from rafe.logger import logger, setFileHandle
 
+
 def cli():
-    if pathlib.Path.home().joinpath('.rafe').exists():
-        setFileHandle(logger, pathlib.Path.home().joinpath('.rafe', '.cfcache', 'package_reports'))
+    if pathlib.Path.home().joinpath(".rafe").exists():
+        setFileHandle(
+            logger, pathlib.Path.home().joinpath(".rafe", ".cfcache", "package_reports")
+        )
     app(prog_name="rafe")
-
-
```

## rafe/metadata.py

```diff
@@ -1,40 +1,42 @@
 import re
 from os.path import isfile, join
 
 import yaml
 
 
 def ns_cfg(cfg):
-    plat = cfg['plat']
+    plat = cfg["plat"]
     return dict(
-        mkl = bool(cfg['MKL']),
-        debug = bool(cfg['DEBUG']),
-        linux = plat.startswith('linux-'),
-        win = plat.startswith('win-'),
-        win32 = bool(plat == 'win-32'),
-        win64 = bool(plat == 'win-64'),
-        x86 = plat.endswith(('-32', '-64')),
-        x86_64 = plat.endswith('-64'),
+        mkl=bool(cfg["MKL"]),
+        debug=bool(cfg["DEBUG"]),
+        linux=plat.startswith("linux-"),
+        win=plat.startswith("win-"),
+        win32=bool(plat == "win-32"),
+        win64=bool(plat == "win-64"),
+        x86=plat.endswith(("-32", "-64")),
+        x86_64=plat.endswith("-64"),
     )
 
 
-sel_pat = re.compile(r'(.+?)\s*\[(.+)\]$')
+sel_pat = re.compile(r"(.+?)\s*\[(.+)\]$")
+
+
 def select_lines(data, namespace):
     lines = []
     for line in data.splitlines():
         line = line.rstrip()
         m = sel_pat.match(line)
         if m:
             cond = m.group(2)
             if eval(cond, namespace, {}):
                 lines.append(m.group(1))
             continue
         lines.append(line)
-    return '\n'.join(lines) + '\n'
+    return "\n".join(lines) + "\n"
 
 
 def yamlize(data):
     res = yaml.safe_load(data)
     # ensure the result is a dict
     if res is None:
         res = {}
@@ -44,37 +46,41 @@
 def parse(data, cfg):
     if cfg is not None:
         data = select_lines(data, ns_cfg(cfg))
     # ensure we create new object, because yamlize is memoized
     res = yamlize(data)
 
     # ensure those are lists
-    for fieldname in ('source/patches',
-                      'build/entry_points',
-                      'test/commands', 'test/imports'):
-        section, key = fieldname.split('/')
+    for fieldname in (
+        "source/patches",
+        "build/entry_points",
+        "test/commands",
+        "test/imports",
+    ):
+        section, key = fieldname.split("/")
         if res.get(section) is None:
             res[section] = {}
         if res[section].get(key, None) is None:
             res[section][key] = []
     # ensure those are strings
-    for fieldname in ('source/md5', ):
-        section, key = fieldname.split('/')
+    for fieldname in ("source/md5",):
+        section, key = fieldname.split("/")
         if res.get(section) is None:
             res[section] = {}
-        res[section][key] = str(res[section].get(key, ''))
+        res[section][key] = str(res[section].get(key, ""))
 
     return res
 
 
 def render_recipe(recipe_dir, cfg=None):
-    path = join(recipe_dir, 'meta.yaml')
+    path = join(recipe_dir, "meta.yaml")
     if not isfile(path):
         return None
     data = open(path).read()
     meta = parse(data, cfg)
     return meta
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     from pprint import pprint
-    pprint(render_recipe('../examples/bitarray'))
+
+    pprint(render_recipe("../examples/bitarray"))
```

## rafe/plugin.py

```diff
@@ -3,124 +3,126 @@
 import pathlib
 import json
 import logging
 
 import importlib.util
 
 import typer
+
 """
 This file helps rafe discover, register, and utilize plugins.
 
 To register plugins, there will be two options:
     1) CLI Based Plugins (metaconvert)
     and
     2) PyLint Plugins (TBD)
 
 We will discover both the module and any exposed flie and cli applicatons associated with them and
 add them to rafe before runtime.
 """
 
-class RafePlugin():
+
+class RafePlugin:
     """
     A class for storing information on runtime plugins. If there is a cli.py
-    exposed, then it adds it to the command line. 
+    exposed, then it adds it to the command line.
 
     Working as importable is the ideal other way to solve this problem.
     """
-    def __init__(
-            self,
-            name: str,
-            path: pathlib.Path,
-            entrypoint: str,
-            isCLI: bool = True
-        ):
 
+    def __init__(
+        self, name: str, path: pathlib.Path, entrypoint: str, isCLI: bool = True
+    ):
         self.parent_module = self._load_parent_module(name, path)
 
         if isCLI == True:
             self.cli_module = self._load_cli_module(name, path)
             self.cli_callable = getattr(self.cli_module, entrypoint)
-            
+
     def _load_parent_module(self, name, path):
-        spec = importlib.util.spec_from_file_location(name, path.joinpath('__init__.py'))
+        spec = importlib.util.spec_from_file_location(
+            name, path.joinpath("__init__.py")
+        )
         module = importlib.util.module_from_spec(spec)
         sys.modules[spec.name] = module
         spec.loader.exec_module(module)
         return module
 
     def _load_cli_module(self, name, path):
         module_cli_import = f"{name}.cli"
-        spec = importlib.util.spec_from_file_location(module_cli_import, path.joinpath('cli.py'))
+        spec = importlib.util.spec_from_file_location(
+            module_cli_import, path.joinpath("cli.py")
+        )
         module = importlib.util.module_from_spec(spec)
         spec.loader.exec_module(module)
-        return module 
+        return module
+
 
-class RafePluginManager():
-    
+class RafePluginManager:
     def __init__(self, app: typer.Typer, path: pathlib.Path, logger: logging.Logger):
         """
         Initializes the configuration class.
         """
         self.logger = logger
         self.configuration_path = path
 
-        self.raw : Dict = self.read_json(path)
+        self.raw: Dict = self.read_json(path)
 
-        self.plugins : List = self.raw["plugins"]
+        self.plugins: List = self.raw["plugins"]
 
         if len(self.plugins) == 0:
-            self.logger.debug('No plugins found.')
+            self.logger.debug("No plugins found.")
         else:
-            self.logger.debug('Loading plugins...')
+            self.logger.debug("Loading plugins...")
             self.load_plugins(app)
-            
+
     def add_plugin(self, name, path, entrypoint):
-        entry = {
-                "name": name,
-                "path": str(path),
-                "entrypoint": entrypoint
-                }
+        entry = {"name": name, "path": str(path), "entrypoint": entrypoint}
 
         self.plugins.append(entry)
         self.logger.debug(f"Adding plugin {name}")
-        self.write_json(self.plugins, self.configuration_path) 
+        self.write_json(self.plugins, self.configuration_path)
 
     def list_plugins(self):
         p = self.read_json(self.configuration_path)
         return p["plugins"]
 
     def load_plugins(self, app):
         for idx in range(len(self.plugins)):
-            name, path, entrypoint = self.plugins[idx]["name"], self.plugins[idx]["path"], self.plugins[idx]["entrypoint"]
+            name, path, entrypoint = (
+                self.plugins[idx]["name"],
+                self.plugins[idx]["path"],
+                self.plugins[idx]["entrypoint"],
+            )
             path = pathlib.Path(path)
-            self.plugins[idx] = RafePlugin(name=name, path=path, entrypoint=entrypoint) 
-            self.logger.debug(f'Loaded Plugin: {name}')
+            self.plugins[idx] = RafePlugin(name=name, path=path, entrypoint=entrypoint)
+            self.logger.debug(f"Loaded Plugin: {name}")
 
         for rplugin in self.plugins:
-            if hasattr(rplugin, 'cli_callable') is True:
-                self.logger.debug(f'Registering Plugin: {name} CLI')
-                rplugin.cli_callable = app.command(rich_help_panel='Plugins')(rplugin.cli_callable)
-                self.logger.debug(f'Successfully Registered: {name} CLI')
+            if hasattr(rplugin, "cli_callable") is True:
+                self.logger.debug(f"Registering Plugin: {name} CLI")
+                rplugin.cli_callable = app.command(rich_help_panel="Plugins")(
+                    rplugin.cli_callable
+                )
+                self.logger.debug(f"Successfully Registered: {name} CLI")
             else:
-                self.logger.debug(f'No CLI found for plugin {name}')
+                self.logger.debug(f"No CLI found for plugin {name}")
 
     @staticmethod
     def write_json(o, path: pathlib.Path):
         """
         Writes the configuration file with the currently stored plugins
         """
-        d  = {}
+        d = {}
         d["plugins"] = o
-        with open(path, 'w') as f:
-             f.write(json.dumps(d))
-        return 
+        with open(path, "w") as f:
+            f.write(json.dumps(d))
+        return
 
     @staticmethod
     def read_json(path: pathlib.Path):
         """
         Reads a configuration file for the plugins and initializes them.
         """
-        with open(path, 'r') as f:
+        with open(path, "r") as f:
             j = json.loads(f.read())
         return j
-
-
```

## rafe/source.py

```diff
@@ -6,76 +6,83 @@
 
 from rafe.utils import download, hashsum_file, rm_rf, tar_xf
 from rafe.config import work_dir, src_cache
 from rafe.metadata import render_recipe
 
 
 def get_dir():
-    lst = [fn for fn in os.listdir(work_dir) if not fn.startswith('.')]
+    lst = [fn for fn in os.listdir(work_dir) if not fn.startswith(".")]
     if len(lst) == 1:
         dir_path = join(work_dir, lst[0])
         if isdir(dir_path):
             return dir_path
     return work_dir
 
 
 def download_to_cache(source):
     if not isdir(src_cache):
         os.makedirs(src_cache)
 
-    fn = basename(urlparse(source['url']).path)
-    md5 = source.get('md5')
+    fn = basename(urlparse(source["url"]).path)
+    md5 = source.get("md5")
     path = join(src_cache, fn)
 
     if not isfile(path):
-        download(source['url'], path, md5)
+        download(source["url"], path, md5)
 
     assert isfile(path)
-    for ht in 'md5', 'sha1', 'sha256':
+    for ht in "md5", "sha1", "sha256":
         if ht in source and hashsum_file(path, ht) != source[ht]:
             raise Exception("%s mismatch: %r" % (ht.upper(), source))
     return path
 
 
 def unpack(source):
     src_path = download_to_cache(source)
 
     os.makedirs(work_dir)
-    if src_path.endswith(('.tar.gz', '.tar.bz2', '.tgz', '.tar.xz', '.tar')):
+    if src_path.endswith((".tar.gz", ".tar.bz2", ".tgz", ".tar.xz", ".tar")):
         tar_xf(src_path, work_dir)
     else:
         raise Exception("not a vaild source")
 
 
 def apply_patch(src_dir, path):
-    print('Applying patch: %r' % path)
+    print("Applying patch: %r" % path)
     assert isfile(path), path
-    args = ['-p0', '-i', path]
-    check_call(['patch',] + args, cwd=src_dir)
-    if sys.platform == 'win32' and os.path.exists(args[-1]):
+    args = ["-p0", "-i", path]
+    check_call(
+        [
+            "patch",
+        ]
+        + args,
+        cwd=src_dir,
+    )
+    if sys.platform == "win32" and os.path.exists(args[-1]):
         os.remove(args[-1])  # clean up .patch_unix file
 
 
 def provide(recipe_dir):
     """
     given the metadata:
       - download (if necessary)
       - unpack
       - apply patches (if any)
     """
     meta = render_recipe(recipe_dir)
-    source = meta.get('source', {})
+    source = meta.get("source", {})
     rm_rf(work_dir)
-    if 'url' in source:
+    if "url" in source:
         unpack(source)
-    else: # no source
+    else:  # no source
         os.makedirs(work_dir)
 
-    if 'patch' in source:
+    if "patch" in source:
         src_dir = get_dir()
-        for patch in source.get('patches', []):
+        for patch in source.get("patches", []):
             apply_patch(src_dir, recipe_dir, patch)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     from rafe.config import recipes_dir
-    provide(join(recipes_dir, 'bitarray'))
+
+    provide(join(recipes_dir, "bitarray"))
```

## rafe/utils.py

```diff
@@ -16,89 +16,87 @@
         print("Downloading %s to %s..." % (url, dst_path))
 
     with urllib.request.urlopen(url) as resp:
         data = resp.read()
 
     if md5 and hashlib.md5(data).hexdigest() != md5:
         sys.exit("Error: MD5 mismatch, expected: %s" % md5)
-    with open(dst_path, 'wb') as fo:
+    with open(dst_path, "wb") as fo:
         fo.write(data)
 
 
-def chunk_file(path, mode='rb', chunksize=262144):
+def chunk_file(path, mode="rb", chunksize=262144):
     """
     read potentially large file in chunks (of 256KB by default) so as not to
     use large amounts of memory
     """
     with open(path, mode) as f:
         while 1:
             chunk = f.read(chunksize)
             if not chunk:
                 break
             yield chunk
 
 
 def bunzip2(bz2path, verbose=False):
-    assert bz2path.endswith('.bz2')
+    assert bz2path.endswith(".bz2")
     path = bz2path[:-4]
     if verbose:
         print("bunz2ing:", bz2path)
-    with open(path, mode='wb') as fo:
+    with open(path, mode="wb") as fo:
         bz2_decomp = BZ2Decompressor()
         for c in chunk_file(bz2path):
             fo.write(bz2_decomp.decompress(c))
     assert isfile(path)
 
 
-def tar_xf(tarball, dir_path, mode='r:*'):
-    if tarball.endswith('.tar.xz'):
-        subprocess.check_call(['unxz', '-f', '-k', tarball])
+def tar_xf(tarball, dir_path, mode="r:*"):
+    if tarball.endswith(".tar.xz"):
+        subprocess.check_call(["unxz", "-f", "-k", tarball])
         tarball = tarball[:-3]
     t = tarfile.open(tarball, mode)
     t.extractall(path=dir_path)
     t.close()
 
 
 def rm_rf(path):
     if islink(path) or isfile(path):
         # Note that we have to check if the destination is a link because
         # exists('/path/to/dead-link') will return False, although
         # islink('/path/to/dead-link') is True.
         os.unlink(path)
 
     elif isdir(path):
-        if sys.platform == 'win32':
-            subprocess.check_call(['cmd', '/c', 'rd', '/s', '/q', path])
+        if sys.platform == "win32":
+            subprocess.check_call(["cmd", "/c", "rd", "/s", "/q", path])
         else:
             shutil.rmtree(path)
 
 
 def clean_dir(dir_path):
     for fn in os.listdir(dir_path):
-        if fn.endswith(('~', '.pyc')):
+        if fn.endswith(("~", ".pyc")):
             rm_rf(join(dir_path, fn))
 
 
-def hashsum_file(path, mode='md5'):
+def hashsum_file(path, mode="md5"):
     h = hashlib.new(mode)
     for chunk in chunk_file(path):
         h.update(chunk)
     return h.hexdigest()
 
 
 def md5_file(path):
-    return hashsum_file(path, 'md5')
+    return hashsum_file(path, "md5")
 
 
 def file_info(path, add_sha256=False):
-    res = {'size': getsize(path),
-           'md5': md5_file(path),
-           'mtime': getmtime(path)}
+    res = {"size": getsize(path), "md5": md5_file(path), "mtime": getmtime(path)}
     if add_sha256:
-        res['sha256'] = hashsum_file(path, mode='sha256')
+        res["sha256"] = hashsum_file(path, mode="sha256")
     return res
 
 
 def possibly_download(url, dst_path, md5, verbose=False):
     "download if necessary"
     if isfile(dst_path) and md5_file(dst_path) == md5:
         return
@@ -106,52 +104,54 @@
 
 
 def human_bytes(n):
     """
     Return the number of bytes n in more human readable form.
     """
     if n < 1024:
-        return '%d' % n
+        return "%d" % n
     k = float(n) / 1024
     if k < 1024:
-        return '%dK' % round(k)
+        return "%dK" % round(k)
     m = k / 1024
     if m < 1024:
-        return '%.1fM' % m
+        return "%.1fM" % m
     g = m / 1024
-    return '%.2fG' % g
+    return "%.2fG" % g
 
 
 class memoized(object):
     """Decorator. Caches a function's return value each time it is called.
     If called later with the same arguments, the cached value is returned
     (not reevaluated).
     """
+
     def __init__(self, func):
         self.func = func
         self.cache = {}
+
     def __call__(self, *args):
         if not isinstance(args, collections.Hashable):
             # uncacheable. a list, for instance.
             # better to not cache than blow up.
             return self.func(*args)
         if args in self.cache:
             return self.cache[args]
         else:
             value = self.func(*args)
             self.cache[args] = value
             return value
 
 
 @contextmanager
-def safe_write(path, mode='w'):
+def safe_write(path, mode="w"):
     # write to a temp file and rename afterwards
     tmp_path = path + ".tmp"
 
     with open(tmp_path, mode=mode) as fo:
         yield fo
 
     os.rename(tmp_path, path)
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     pass
```

## Comparing `rafe-0.1.4.dist-info/LICENSE.txt` & `rafe-0.1.5.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `rafe-0.1.4.dist-info/RECORD` & `rafe-0.1.5.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-rafe/__init__.py,sha256=o6ivA9leXbedqRnppSywtlYrrZBzbsCSV5gVEsaLPmo,71
-rafe/__main__.py,sha256=Xdi9npjfSPK1kUE_D8_gD9XeNEDXBwiJyjhSFTz2FMA,61
-rafe/build.py,sha256=9DGweA2964yPxeYt_2wg3GYAwMXvcbishNfMHxt44gY,2014
-rafe/cfgraph.py,sha256=RnHBbEB3CtNj3H3ufCm1hzgHaizMB3-nJz6_tMD3DLw,22950
-rafe/cli.py,sha256=vYR4CseuSv7Hv5eJHb4LhrgyiDpURMnDyvNJRY6ZxNw,13115
-rafe/cli_api.py,sha256=wPfPGSbbRuq6u79-PDuxuf8_IdEor0i2MzB4Xul6LMk,8138
-rafe/config.py,sha256=_vCMBaeJnaha99okzm0hxPs8YXWFUDjUDbs8t_b1CPY,2366
-rafe/logger.py,sha256=F-JOAxAKY8d1OTz5jeOMF1vDvB9LdHXVLefUNQ1eRnA,1241
-rafe/main.py,sha256=-IgcTXhq8hDcG3TdmN9M06rlKW6lZINRXkXI1awGcPE,281
-rafe/metadata.py,sha256=klIeils50qlxx2lLftQ8Zj7QbkCgQpjxjK6RRfagvUk,2087
-rafe/plugin.py,sha256=joOwfWb9Q3fcYVRCfTqCS-Rlv3zx7s1uEQvDSqAOS8g,4014
-rafe/source.py,sha256=vd5EKOXPei24VJHaiCE7JfDy1dMTZDk5SUDLqootnus,2162
-rafe/utils.py,sha256=nf9tXvOa_Ubm-_EZGTq0QLZImm8Gs0xhK_qTqDJzNtY,4064
-rafe-0.1.4.dist-info/LICENSE.txt,sha256=bk8H6oMXQAcDs6uLXh0jCW-6bcz7HAvG5yO1DpA9uBA,1455
-rafe-0.1.4.dist-info/METADATA,sha256=Ef-8LCNCr9yuAqF-WyEPTdntfSGvCjrfUq5wX_rX_nc,1125
-rafe-0.1.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-rafe-0.1.4.dist-info/entry_points.txt,sha256=648OfD-f-wH9Xb2Xfwt-b-5WugCsqUsOHh2WFvWtuRA,40
-rafe-0.1.4.dist-info/top_level.txt,sha256=t4Lb6m-BEtVPU2Zx8g-0oMvlWBtYQWXQpWuHJeOkKYU,5
-rafe-0.1.4.dist-info/RECORD,,
+rafe/__init__.py,sha256=obdvdtCKFshboHg0vZumZJzdjxmTu2iEgo7KGIu1b8M,71
+rafe/__main__.py,sha256=qF8ob9wqPM7eJ1JheF-fyHg39ixJiFyvLvhgBW1tbqY,63
+rafe/build.py,sha256=lIwiMq8cB6yYsn7hsZqMoM05WS0dT8NwgmBs6BYIwHk,2415
+rafe/cfgraph.py,sha256=lX-9niyalbb7PUuzSMaAykZhyk5K6t2qDDZFzbYN5AM,26009
+rafe/cli.py,sha256=VG_iiYT8_puS4C8g7smHR9zfmf5d5rM7L7JOKBAEuas,13854
+rafe/cli_api.py,sha256=jq34twMfqXmGifE9p5nQNx7AGl-fMjeJUVP1O97PsEQ,9039
+rafe/config.py,sha256=Pam2-2cxGr8y-LnglnCn-5YZ95xHc8KvBeJpTzrG0yk,2639
+rafe/logger.py,sha256=tfmaFwDnlBnXhzNZOPpScAsizcnwUtcik1fxuENN_Xs,1286
+rafe/main.py,sha256=VfruOJEUIKlKWbtuVzxiqi5yM5ePiAcqbD8ZQDAsj1M,302
+rafe/metadata.py,sha256=IzbUBadd_8WDAXcG4MCgXJAXU2C7qE0eRvcruvRU6Y4,2068
+rafe/plugin.py,sha256=3U_3mbJUkrI7jhjHB2Uro5MNdIELOypyWhM-mH7k_OU,3998
+rafe/source.py,sha256=WTpf0YE0eeIyie1gX4qfuVIl_PNSz7oRXyYsleCPd_4,2217
+rafe/utils.py,sha256=CcMijLcc6KNdA4vWlfuqFRASNvKq4vvcfg6PWkLHseY,4044
+rafe-0.1.5.dist-info/LICENSE.txt,sha256=bk8H6oMXQAcDs6uLXh0jCW-6bcz7HAvG5yO1DpA9uBA,1455
+rafe-0.1.5.dist-info/METADATA,sha256=nron6FfOZOAjSG64DnmAMQoui9LieP9hHzbxQhZTmlo,1878
+rafe-0.1.5.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+rafe-0.1.5.dist-info/entry_points.txt,sha256=648OfD-f-wH9Xb2Xfwt-b-5WugCsqUsOHh2WFvWtuRA,40
+rafe-0.1.5.dist-info/top_level.txt,sha256=t4Lb6m-BEtVPU2Zx8g-0oMvlWBtYQWXQpWuHJeOkKYU,5
+rafe-0.1.5.dist-info/RECORD,,
```

