# Comparing `tmp/towhee.models-1.1.0-py3-none-any.whl.zip` & `tmp/towhee.models-1.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,282 +1,282 @@
-Zip file size: 1808311 bytes, number of entries: 280
--rw-r--r--  2.0 unx     8596 b- defN 22-Oct-08 02:27 towhee/models/README.md
--rw-r--r--  2.0 unx     8570 b- defN 22-Oct-08 02:27 towhee/models/README_CN.md
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/__init__.py
--rw-r--r--  2.0 unx      678 b- defN 22-Aug-19 02:52 towhee/models/acar_net/__init__.py
--rw-r--r--  2.0 unx    10305 b- defN 22-Oct-08 02:27 towhee/models/acar_net/backbone.py
--rw-r--r--  2.0 unx     9275 b- defN 22-Oct-08 02:27 towhee/models/acar_net/head.py
--rw-r--r--  2.0 unx     3613 b- defN 22-Aug-19 02:52 towhee/models/acar_net/model.py
--rw-r--r--  2.0 unx     4102 b- defN 22-Oct-08 02:27 towhee/models/acar_net/neck.py
--rw-r--r--  2.0 unx     1618 b- defN 22-Aug-19 02:52 towhee/models/acar_net/utils.py
--rw-r--r--  2.0 unx      709 b- defN 22-Aug-19 02:52 towhee/models/action_clip/__init__.py
--rw-r--r--  2.0 unx     4305 b- defN 22-Oct-08 02:27 towhee/models/action_clip/action_clip.py
--rw-r--r--  2.0 unx     2039 b- defN 22-Oct-08 02:27 towhee/models/action_clip/action_clip_utils.py
--rw-r--r--  2.0 unx     1804 b- defN 22-Oct-08 02:27 towhee/models/action_clip/text_prompt.py
--rw-r--r--  2.0 unx     8805 b- defN 22-Oct-08 02:27 towhee/models/action_clip/visual_prompt.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/allinone/__init__.py
--rw-r--r--  2.0 unx     1289 b- defN 22-Aug-19 02:52 towhee/models/allinone/allinone.py
--rw-r--r--  2.0 unx      622 b- defN 22-Aug-19 02:52 towhee/models/bridgeformer/__init__.py
--rw-r--r--  2.0 unx     3300 b- defN 22-Oct-08 02:27 towhee/models/bridgeformer/bridge_former.py
--rw-r--r--  2.0 unx    10421 b- defN 22-Oct-08 02:27 towhee/models/bridgeformer/bridge_former_training.py
--rw-r--r--  2.0 unx    16155 b- defN 22-Oct-08 02:27 towhee/models/bridgeformer/bridge_former_training_block.py
--rw-r--r--  2.0 unx     1560 b- defN 22-Aug-19 02:52 towhee/models/clip/README.md
--rw-r--r--  2.0 unx      639 b- defN 22-Aug-19 02:52 towhee/models/clip/__init__.py
--rw-r--r--  2.0 unx    20896 b- defN 23-May-17 03:38 towhee/models/clip/auxilary.py
--rw-r--r--  2.0 unx  1356917 b- defN 22-Aug-19 02:52 towhee/models/clip/bpe_simple_vocab_16e6.txt.gz
--rw-r--r--  2.0 unx    30031 b- defN 22-Nov-24 11:16 towhee/models/clip/clip.py
--rw-r--r--  2.0 unx     9935 b- defN 22-Nov-11 10:02 towhee/models/clip/clip_utils.py
--rw-r--r--  2.0 unx     5618 b- defN 22-Oct-08 02:27 towhee/models/clip/simple_tokenizer.py
--rw-r--r--  2.0 unx       46 b- defN 22-Aug-19 02:52 towhee/models/clip4clip/__init__.py
--rw-r--r--  2.0 unx    10156 b- defN 23-Feb-06 09:20 towhee/models/clip4clip/clip4clip.py
--rw-r--r--  2.0 unx     2605 b- defN 22-Aug-19 02:52 towhee/models/clip4clip/until_module.py
--rw-r--r--  2.0 unx     2536 b- defN 23-May-17 03:38 towhee/models/clip4clip/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/coca/__init__.py
--rw-r--r--  2.0 unx    11514 b- defN 22-Oct-08 02:27 towhee/models/coca/coca.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/coformer/__init__.py
--rw-r--r--  2.0 unx     5167 b- defN 22-Oct-08 02:27 towhee/models/coformer/backbone.py
--rw-r--r--  2.0 unx    10222 b- defN 22-Oct-08 02:27 towhee/models/coformer/coformer.py
--rw-r--r--  2.0 unx    10008 b- defN 22-Aug-19 02:52 towhee/models/coformer/config.py
--rw-r--r--  2.0 unx    15471 b- defN 22-Oct-08 02:27 towhee/models/coformer/transformer.py
--rw-r--r--  2.0 unx     2919 b- defN 22-Aug-19 02:52 towhee/models/coformer/utils.py
--rw-r--r--  2.0 unx      630 b- defN 22-Aug-19 02:52 towhee/models/collaborative_experts/__init__.py
--rw-r--r--  2.0 unx    60484 b- defN 22-Oct-08 02:27 towhee/models/collaborative_experts/collaborative_experts.py
--rw-r--r--  2.0 unx     3587 b- defN 22-Oct-08 02:27 towhee/models/collaborative_experts/net_vlad.py
--rw-r--r--  2.0 unx     1612 b- defN 22-Aug-19 02:52 towhee/models/collaborative_experts/util.py
--rw-r--r--  2.0 unx      661 b- defN 22-Oct-08 02:27 towhee/models/convnext/__init__.py
--rw-r--r--  2.0 unx     3245 b- defN 22-Oct-08 02:27 towhee/models/convnext/configs.py
--rw-r--r--  2.0 unx     4425 b- defN 22-Nov-24 11:16 towhee/models/convnext/convnext.py
--rw-r--r--  2.0 unx     4107 b- defN 22-Oct-08 02:27 towhee/models/convnext/utils.py
--rw-r--r--  2.0 unx      614 b- defN 22-Aug-19 02:52 towhee/models/cvnet/__init__.py
--rw-r--r--  2.0 unx     4665 b- defN 22-Nov-24 11:16 towhee/models/cvnet/cvnet.py
--rw-r--r--  2.0 unx     6640 b- defN 22-Aug-19 02:52 towhee/models/cvnet/cvnet_block.py
--rw-r--r--  2.0 unx     2439 b- defN 22-Oct-08 02:27 towhee/models/cvnet/cvnet_utils.py
--rw-r--r--  2.0 unx     8106 b- defN 22-Oct-08 02:27 towhee/models/cvnet/resnet.py
--rw-r--r--  2.0 unx       19 b- defN 22-Aug-19 02:52 towhee/models/drl/__init__.py
--rw-r--r--  2.0 unx    35156 b- defN 23-May-17 03:38 towhee/models/drl/drl.py
--rw-r--r--  2.0 unx     8633 b- defN 23-May-17 03:38 towhee/models/drl/module_cross.py
--rw-r--r--  2.0 unx     4891 b- defN 22-Aug-19 02:52 towhee/models/drl/until_module.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/embedding/__init__.py
--rw-r--r--  2.0 unx     2061 b- defN 22-Aug-19 02:52 towhee/models/embedding/embedding_extractor.py
--rw-r--r--  2.0 unx      623 b- defN 22-Aug-19 02:52 towhee/models/frozen_in_time/__init__.py
--rw-r--r--  2.0 unx    14921 b- defN 22-Oct-08 02:27 towhee/models/frozen_in_time/frozen_in_time.py
--rw-r--r--  2.0 unx     2394 b- defN 22-Aug-19 02:52 towhee/models/frozen_in_time/frozen_utils.py
--rw-r--r--  2.0 unx    15910 b- defN 22-Oct-08 02:27 towhee/models/frozen_in_time/frozen_video_transformer.py
--rw-r--r--  2.0 unx      659 b- defN 22-Oct-08 02:27 towhee/models/hornet/__init__.py
--rw-r--r--  2.0 unx     4309 b- defN 22-Oct-08 02:27 towhee/models/hornet/configs.py
--rw-r--r--  2.0 unx     5036 b- defN 22-Nov-24 11:16 towhee/models/hornet/hornet.py
--rw-r--r--  2.0 unx     5983 b- defN 22-Oct-08 02:27 towhee/models/hornet/utils.py
--rw-r--r--  2.0 unx      612 b- defN 22-Oct-08 02:27 towhee/models/isc/__init__.py
--rw-r--r--  2.0 unx     3470 b- defN 23-Feb-06 09:20 towhee/models/isc/isc.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/layers/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 22-Oct-08 02:27 towhee/models/layers/aspp.py
--rw-r--r--  2.0 unx     6693 b- defN 22-Aug-19 02:52 towhee/models/layers/attention.py
--rw-r--r--  2.0 unx     5783 b- defN 22-Aug-19 02:52 towhee/models/layers/cond_conv2d.py
--rw-r--r--  2.0 unx     2706 b- defN 22-Aug-19 02:52 towhee/models/layers/conv2d_same.py
--rw-r--r--  2.0 unx     4180 b- defN 22-Aug-19 02:52 towhee/models/layers/conv2d_separable.py
--rw-r--r--  2.0 unx     4354 b- defN 22-Oct-08 02:27 towhee/models/layers/conv4d.py
--rw-r--r--  2.0 unx     5125 b- defN 22-Oct-12 06:36 towhee/models/layers/conv_bn_activation.py
--rw-r--r--  2.0 unx     1918 b- defN 22-Oct-08 02:27 towhee/models/layers/convmlp.py
--rw-r--r--  2.0 unx     4369 b- defN 22-Aug-19 02:52 towhee/models/layers/cross_attention.py
--rw-r--r--  2.0 unx     5964 b- defN 22-Aug-19 02:52 towhee/models/layers/dropblock2d.py
--rw-r--r--  2.0 unx     2200 b- defN 22-Aug-19 02:52 towhee/models/layers/droppath.py
--rw-r--r--  2.0 unx     2175 b- defN 22-Oct-08 02:27 towhee/models/layers/ffn.py
--rw-r--r--  2.0 unx     2052 b- defN 22-Oct-08 02:27 towhee/models/layers/gatedmlp.py
--rw-r--r--  2.0 unx    10670 b- defN 22-Aug-19 02:52 towhee/models/layers/layers_with_relprop.py
--rw-r--r--  2.0 unx     4347 b- defN 23-Jun-09 07:02 towhee/models/layers/mbconv.py
--rw-r--r--  2.0 unx     2971 b- defN 22-Aug-19 02:52 towhee/models/layers/mixed_conv2d.py
--rw-r--r--  2.0 unx     2566 b- defN 22-Aug-19 02:52 towhee/models/layers/mlp.py
--rw-r--r--  2.0 unx    13234 b- defN 22-Oct-08 02:27 towhee/models/layers/multi_scale_attention.py
--rw-r--r--  2.0 unx     6651 b- defN 22-Oct-08 02:27 towhee/models/layers/multi_scale_transformer_block.py
--rw-r--r--  2.0 unx     3312 b- defN 22-Oct-08 02:27 towhee/models/layers/netvlad.py
--rw-r--r--  2.0 unx     6047 b- defN 23-May-17 03:38 towhee/models/layers/non_local.py
--rw-r--r--  2.0 unx     5759 b- defN 22-Aug-19 02:52 towhee/models/layers/padding_functions.py
--rw-r--r--  2.0 unx     4015 b- defN 22-Aug-19 02:52 towhee/models/layers/patch_embed2d.py
--rw-r--r--  2.0 unx     3620 b- defN 23-May-17 03:38 towhee/models/layers/patch_embed3d.py
--rw-r--r--  2.0 unx     2458 b- defN 22-Oct-08 02:27 towhee/models/layers/patch_merging.py
--rw-r--r--  2.0 unx     3009 b- defN 23-May-17 03:38 towhee/models/layers/patch_merging3d.py
--rw-r--r--  2.0 unx     3343 b- defN 22-Oct-08 02:27 towhee/models/layers/pool_attention.py
--rw-r--r--  2.0 unx     3084 b- defN 22-Aug-19 02:52 towhee/models/layers/position_encoding.py
--rw-r--r--  2.0 unx     4749 b- defN 22-Oct-08 02:27 towhee/models/layers/relative_self_attention.py
--rw-r--r--  2.0 unx     2028 b- defN 22-Aug-19 02:52 towhee/models/layers/resnet_basic_3d_module.py
--rw-r--r--  2.0 unx     3273 b- defN 22-Aug-19 02:52 towhee/models/layers/sam.py
--rw-r--r--  2.0 unx     1500 b- defN 22-Aug-19 02:52 towhee/models/layers/sequence_pool.py
--rw-r--r--  2.0 unx     3663 b- defN 22-Aug-19 02:52 towhee/models/layers/spatial_temporal_cls_positional_encoding.py
--rw-r--r--  2.0 unx      639 b- defN 22-Aug-19 02:52 towhee/models/layers/spp.py
--rw-r--r--  2.0 unx     5934 b- defN 23-May-17 03:38 towhee/models/layers/swin_transformer_block3d.py
--rw-r--r--  2.0 unx     1964 b- defN 22-Aug-19 02:52 towhee/models/layers/temporal_cg_avgpool3d.py
--rw-r--r--  2.0 unx     1964 b- defN 22-Aug-19 02:52 towhee/models/layers/tf_avgpool3d.py
--rw-r--r--  2.0 unx     2351 b- defN 22-Aug-19 02:52 towhee/models/layers/time2vec.py
--rw-r--r--  2.0 unx     2054 b- defN 22-Aug-19 02:52 towhee/models/layers/transformer_encoder.py
--rw-r--r--  2.0 unx     3223 b- defN 22-Aug-19 02:52 towhee/models/layers/vision_transformer_basic_head.py
--rw-r--r--  2.0 unx     8064 b- defN 22-Oct-08 02:27 towhee/models/layers/window_attention.py
--rw-r--r--  2.0 unx     5352 b- defN 23-May-17 03:38 towhee/models/layers/window_attention3d.py
--rw-r--r--  2.0 unx     1202 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/__init__.py
--rw-r--r--  2.0 unx     1312 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/gelu.py
--rw-r--r--  2.0 unx     1393 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/hardmish.py
--rw-r--r--  2.0 unx     1482 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/hardsigmoid.py
--rw-r--r--  2.0 unx     1424 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/hardswish.py
--rw-r--r--  2.0 unx     1328 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/mish.py
--rw-r--r--  2.0 unx     1320 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/prelu.py
--rw-r--r--  2.0 unx     1378 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/sigmoid.py
--rw-r--r--  2.0 unx      815 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/swiglu.py
--rw-r--r--  2.0 unx     1287 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/swish.py
--rw-r--r--  2.0 unx     1290 b- defN 22-Aug-19 02:52 towhee/models/layers/activations/tanh.py
--rw-r--r--  2.0 unx       26 b- defN 22-Aug-19 02:52 towhee/models/lightning_dot/__init__.py
--rw-r--r--  2.0 unx     5800 b- defN 22-Aug-19 02:52 towhee/models/lightning_dot/bi_encoder.py
--rw-r--r--  2.0 unx      593 b- defN 22-Aug-19 02:52 towhee/models/loss/__init__.py
--rw-r--r--  2.0 unx     2467 b- defN 22-Aug-19 02:52 towhee/models/loss/focal_loss.py
--rw-r--r--  2.0 unx      616 b- defN 22-Aug-19 02:52 towhee/models/max_vit/__init__.py
--rw-r--r--  2.0 unx     1802 b- defN 22-Aug-19 02:52 towhee/models/max_vit/configs.py
--rw-r--r--  2.0 unx     8066 b- defN 22-Nov-24 11:16 towhee/models/max_vit/max_vit.py
--rw-r--r--  2.0 unx    11404 b- defN 22-Oct-08 02:27 towhee/models/max_vit/max_vit_block.py
--rw-r--r--  2.0 unx     5382 b- defN 22-Aug-19 02:52 towhee/models/max_vit/max_vit_utils.py
--rw-r--r--  2.0 unx      702 b- defN 22-Oct-08 02:27 towhee/models/mcprop/__init__.py
--rw-r--r--  2.0 unx     3053 b- defN 22-Oct-08 02:27 towhee/models/mcprop/depthaggregator.py
--rw-r--r--  2.0 unx     2388 b- defN 22-Oct-08 02:27 towhee/models/mcprop/featurefusion.py
--rw-r--r--  2.0 unx     1413 b- defN 22-Oct-08 02:27 towhee/models/mcprop/imageextractor.py
--rw-r--r--  2.0 unx     3070 b- defN 22-Oct-08 02:27 towhee/models/mcprop/loss.py
--rw-r--r--  2.0 unx     6296 b- defN 22-Oct-08 02:27 towhee/models/mcprop/matching.py
--rw-r--r--  2.0 unx     1399 b- defN 22-Oct-08 02:27 towhee/models/mcprop/textextractor.py
--rw-r--r--  2.0 unx     2029 b- defN 22-Oct-08 02:27 towhee/models/mcprop/transformerpooling.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/mdmmt/__init__.py
--rw-r--r--  2.0 unx    15492 b- defN 22-Aug-19 02:52 towhee/models/mdmmt/bert_mmt.py
--rw-r--r--  2.0 unx    13512 b- defN 22-Oct-08 02:27 towhee/models/mdmmt/mmt.py
--rw-r--r--  2.0 unx     1602 b- defN 22-Oct-08 02:27 towhee/models/metaformer/addpositionembed.py
--rw-r--r--  2.0 unx     2453 b- defN 22-Oct-08 02:27 towhee/models/metaformer/attention.py
--rw-r--r--  2.0 unx     1803 b- defN 22-Oct-08 02:27 towhee/models/metaformer/basicblocks.py
--rw-r--r--  2.0 unx    10807 b- defN 22-Oct-08 02:27 towhee/models/metaformer/metaformer.py
--rw-r--r--  2.0 unx     3109 b- defN 22-Oct-08 02:27 towhee/models/metaformer/metaformerblock.py
--rw-r--r--  2.0 unx     1684 b- defN 22-Oct-08 02:27 towhee/models/metaformer/spatialfc.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/movinet/__init__.py
--rw-r--r--  2.0 unx    27214 b- defN 22-Aug-19 02:52 towhee/models/movinet/config.py
--rw-r--r--  2.0 unx     8063 b- defN 22-Aug-19 02:52 towhee/models/movinet/movinet.py
--rw-r--r--  2.0 unx    13711 b- defN 22-Aug-19 02:52 towhee/models/movinet/movinet_block.py
--rw-r--r--  2.0 unx       21 b- defN 22-Aug-19 02:52 towhee/models/mpvit/__init__.py
--rw-r--r--  2.0 unx    28795 b- defN 22-Oct-08 02:27 towhee/models/mpvit/mpvit.py
--rw-r--r--  2.0 unx       47 b- defN 22-Aug-19 02:52 towhee/models/multiscale_vision_transformers/__init__.py
--rw-r--r--  2.0 unx     4509 b- defN 22-Aug-19 02:52 towhee/models/multiscale_vision_transformers/create_mvit.py
--rw-r--r--  2.0 unx    34605 b- defN 22-Oct-08 02:27 towhee/models/multiscale_vision_transformers/mvit.py
--rw-r--r--  2.0 unx      613 b- defN 22-Aug-19 02:52 towhee/models/nnfp/__init__.py
--rw-r--r--  2.0 unx     5235 b- defN 22-Aug-19 02:52 towhee/models/nnfp/nnfp.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/omnivore/__init__.py
--rw-r--r--  2.0 unx    15170 b- defN 22-Aug-19 02:52 towhee/models/omnivore/omnivore.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/perceiver/__init__.py
--rw-r--r--  2.0 unx     1446 b- defN 22-Aug-19 02:52 towhee/models/perceiver/create_cross_attention.py
--rw-r--r--  2.0 unx     1277 b- defN 22-Aug-19 02:52 towhee/models/perceiver/create_self_attention.py
--rw-r--r--  2.0 unx      979 b- defN 22-Aug-19 02:52 towhee/models/perceiver/create_self_attention_block.py
--rw-r--r--  2.0 unx     1233 b- defN 22-Aug-19 02:52 towhee/models/perceiver/cross_attention.py
--rw-r--r--  2.0 unx      344 b- defN 22-Aug-19 02:52 towhee/models/perceiver/mlp.py
--rw-r--r--  2.0 unx     1435 b- defN 22-Aug-19 02:52 towhee/models/perceiver/multi_head_attention.py
--rw-r--r--  2.0 unx      667 b- defN 22-Aug-19 02:52 towhee/models/perceiver/residual.py
--rw-r--r--  2.0 unx     1234 b- defN 22-Aug-19 02:52 towhee/models/perceiver/self_attention.py
--rw-r--r--  2.0 unx      434 b- defN 22-Aug-19 02:52 towhee/models/perceiver/sequential.py
--rw-r--r--  2.0 unx     1767 b- defN 22-Oct-08 02:27 towhee/models/poolformer/basic_blocks.py
--rw-r--r--  2.0 unx      978 b- defN 22-Oct-08 02:27 towhee/models/poolformer/groupnorm.py
--rw-r--r--  2.0 unx     1492 b- defN 22-Oct-08 02:27 towhee/models/poolformer/layernormchannel.py
--rw-r--r--  2.0 unx     1839 b- defN 22-Oct-08 02:27 towhee/models/poolformer/mlp.py
--rw-r--r--  2.0 unx     1558 b- defN 22-Oct-08 02:27 towhee/models/poolformer/patchembed.py
--rw-r--r--  2.0 unx    10490 b- defN 22-Oct-08 02:27 towhee/models/poolformer/poolformer.py
--rw-r--r--  2.0 unx     3034 b- defN 22-Oct-08 02:27 towhee/models/poolformer/poolformerblock.py
--rw-r--r--  2.0 unx     1122 b- defN 22-Oct-08 02:27 towhee/models/poolformer/pooling.py
--rw-r--r--  2.0 unx      661 b- defN 22-Oct-18 06:57 towhee/models/replknet/__init__.py
--rw-r--r--  2.0 unx     2718 b- defN 22-Nov-24 11:16 towhee/models/replknet/configs.py
--rw-r--r--  2.0 unx     8942 b- defN 22-Nov-24 11:16 towhee/models/replknet/replknet.py
--rw-r--r--  2.0 unx    10184 b- defN 22-Oct-18 06:57 towhee/models/replknet/utils.py
--rw-r--r--  2.0 unx      637 b- defN 22-Aug-19 02:52 towhee/models/repmlp/__init__.py
--rw-r--r--  2.0 unx     8448 b- defN 22-Oct-08 02:27 towhee/models/repmlp/blocks.py
--rw-r--r--  2.0 unx     2339 b- defN 22-Aug-19 02:52 towhee/models/repmlp/configs.py
--rw-r--r--  2.0 unx     7259 b- defN 22-Nov-24 11:16 towhee/models/repmlp/repmlp.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/retina_face/__init__.py
--rw-r--r--  2.0 unx     2633 b- defN 23-May-17 03:38 towhee/models/retina_face/configs.py
--rw-r--r--  2.0 unx     3052 b- defN 23-May-17 03:38 towhee/models/retina_face/heads.py
--rw-r--r--  2.0 unx     2557 b- defN 23-May-17 03:38 towhee/models/retina_face/mobilenet_v1.py
--rw-r--r--  2.0 unx     2694 b- defN 23-May-17 03:38 towhee/models/retina_face/prior_box.py
--rw-r--r--  2.0 unx     7031 b- defN 23-May-17 03:38 towhee/models/retina_face/retinaface.py
--rw-r--r--  2.0 unx     2850 b- defN 23-May-17 03:38 towhee/models/retina_face/retinaface_fpn.py
--rw-r--r--  2.0 unx     2640 b- defN 23-May-17 03:38 towhee/models/retina_face/ssh.py
--rw-r--r--  2.0 unx     5694 b- defN 23-May-17 03:38 towhee/models/retina_face/utils.py
--rw-r--r--  2.0 unx      672 b- defN 22-Nov-07 10:51 towhee/models/shunted_transformer/__init__.py
--rw-r--r--  2.0 unx     2134 b- defN 22-Nov-07 10:51 towhee/models/shunted_transformer/configs.py
--rw-r--r--  2.0 unx     5411 b- defN 22-Nov-24 11:16 towhee/models/shunted_transformer/shunted_transformer.py
--rw-r--r--  2.0 unx    10283 b- defN 22-Nov-07 10:51 towhee/models/shunted_transformer/utils.py
--rw-r--r--  2.0 unx      612 b- defN 22-Aug-19 02:52 towhee/models/svt/__init__.py
--rw-r--r--  2.0 unx     2925 b- defN 22-Aug-19 02:52 towhee/models/svt/svt.py
--rw-r--r--  2.0 unx     1375 b- defN 22-Aug-19 02:52 towhee/models/svt/svt_utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/swin_transformer/__init__.py
--rw-r--r--  2.0 unx     3558 b- defN 22-Aug-19 02:52 towhee/models/swin_transformer/basic_layer.py
--rw-r--r--  2.0 unx    10536 b- defN 23-May-17 03:38 towhee/models/swin_transformer/configs.py
--rw-r--r--  2.0 unx     7134 b- defN 22-Aug-19 02:52 towhee/models/swin_transformer/model.py
--rw-r--r--  2.0 unx     6114 b- defN 22-Aug-19 02:52 towhee/models/swin_transformer/swin_transformer_block.py
--rw-r--r--  2.0 unx      620 b- defN 22-Aug-19 02:52 towhee/models/timesformer/__init__.py
--rw-r--r--  2.0 unx     9537 b- defN 22-Oct-08 02:27 towhee/models/timesformer/timesformer.py
--rw-r--r--  2.0 unx     6249 b- defN 22-Aug-19 02:52 towhee/models/timesformer/timesformer_block.py
--rw-r--r--  2.0 unx     9090 b- defN 22-Aug-19 02:52 towhee/models/timesformer/timesformer_utils.py
--rw-r--r--  2.0 unx      638 b- defN 22-Aug-19 02:52 towhee/models/transrac/__init__.py
--rw-r--r--  2.0 unx     5724 b- defN 22-Oct-08 02:27 towhee/models/transrac/transrac.py
--rw-r--r--  2.0 unx     3378 b- defN 22-Aug-19 02:52 towhee/models/transrac/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/tsm/__init__.py
--rw-r--r--  2.0 unx     4683 b- defN 22-Aug-19 02:52 towhee/models/tsm/config.py
--rw-r--r--  2.0 unx     5394 b- defN 22-Aug-19 02:52 towhee/models/tsm/mobilenet_v2.py
--rw-r--r--  2.0 unx     5815 b- defN 22-Aug-19 02:52 towhee/models/tsm/temporal_shift.py
--rw-r--r--  2.0 unx    13942 b- defN 22-Aug-19 02:52 towhee/models/tsm/tsm.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/uniformer/__init__.py
--rw-r--r--  2.0 unx     4337 b- defN 22-Aug-19 02:52 towhee/models/uniformer/config.py
--rw-r--r--  2.0 unx    21976 b- defN 22-Aug-19 02:52 towhee/models/uniformer/uniformer.py
--rw-r--r--  2.0 unx       89 b- defN 22-Nov-24 11:16 towhee/models/utils/__init__.py
--rw-r--r--  2.0 unx     4633 b- defN 22-Aug-19 02:52 towhee/models/utils/audio_preprocess.py
--rw-r--r--  2.0 unx     1488 b- defN 22-Aug-19 02:52 towhee/models/utils/basic_ops.py
--rw-r--r--  2.0 unx      808 b- defN 22-Aug-19 02:52 towhee/models/utils/causal_module.py
--rw-r--r--  2.0 unx     4368 b- defN 22-Aug-19 02:52 towhee/models/utils/create_act.py
--rw-r--r--  2.0 unx     2159 b- defN 22-Aug-19 02:52 towhee/models/utils/create_conv2d.py
--rw-r--r--  2.0 unx     1328 b- defN 22-Aug-19 02:52 towhee/models/utils/create_conv2d_pad.py
--rw-r--r--  2.0 unx     1664 b- defN 22-Nov-24 11:16 towhee/models/utils/create_model.py
--rw-r--r--  2.0 unx     3929 b- defN 22-Aug-19 02:52 towhee/models/utils/create_resnet_basic_3d_module.py
--rw-r--r--  2.0 unx     3658 b- defN 22-Nov-24 11:16 towhee/models/utils/download.py
--rw-r--r--  2.0 unx     1208 b- defN 22-Aug-19 02:52 towhee/models/utils/fuse_bn.py
--rw-r--r--  2.0 unx     1153 b- defN 22-Aug-19 02:52 towhee/models/utils/gelu_ignore_parameters.py
--rw-r--r--  2.0 unx     1370 b- defN 22-Aug-19 02:52 towhee/models/utils/general_utils.py
--rw-r--r--  2.0 unx     1720 b- defN 22-Aug-19 02:52 towhee/models/utils/get_relative_position_index.py
--rw-r--r--  2.0 unx     1545 b- defN 23-May-17 03:38 towhee/models/utils/get_window_size.py
--rw-r--r--  2.0 unx     2642 b- defN 22-Aug-19 02:52 towhee/models/utils/init_vit_weights.py
--rw-r--r--  2.0 unx     2032 b- defN 23-May-17 03:38 towhee/models/utils/pretrained_utils.py
--rw-r--r--  2.0 unx     1556 b- defN 22-Aug-19 02:52 towhee/models/utils/round_width.py
--rw-r--r--  2.0 unx     8505 b- defN 22-Aug-19 02:52 towhee/models/utils/video_transforms.py
--rw-r--r--  2.0 unx     3957 b- defN 22-Aug-19 02:52 towhee/models/utils/weight_init.py
--rw-r--r--  2.0 unx     1114 b- defN 23-May-17 03:38 towhee/models/utils/window_partition.py
--rw-r--r--  2.0 unx     1478 b- defN 23-May-17 03:38 towhee/models/utils/window_partition3d.py
--rw-r--r--  2.0 unx     1277 b- defN 23-May-17 03:38 towhee/models/utils/window_reverse.py
--rw-r--r--  2.0 unx     1474 b- defN 23-May-17 03:38 towhee/models/utils/window_reverse3d.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/vggish/__init__.py
--rw-r--r--  2.0 unx     2049 b- defN 22-Aug-19 02:52 towhee/models/vggish/torch_vggish.py
--rw-r--r--  2.0 unx      824 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/__init__.py
--rw-r--r--  2.0 unx     1643 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/compute_mask.py
--rw-r--r--  2.0 unx     3992 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/get_configs.py
--rw-r--r--  2.0 unx    14573 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/video_swin_transformer.py
--rw-r--r--  2.0 unx     3925 b- defN 22-Aug-19 02:52 towhee/models/video_swin_transformer/video_swin_transformer_block.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/violet/__init__.py
--rw-r--r--  2.0 unx     3916 b- defN 22-Aug-19 02:52 towhee/models/violet/violet.py
--rw-r--r--  2.0 unx     1007 b- defN 22-Oct-28 02:52 towhee/models/vis4mer/__init__.py
--rw-r--r--  2.0 unx     1471 b- defN 22-Oct-08 02:27 towhee/models/vis4mer/activation.py
--rw-r--r--  2.0 unx     1944 b- defN 22-Oct-12 06:36 towhee/models/vis4mer/get_initializer.py
--rw-r--r--  2.0 unx     2347 b- defN 22-Oct-12 06:36 towhee/models/vis4mer/linearactivation.py
--rw-r--r--  2.0 unx     1662 b- defN 22-Oct-08 02:27 towhee/models/vis4mer/transposelinear.py
--rw-r--r--  2.0 unx    10305 b- defN 22-Oct-28 02:52 towhee/models/vis4mer/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Aug-19 02:52 towhee/models/visualization/__init__.py
--rw-r--r--  2.0 unx     8524 b- defN 22-Oct-08 02:27 towhee/models/visualization/clip_visualization.py
--rw-r--r--  2.0 unx     3193 b- defN 22-Oct-08 02:27 towhee/models/visualization/embedding_visualization.py
--rw-r--r--  2.0 unx     7065 b- defN 22-Oct-08 02:27 towhee/models/visualization/transformer_visualization.py
--rw-r--r--  2.0 unx      612 b- defN 22-Aug-19 02:52 towhee/models/vit/__init__.py
--rw-r--r--  2.0 unx    10295 b- defN 22-Oct-08 02:27 towhee/models/vit/vit.py
--rw-r--r--  2.0 unx     3878 b- defN 22-Oct-08 02:27 towhee/models/vit/vit_block.py
--rw-r--r--  2.0 unx     1868 b- defN 22-Aug-19 02:52 towhee/models/vit/vit_utils.py
--rw-r--r--  2.0 unx      617 b- defN 22-Aug-19 02:52 towhee/models/wave_vit/__init__.py
--rw-r--r--  2.0 unx    10014 b- defN 22-Oct-08 02:27 towhee/models/wave_vit/wave_vit.py
--rw-r--r--  2.0 unx     9731 b- defN 22-Aug-19 02:52 towhee/models/wave_vit/wave_vit_block.py
--rw-r--r--  2.0 unx     6361 b- defN 22-Aug-19 02:52 towhee/models/wave_vit/wave_vit_utils.py
--rw-r--r--  2.0 unx    11357 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    13974 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       55 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        7 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    26308 b- defN 23-Jun-09 07:06 towhee.models-1.1.0.dist-info/RECORD
-280 files, 2691090 bytes uncompressed, 1766083 bytes compressed:  34.4%
+Zip file size: 1809120 bytes, number of entries: 280
+-rw-r--r--  2.0 unx     8596 b- defN 23-Jul-04 06:36 towhee/models/README.md
+-rw-r--r--  2.0 unx     8570 b- defN 23-Jul-04 06:36 towhee/models/README_CN.md
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/__init__.py
+-rw-r--r--  2.0 unx      678 b- defN 23-Jul-04 06:36 towhee/models/acar_net/__init__.py
+-rw-r--r--  2.0 unx    10305 b- defN 23-Jul-04 06:36 towhee/models/acar_net/backbone.py
+-rw-r--r--  2.0 unx     9275 b- defN 23-Jul-04 06:36 towhee/models/acar_net/head.py
+-rw-r--r--  2.0 unx     3613 b- defN 23-Jul-04 06:36 towhee/models/acar_net/model.py
+-rw-r--r--  2.0 unx     4102 b- defN 23-Jul-04 06:36 towhee/models/acar_net/neck.py
+-rw-r--r--  2.0 unx     1618 b- defN 23-Jul-04 06:36 towhee/models/acar_net/utils.py
+-rw-r--r--  2.0 unx      709 b- defN 23-Jul-04 06:36 towhee/models/action_clip/__init__.py
+-rw-r--r--  2.0 unx     4305 b- defN 23-Jul-04 06:36 towhee/models/action_clip/action_clip.py
+-rw-r--r--  2.0 unx     2039 b- defN 23-Jul-04 06:36 towhee/models/action_clip/action_clip_utils.py
+-rw-r--r--  2.0 unx     1804 b- defN 23-Jul-04 06:36 towhee/models/action_clip/text_prompt.py
+-rw-r--r--  2.0 unx     8805 b- defN 23-Jul-04 06:36 towhee/models/action_clip/visual_prompt.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/allinone/__init__.py
+-rw-r--r--  2.0 unx     1289 b- defN 23-Jul-04 06:36 towhee/models/allinone/allinone.py
+-rw-r--r--  2.0 unx      622 b- defN 23-Jul-04 06:36 towhee/models/bridgeformer/__init__.py
+-rw-r--r--  2.0 unx     3300 b- defN 23-Jul-04 06:36 towhee/models/bridgeformer/bridge_former.py
+-rw-r--r--  2.0 unx    10421 b- defN 23-Jul-04 06:36 towhee/models/bridgeformer/bridge_former_training.py
+-rw-r--r--  2.0 unx    16155 b- defN 23-Jul-04 06:36 towhee/models/bridgeformer/bridge_former_training_block.py
+-rw-r--r--  2.0 unx     1560 b- defN 23-Jul-04 06:36 towhee/models/clip/README.md
+-rw-r--r--  2.0 unx      639 b- defN 23-Jul-04 06:36 towhee/models/clip/__init__.py
+-rw-r--r--  2.0 unx    20896 b- defN 23-Jul-04 06:36 towhee/models/clip/auxilary.py
+-rw-r--r--  2.0 unx  1356917 b- defN 23-Jul-04 06:36 towhee/models/clip/bpe_simple_vocab_16e6.txt.gz
+-rw-r--r--  2.0 unx    30031 b- defN 23-Jul-04 06:36 towhee/models/clip/clip.py
+-rw-r--r--  2.0 unx     9935 b- defN 23-Jul-04 06:36 towhee/models/clip/clip_utils.py
+-rw-r--r--  2.0 unx     5618 b- defN 23-Jul-04 06:36 towhee/models/clip/simple_tokenizer.py
+-rw-r--r--  2.0 unx       46 b- defN 23-Jul-04 06:36 towhee/models/clip4clip/__init__.py
+-rw-r--r--  2.0 unx    10156 b- defN 23-Jul-04 06:36 towhee/models/clip4clip/clip4clip.py
+-rw-r--r--  2.0 unx     2605 b- defN 23-Jul-04 06:36 towhee/models/clip4clip/until_module.py
+-rw-r--r--  2.0 unx     2536 b- defN 23-Jul-04 06:36 towhee/models/clip4clip/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/coca/__init__.py
+-rw-r--r--  2.0 unx    11514 b- defN 23-Jul-04 06:36 towhee/models/coca/coca.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/coformer/__init__.py
+-rw-r--r--  2.0 unx     5167 b- defN 23-Jul-04 06:36 towhee/models/coformer/backbone.py
+-rw-r--r--  2.0 unx    10222 b- defN 23-Jul-04 06:36 towhee/models/coformer/coformer.py
+-rw-r--r--  2.0 unx    10008 b- defN 23-Jul-04 06:36 towhee/models/coformer/config.py
+-rw-r--r--  2.0 unx    15471 b- defN 23-Jul-04 06:36 towhee/models/coformer/transformer.py
+-rw-r--r--  2.0 unx     2919 b- defN 23-Jul-04 06:36 towhee/models/coformer/utils.py
+-rw-r--r--  2.0 unx      630 b- defN 23-Jul-04 06:36 towhee/models/collaborative_experts/__init__.py
+-rw-r--r--  2.0 unx    60484 b- defN 23-Jul-04 06:36 towhee/models/collaborative_experts/collaborative_experts.py
+-rw-r--r--  2.0 unx     3587 b- defN 23-Jul-04 06:36 towhee/models/collaborative_experts/net_vlad.py
+-rw-r--r--  2.0 unx     1612 b- defN 23-Jul-04 06:36 towhee/models/collaborative_experts/util.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Jul-04 06:36 towhee/models/convnext/__init__.py
+-rw-r--r--  2.0 unx     3245 b- defN 23-Jul-04 06:36 towhee/models/convnext/configs.py
+-rw-r--r--  2.0 unx     4425 b- defN 23-Jul-04 06:36 towhee/models/convnext/convnext.py
+-rw-r--r--  2.0 unx     4107 b- defN 23-Jul-04 06:36 towhee/models/convnext/utils.py
+-rw-r--r--  2.0 unx      614 b- defN 23-Jul-04 06:36 towhee/models/cvnet/__init__.py
+-rw-r--r--  2.0 unx     4665 b- defN 23-Jul-04 06:36 towhee/models/cvnet/cvnet.py
+-rw-r--r--  2.0 unx     6640 b- defN 23-Jul-04 06:36 towhee/models/cvnet/cvnet_block.py
+-rw-r--r--  2.0 unx     2439 b- defN 23-Jul-04 06:36 towhee/models/cvnet/cvnet_utils.py
+-rw-r--r--  2.0 unx     8106 b- defN 23-Jul-04 06:36 towhee/models/cvnet/resnet.py
+-rw-r--r--  2.0 unx       19 b- defN 23-Jul-04 06:36 towhee/models/drl/__init__.py
+-rw-r--r--  2.0 unx    35156 b- defN 23-Jul-04 06:36 towhee/models/drl/drl.py
+-rw-r--r--  2.0 unx     8633 b- defN 23-Jul-04 06:36 towhee/models/drl/module_cross.py
+-rw-r--r--  2.0 unx     4891 b- defN 23-Jul-04 06:36 towhee/models/drl/until_module.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/embedding/__init__.py
+-rw-r--r--  2.0 unx     2061 b- defN 23-Jul-04 06:36 towhee/models/embedding/embedding_extractor.py
+-rw-r--r--  2.0 unx      623 b- defN 23-Jul-04 06:36 towhee/models/frozen_in_time/__init__.py
+-rw-r--r--  2.0 unx    14921 b- defN 23-Jul-04 06:36 towhee/models/frozen_in_time/frozen_in_time.py
+-rw-r--r--  2.0 unx     2394 b- defN 23-Jul-04 06:36 towhee/models/frozen_in_time/frozen_utils.py
+-rw-r--r--  2.0 unx    15910 b- defN 23-Jul-04 06:36 towhee/models/frozen_in_time/frozen_video_transformer.py
+-rw-r--r--  2.0 unx      659 b- defN 23-Jul-04 06:36 towhee/models/hornet/__init__.py
+-rw-r--r--  2.0 unx     4309 b- defN 23-Jul-04 06:36 towhee/models/hornet/configs.py
+-rw-r--r--  2.0 unx     5036 b- defN 23-Jul-04 06:36 towhee/models/hornet/hornet.py
+-rw-r--r--  2.0 unx     5983 b- defN 23-Jul-04 06:36 towhee/models/hornet/utils.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Jul-04 06:36 towhee/models/isc/__init__.py
+-rw-r--r--  2.0 unx     3470 b- defN 23-Jul-04 06:36 towhee/models/isc/isc.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/layers/__init__.py
+-rw-r--r--  2.0 unx     1638 b- defN 23-Jul-04 06:36 towhee/models/layers/aspp.py
+-rw-r--r--  2.0 unx     6693 b- defN 23-Jul-04 06:36 towhee/models/layers/attention.py
+-rw-r--r--  2.0 unx     5783 b- defN 23-Jul-04 06:36 towhee/models/layers/cond_conv2d.py
+-rw-r--r--  2.0 unx     2706 b- defN 23-Jul-04 06:36 towhee/models/layers/conv2d_same.py
+-rw-r--r--  2.0 unx     4180 b- defN 23-Jul-04 06:36 towhee/models/layers/conv2d_separable.py
+-rw-r--r--  2.0 unx     4354 b- defN 23-Jul-04 06:36 towhee/models/layers/conv4d.py
+-rw-r--r--  2.0 unx     5125 b- defN 23-Jul-04 06:36 towhee/models/layers/conv_bn_activation.py
+-rw-r--r--  2.0 unx     1918 b- defN 23-Jul-04 06:36 towhee/models/layers/convmlp.py
+-rw-r--r--  2.0 unx     4369 b- defN 23-Jul-04 06:36 towhee/models/layers/cross_attention.py
+-rw-r--r--  2.0 unx     5964 b- defN 23-Jul-04 06:36 towhee/models/layers/dropblock2d.py
+-rw-r--r--  2.0 unx     2200 b- defN 23-Jul-04 06:36 towhee/models/layers/droppath.py
+-rw-r--r--  2.0 unx     2175 b- defN 23-Jul-04 06:36 towhee/models/layers/ffn.py
+-rw-r--r--  2.0 unx     2052 b- defN 23-Jul-04 06:36 towhee/models/layers/gatedmlp.py
+-rw-r--r--  2.0 unx    10670 b- defN 23-Jul-04 06:36 towhee/models/layers/layers_with_relprop.py
+-rw-r--r--  2.0 unx     4347 b- defN 23-Jul-04 06:36 towhee/models/layers/mbconv.py
+-rw-r--r--  2.0 unx     2971 b- defN 23-Jul-04 06:36 towhee/models/layers/mixed_conv2d.py
+-rw-r--r--  2.0 unx     2566 b- defN 23-Jul-04 06:36 towhee/models/layers/mlp.py
+-rw-r--r--  2.0 unx    13234 b- defN 23-Jul-04 06:36 towhee/models/layers/multi_scale_attention.py
+-rw-r--r--  2.0 unx     6651 b- defN 23-Jul-04 06:36 towhee/models/layers/multi_scale_transformer_block.py
+-rw-r--r--  2.0 unx     3312 b- defN 23-Jul-04 06:36 towhee/models/layers/netvlad.py
+-rw-r--r--  2.0 unx     6047 b- defN 23-Jul-04 06:36 towhee/models/layers/non_local.py
+-rw-r--r--  2.0 unx     5759 b- defN 23-Jul-04 06:36 towhee/models/layers/padding_functions.py
+-rw-r--r--  2.0 unx     4015 b- defN 23-Jul-04 06:36 towhee/models/layers/patch_embed2d.py
+-rw-r--r--  2.0 unx     3620 b- defN 23-Jul-04 06:36 towhee/models/layers/patch_embed3d.py
+-rw-r--r--  2.0 unx     2458 b- defN 23-Jul-04 06:36 towhee/models/layers/patch_merging.py
+-rw-r--r--  2.0 unx     3009 b- defN 23-Jul-04 06:36 towhee/models/layers/patch_merging3d.py
+-rw-r--r--  2.0 unx     3343 b- defN 23-Jul-04 06:36 towhee/models/layers/pool_attention.py
+-rw-r--r--  2.0 unx     3084 b- defN 23-Jul-04 06:36 towhee/models/layers/position_encoding.py
+-rw-r--r--  2.0 unx     4749 b- defN 23-Jul-04 06:36 towhee/models/layers/relative_self_attention.py
+-rw-r--r--  2.0 unx     2028 b- defN 23-Jul-04 06:36 towhee/models/layers/resnet_basic_3d_module.py
+-rw-r--r--  2.0 unx     3273 b- defN 23-Jul-04 06:36 towhee/models/layers/sam.py
+-rw-r--r--  2.0 unx     1500 b- defN 23-Jul-04 06:36 towhee/models/layers/sequence_pool.py
+-rw-r--r--  2.0 unx     3663 b- defN 23-Jul-04 06:36 towhee/models/layers/spatial_temporal_cls_positional_encoding.py
+-rw-r--r--  2.0 unx      639 b- defN 23-Jul-04 06:36 towhee/models/layers/spp.py
+-rw-r--r--  2.0 unx     5934 b- defN 23-Jul-04 06:36 towhee/models/layers/swin_transformer_block3d.py
+-rw-r--r--  2.0 unx     1964 b- defN 23-Jul-04 06:36 towhee/models/layers/temporal_cg_avgpool3d.py
+-rw-r--r--  2.0 unx     1964 b- defN 23-Jul-04 06:36 towhee/models/layers/tf_avgpool3d.py
+-rw-r--r--  2.0 unx     2351 b- defN 23-Jul-04 06:36 towhee/models/layers/time2vec.py
+-rw-r--r--  2.0 unx     2054 b- defN 23-Jul-04 06:36 towhee/models/layers/transformer_encoder.py
+-rw-r--r--  2.0 unx     3223 b- defN 23-Jul-04 06:36 towhee/models/layers/vision_transformer_basic_head.py
+-rw-r--r--  2.0 unx     8064 b- defN 23-Jul-04 06:36 towhee/models/layers/window_attention.py
+-rw-r--r--  2.0 unx     5352 b- defN 23-Jul-04 06:36 towhee/models/layers/window_attention3d.py
+-rw-r--r--  2.0 unx     1202 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/__init__.py
+-rw-r--r--  2.0 unx     1312 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/gelu.py
+-rw-r--r--  2.0 unx     1393 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/hardmish.py
+-rw-r--r--  2.0 unx     1482 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/hardsigmoid.py
+-rw-r--r--  2.0 unx     1424 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/hardswish.py
+-rw-r--r--  2.0 unx     1328 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/mish.py
+-rw-r--r--  2.0 unx     1320 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/prelu.py
+-rw-r--r--  2.0 unx     1378 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/sigmoid.py
+-rw-r--r--  2.0 unx      815 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/swiglu.py
+-rw-r--r--  2.0 unx     1287 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/swish.py
+-rw-r--r--  2.0 unx     1290 b- defN 23-Jul-04 06:36 towhee/models/layers/activations/tanh.py
+-rw-r--r--  2.0 unx       26 b- defN 23-Jul-04 06:36 towhee/models/lightning_dot/__init__.py
+-rw-r--r--  2.0 unx     5800 b- defN 23-Jul-04 06:36 towhee/models/lightning_dot/bi_encoder.py
+-rw-r--r--  2.0 unx      593 b- defN 23-Jul-04 06:36 towhee/models/loss/__init__.py
+-rw-r--r--  2.0 unx     2467 b- defN 23-Jul-04 06:36 towhee/models/loss/focal_loss.py
+-rw-r--r--  2.0 unx      616 b- defN 23-Jul-04 06:36 towhee/models/max_vit/__init__.py
+-rw-r--r--  2.0 unx     1802 b- defN 23-Jul-04 06:36 towhee/models/max_vit/configs.py
+-rw-r--r--  2.0 unx     8066 b- defN 23-Jul-04 06:36 towhee/models/max_vit/max_vit.py
+-rw-r--r--  2.0 unx    11404 b- defN 23-Jul-04 06:36 towhee/models/max_vit/max_vit_block.py
+-rw-r--r--  2.0 unx     5382 b- defN 23-Jul-04 06:36 towhee/models/max_vit/max_vit_utils.py
+-rw-r--r--  2.0 unx      702 b- defN 23-Jul-04 06:36 towhee/models/mcprop/__init__.py
+-rw-r--r--  2.0 unx     3053 b- defN 23-Jul-04 06:36 towhee/models/mcprop/depthaggregator.py
+-rw-r--r--  2.0 unx     2388 b- defN 23-Jul-04 06:36 towhee/models/mcprop/featurefusion.py
+-rw-r--r--  2.0 unx     1413 b- defN 23-Jul-04 06:36 towhee/models/mcprop/imageextractor.py
+-rw-r--r--  2.0 unx     3070 b- defN 23-Jul-04 06:36 towhee/models/mcprop/loss.py
+-rw-r--r--  2.0 unx     6296 b- defN 23-Jul-04 06:36 towhee/models/mcprop/matching.py
+-rw-r--r--  2.0 unx     1399 b- defN 23-Jul-04 06:36 towhee/models/mcprop/textextractor.py
+-rw-r--r--  2.0 unx     2029 b- defN 23-Jul-04 06:36 towhee/models/mcprop/transformerpooling.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/mdmmt/__init__.py
+-rw-r--r--  2.0 unx    15492 b- defN 23-Jul-04 06:36 towhee/models/mdmmt/bert_mmt.py
+-rw-r--r--  2.0 unx    13512 b- defN 23-Jul-04 06:36 towhee/models/mdmmt/mmt.py
+-rw-r--r--  2.0 unx     1602 b- defN 23-Jul-04 06:36 towhee/models/metaformer/addpositionembed.py
+-rw-r--r--  2.0 unx     2453 b- defN 23-Jul-04 06:36 towhee/models/metaformer/attention.py
+-rw-r--r--  2.0 unx     1803 b- defN 23-Jul-04 06:36 towhee/models/metaformer/basicblocks.py
+-rw-r--r--  2.0 unx    10807 b- defN 23-Jul-04 06:36 towhee/models/metaformer/metaformer.py
+-rw-r--r--  2.0 unx     3109 b- defN 23-Jul-04 06:36 towhee/models/metaformer/metaformerblock.py
+-rw-r--r--  2.0 unx     1684 b- defN 23-Jul-04 06:36 towhee/models/metaformer/spatialfc.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/movinet/__init__.py
+-rw-r--r--  2.0 unx    27214 b- defN 23-Jul-04 06:36 towhee/models/movinet/config.py
+-rw-r--r--  2.0 unx     8063 b- defN 23-Jul-04 06:36 towhee/models/movinet/movinet.py
+-rw-r--r--  2.0 unx    13711 b- defN 23-Jul-04 06:36 towhee/models/movinet/movinet_block.py
+-rw-r--r--  2.0 unx       21 b- defN 23-Jul-04 06:36 towhee/models/mpvit/__init__.py
+-rw-r--r--  2.0 unx    28795 b- defN 23-Jul-04 06:36 towhee/models/mpvit/mpvit.py
+-rw-r--r--  2.0 unx       47 b- defN 23-Jul-04 06:36 towhee/models/multiscale_vision_transformers/__init__.py
+-rw-r--r--  2.0 unx     4509 b- defN 23-Jul-04 06:36 towhee/models/multiscale_vision_transformers/create_mvit.py
+-rw-r--r--  2.0 unx    34605 b- defN 23-Jul-04 06:36 towhee/models/multiscale_vision_transformers/mvit.py
+-rw-r--r--  2.0 unx      613 b- defN 23-Jul-04 06:36 towhee/models/nnfp/__init__.py
+-rw-r--r--  2.0 unx     5235 b- defN 23-Jul-04 06:36 towhee/models/nnfp/nnfp.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/omnivore/__init__.py
+-rw-r--r--  2.0 unx    15170 b- defN 23-Jul-04 06:36 towhee/models/omnivore/omnivore.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/perceiver/__init__.py
+-rw-r--r--  2.0 unx     1446 b- defN 23-Jul-04 06:36 towhee/models/perceiver/create_cross_attention.py
+-rw-r--r--  2.0 unx     1277 b- defN 23-Jul-04 06:36 towhee/models/perceiver/create_self_attention.py
+-rw-r--r--  2.0 unx      979 b- defN 23-Jul-04 06:36 towhee/models/perceiver/create_self_attention_block.py
+-rw-r--r--  2.0 unx     1233 b- defN 23-Jul-04 06:36 towhee/models/perceiver/cross_attention.py
+-rw-r--r--  2.0 unx      344 b- defN 23-Jul-04 06:36 towhee/models/perceiver/mlp.py
+-rw-r--r--  2.0 unx     1435 b- defN 23-Jul-04 06:36 towhee/models/perceiver/multi_head_attention.py
+-rw-r--r--  2.0 unx      667 b- defN 23-Jul-04 06:36 towhee/models/perceiver/residual.py
+-rw-r--r--  2.0 unx     1234 b- defN 23-Jul-04 06:36 towhee/models/perceiver/self_attention.py
+-rw-r--r--  2.0 unx      434 b- defN 23-Jul-04 06:36 towhee/models/perceiver/sequential.py
+-rw-r--r--  2.0 unx     1767 b- defN 23-Jul-04 06:36 towhee/models/poolformer/basic_blocks.py
+-rw-r--r--  2.0 unx      978 b- defN 23-Jul-04 06:36 towhee/models/poolformer/groupnorm.py
+-rw-r--r--  2.0 unx     1492 b- defN 23-Jul-04 06:36 towhee/models/poolformer/layernormchannel.py
+-rw-r--r--  2.0 unx     1839 b- defN 23-Jul-04 06:36 towhee/models/poolformer/mlp.py
+-rw-r--r--  2.0 unx     1558 b- defN 23-Jul-04 06:36 towhee/models/poolformer/patchembed.py
+-rw-r--r--  2.0 unx    10490 b- defN 23-Jul-04 06:36 towhee/models/poolformer/poolformer.py
+-rw-r--r--  2.0 unx     3034 b- defN 23-Jul-04 06:36 towhee/models/poolformer/poolformerblock.py
+-rw-r--r--  2.0 unx     1122 b- defN 23-Jul-04 06:36 towhee/models/poolformer/pooling.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Jul-04 06:36 towhee/models/replknet/__init__.py
+-rw-r--r--  2.0 unx     2718 b- defN 23-Jul-04 06:36 towhee/models/replknet/configs.py
+-rw-r--r--  2.0 unx     8942 b- defN 23-Jul-04 06:36 towhee/models/replknet/replknet.py
+-rw-r--r--  2.0 unx    10184 b- defN 23-Jul-04 06:36 towhee/models/replknet/utils.py
+-rw-r--r--  2.0 unx      637 b- defN 23-Jul-04 06:36 towhee/models/repmlp/__init__.py
+-rw-r--r--  2.0 unx     8448 b- defN 23-Jul-04 06:36 towhee/models/repmlp/blocks.py
+-rw-r--r--  2.0 unx     2339 b- defN 23-Jul-04 06:36 towhee/models/repmlp/configs.py
+-rw-r--r--  2.0 unx     7259 b- defN 23-Jul-04 06:36 towhee/models/repmlp/repmlp.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/retina_face/__init__.py
+-rw-r--r--  2.0 unx     2633 b- defN 23-Jul-04 06:36 towhee/models/retina_face/configs.py
+-rw-r--r--  2.0 unx     3052 b- defN 23-Jul-04 06:36 towhee/models/retina_face/heads.py
+-rw-r--r--  2.0 unx     2557 b- defN 23-Jul-04 06:36 towhee/models/retina_face/mobilenet_v1.py
+-rw-r--r--  2.0 unx     2694 b- defN 23-Jul-04 06:36 towhee/models/retina_face/prior_box.py
+-rw-r--r--  2.0 unx     7031 b- defN 23-Jul-04 06:36 towhee/models/retina_face/retinaface.py
+-rw-r--r--  2.0 unx     2850 b- defN 23-Jul-04 06:36 towhee/models/retina_face/retinaface_fpn.py
+-rw-r--r--  2.0 unx     2640 b- defN 23-Jul-04 06:36 towhee/models/retina_face/ssh.py
+-rw-r--r--  2.0 unx     5694 b- defN 23-Jul-04 06:36 towhee/models/retina_face/utils.py
+-rw-r--r--  2.0 unx      672 b- defN 23-Jul-04 06:36 towhee/models/shunted_transformer/__init__.py
+-rw-r--r--  2.0 unx     2134 b- defN 23-Jul-04 06:36 towhee/models/shunted_transformer/configs.py
+-rw-r--r--  2.0 unx     5411 b- defN 23-Jul-04 06:36 towhee/models/shunted_transformer/shunted_transformer.py
+-rw-r--r--  2.0 unx    10283 b- defN 23-Jul-04 06:36 towhee/models/shunted_transformer/utils.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Jul-04 06:36 towhee/models/svt/__init__.py
+-rw-r--r--  2.0 unx     2925 b- defN 23-Jul-04 06:36 towhee/models/svt/svt.py
+-rw-r--r--  2.0 unx     1375 b- defN 23-Jul-04 06:36 towhee/models/svt/svt_utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/swin_transformer/__init__.py
+-rw-r--r--  2.0 unx     3558 b- defN 23-Jul-04 06:36 towhee/models/swin_transformer/basic_layer.py
+-rw-r--r--  2.0 unx    10536 b- defN 23-Jul-04 06:36 towhee/models/swin_transformer/configs.py
+-rw-r--r--  2.0 unx     7134 b- defN 23-Jul-04 06:36 towhee/models/swin_transformer/model.py
+-rw-r--r--  2.0 unx     6114 b- defN 23-Jul-04 06:36 towhee/models/swin_transformer/swin_transformer_block.py
+-rw-r--r--  2.0 unx      620 b- defN 23-Jul-04 06:36 towhee/models/timesformer/__init__.py
+-rw-r--r--  2.0 unx     9537 b- defN 23-Jul-04 06:36 towhee/models/timesformer/timesformer.py
+-rw-r--r--  2.0 unx     6249 b- defN 23-Jul-04 06:36 towhee/models/timesformer/timesformer_block.py
+-rw-r--r--  2.0 unx     9090 b- defN 23-Jul-04 06:36 towhee/models/timesformer/timesformer_utils.py
+-rw-r--r--  2.0 unx      638 b- defN 23-Jul-04 06:36 towhee/models/transrac/__init__.py
+-rw-r--r--  2.0 unx     5724 b- defN 23-Jul-04 06:36 towhee/models/transrac/transrac.py
+-rw-r--r--  2.0 unx     3378 b- defN 23-Jul-04 06:36 towhee/models/transrac/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/tsm/__init__.py
+-rw-r--r--  2.0 unx     4683 b- defN 23-Jul-04 06:36 towhee/models/tsm/config.py
+-rw-r--r--  2.0 unx     5394 b- defN 23-Jul-04 06:36 towhee/models/tsm/mobilenet_v2.py
+-rw-r--r--  2.0 unx     5815 b- defN 23-Jul-04 06:36 towhee/models/tsm/temporal_shift.py
+-rw-r--r--  2.0 unx    13942 b- defN 23-Jul-04 06:36 towhee/models/tsm/tsm.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/uniformer/__init__.py
+-rw-r--r--  2.0 unx     4337 b- defN 23-Jul-04 06:36 towhee/models/uniformer/config.py
+-rw-r--r--  2.0 unx    21976 b- defN 23-Jul-04 06:36 towhee/models/uniformer/uniformer.py
+-rw-r--r--  2.0 unx       89 b- defN 23-Jul-04 06:36 towhee/models/utils/__init__.py
+-rw-r--r--  2.0 unx     4633 b- defN 23-Jul-04 06:36 towhee/models/utils/audio_preprocess.py
+-rw-r--r--  2.0 unx     1488 b- defN 23-Jul-04 06:36 towhee/models/utils/basic_ops.py
+-rw-r--r--  2.0 unx      808 b- defN 23-Jul-04 06:36 towhee/models/utils/causal_module.py
+-rw-r--r--  2.0 unx     4368 b- defN 23-Jul-04 06:36 towhee/models/utils/create_act.py
+-rw-r--r--  2.0 unx     2159 b- defN 23-Jul-04 06:36 towhee/models/utils/create_conv2d.py
+-rw-r--r--  2.0 unx     1328 b- defN 23-Jul-04 06:36 towhee/models/utils/create_conv2d_pad.py
+-rw-r--r--  2.0 unx     1664 b- defN 23-Jul-04 06:36 towhee/models/utils/create_model.py
+-rw-r--r--  2.0 unx     3929 b- defN 23-Jul-04 06:36 towhee/models/utils/create_resnet_basic_3d_module.py
+-rw-r--r--  2.0 unx     3658 b- defN 23-Jul-04 06:36 towhee/models/utils/download.py
+-rw-r--r--  2.0 unx     1208 b- defN 23-Jul-04 06:36 towhee/models/utils/fuse_bn.py
+-rw-r--r--  2.0 unx     1153 b- defN 23-Jul-04 06:36 towhee/models/utils/gelu_ignore_parameters.py
+-rw-r--r--  2.0 unx     1370 b- defN 23-Jul-04 06:36 towhee/models/utils/general_utils.py
+-rw-r--r--  2.0 unx     1720 b- defN 23-Jul-04 06:36 towhee/models/utils/get_relative_position_index.py
+-rw-r--r--  2.0 unx     1545 b- defN 23-Jul-04 06:36 towhee/models/utils/get_window_size.py
+-rw-r--r--  2.0 unx     2642 b- defN 23-Jul-04 06:36 towhee/models/utils/init_vit_weights.py
+-rw-r--r--  2.0 unx     2032 b- defN 23-Jul-04 06:36 towhee/models/utils/pretrained_utils.py
+-rw-r--r--  2.0 unx     1556 b- defN 23-Jul-04 06:36 towhee/models/utils/round_width.py
+-rw-r--r--  2.0 unx     8505 b- defN 23-Jul-04 06:36 towhee/models/utils/video_transforms.py
+-rw-r--r--  2.0 unx     3957 b- defN 23-Jul-04 06:36 towhee/models/utils/weight_init.py
+-rw-r--r--  2.0 unx     1114 b- defN 23-Jul-04 06:36 towhee/models/utils/window_partition.py
+-rw-r--r--  2.0 unx     1478 b- defN 23-Jul-04 06:36 towhee/models/utils/window_partition3d.py
+-rw-r--r--  2.0 unx     1277 b- defN 23-Jul-04 06:36 towhee/models/utils/window_reverse.py
+-rw-r--r--  2.0 unx     1474 b- defN 23-Jul-04 06:36 towhee/models/utils/window_reverse3d.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/vggish/__init__.py
+-rw-r--r--  2.0 unx     2049 b- defN 23-Jul-04 06:36 towhee/models/vggish/torch_vggish.py
+-rw-r--r--  2.0 unx      824 b- defN 23-Jul-04 06:36 towhee/models/video_swin_transformer/__init__.py
+-rw-r--r--  2.0 unx     1643 b- defN 23-Jul-04 06:36 towhee/models/video_swin_transformer/compute_mask.py
+-rw-r--r--  2.0 unx     3992 b- defN 23-Jul-04 06:36 towhee/models/video_swin_transformer/get_configs.py
+-rw-r--r--  2.0 unx    14573 b- defN 23-Jul-04 06:36 towhee/models/video_swin_transformer/video_swin_transformer.py
+-rw-r--r--  2.0 unx     3925 b- defN 23-Jul-04 06:36 towhee/models/video_swin_transformer/video_swin_transformer_block.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/violet/__init__.py
+-rw-r--r--  2.0 unx     3916 b- defN 23-Jul-04 06:36 towhee/models/violet/violet.py
+-rw-r--r--  2.0 unx     1007 b- defN 23-Jul-04 06:36 towhee/models/vis4mer/__init__.py
+-rw-r--r--  2.0 unx     1471 b- defN 23-Jul-04 06:36 towhee/models/vis4mer/activation.py
+-rw-r--r--  2.0 unx     1944 b- defN 23-Jul-04 06:36 towhee/models/vis4mer/get_initializer.py
+-rw-r--r--  2.0 unx     2347 b- defN 23-Jul-04 06:36 towhee/models/vis4mer/linearactivation.py
+-rw-r--r--  2.0 unx     1662 b- defN 23-Jul-04 06:36 towhee/models/vis4mer/transposelinear.py
+-rw-r--r--  2.0 unx    10305 b- defN 23-Jul-04 06:36 towhee/models/vis4mer/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-04 06:36 towhee/models/visualization/__init__.py
+-rw-r--r--  2.0 unx     8524 b- defN 23-Jul-04 06:36 towhee/models/visualization/clip_visualization.py
+-rw-r--r--  2.0 unx     3193 b- defN 23-Jul-04 06:36 towhee/models/visualization/embedding_visualization.py
+-rw-r--r--  2.0 unx     7065 b- defN 23-Jul-04 06:36 towhee/models/visualization/transformer_visualization.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Jul-04 06:36 towhee/models/vit/__init__.py
+-rw-r--r--  2.0 unx    10295 b- defN 23-Jul-04 06:36 towhee/models/vit/vit.py
+-rw-r--r--  2.0 unx     3878 b- defN 23-Jul-04 06:36 towhee/models/vit/vit_block.py
+-rw-r--r--  2.0 unx     1868 b- defN 23-Jul-04 06:36 towhee/models/vit/vit_utils.py
+-rw-r--r--  2.0 unx      617 b- defN 23-Jul-04 06:36 towhee/models/wave_vit/__init__.py
+-rw-r--r--  2.0 unx    10014 b- defN 23-Jul-04 06:36 towhee/models/wave_vit/wave_vit.py
+-rw-r--r--  2.0 unx     9731 b- defN 23-Jul-04 06:36 towhee/models/wave_vit/wave_vit_block.py
+-rw-r--r--  2.0 unx     6361 b- defN 23-Jul-04 06:36 towhee/models/wave_vit/wave_vit_utils.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-Jul-04 10:18 towhee.models-1.1.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    18516 b- defN 23-Jul-04 10:18 towhee.models-1.1.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-04 10:18 towhee.models-1.1.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       55 b- defN 23-Jul-04 10:18 towhee.models-1.1.1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 23-Jul-04 10:18 towhee.models-1.1.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    26308 b- defN 23-Jul-04 10:18 towhee.models-1.1.1.dist-info/RECORD
+280 files, 2695632 bytes uncompressed, 1766892 bytes compressed:  34.5%
```

## zipnote {}

```diff
@@ -816,26 +816,26 @@
 
 Filename: towhee/models/wave_vit/wave_vit_block.py
 Comment: 
 
 Filename: towhee/models/wave_vit/wave_vit_utils.py
 Comment: 
 
-Filename: towhee.models-1.1.0.dist-info/LICENSE
+Filename: towhee.models-1.1.1.dist-info/LICENSE
 Comment: 
 
-Filename: towhee.models-1.1.0.dist-info/METADATA
+Filename: towhee.models-1.1.1.dist-info/METADATA
 Comment: 
 
-Filename: towhee.models-1.1.0.dist-info/WHEEL
+Filename: towhee.models-1.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: towhee.models-1.1.0.dist-info/entry_points.txt
+Filename: towhee.models-1.1.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: towhee.models-1.1.0.dist-info/top_level.txt
+Filename: towhee.models-1.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: towhee.models-1.1.0.dist-info/RECORD
+Filename: towhee.models-1.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## Comparing `towhee.models-1.1.0.dist-info/LICENSE` & `towhee.models-1.1.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `towhee.models-1.1.0.dist-info/METADATA` & `towhee.models-1.1.1.dist-info/METADATA`

 * *Files 24% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: towhee.models
-Version: 1.1.0
+Version: 1.1.1
 Summary: Towhee is a framework that helps you encode your unstructured data into embeddings.
 Home-page: https://github.com/towhee-io/towhee
 Author: Towhee Team
 Author-email: towhee-team@zilliz.com
 License: http://www.apache.org/licenses/LICENSE-2.0
 Platform: unix
 Platform: linux
@@ -14,15 +14,15 @@
 License-File: LICENSE
 Requires-Dist: requests (>=2.12.5)
 Requires-Dist: tqdm (>=4.59.0)
 Requires-Dist: tabulate
 Requires-Dist: numpy
 Requires-Dist: twine
 Requires-Dist: tenacity
-Requires-Dist: pydantic
+Requires-Dist: pydantic (<2)
 Requires-Dist: contextvars ; python_version <= "3.6"
 Requires-Dist: importlib-resources ; python_version<'3.7'
 
 &nbsp;
 
 <p align="center">
     <img src="towhee_logo.png#gh-light-mode-only" width="60%"/>
@@ -47,80 +47,169 @@
   <a href="https://twitter.com/towheeio">
     <img src="https://img.shields.io/badge/follow-twitter-blue?style=flat" alt="twitter"/>
   </a>
   <a href="https://www.apache.org/licenses/LICENSE-2.0">
     <img src="https://img.shields.io/badge/license-apache2.0-green?style=flat" alt="license"/>
   </a>
   <a href="https://github.com/towhee-io/towhee/actions/workflows/pylint.yml">
-    <img src="https://github.com/towhee-io/towhee/actions/workflows/pylint.yml/badge.svg" alt="github actions"/>
-  </a>
-  <a href="https://pypi.org/project/towhee/">
-    <img src="https://img.shields.io/pypi/v/towhee?label=Release&color&logo=Python" alt="github actions"/>
+    <img src="https://img.shields.io/github/workflow/status/towhee-io/towhee/Workflow%20for%20pylint/main?label=pylint&style=flat" alt="github actions"/>
   </a>
   <a href="https://app.codecov.io/gh/towhee-io/towhee">
     <img src="https://img.shields.io/codecov/c/github/towhee-io/towhee?style=flat" alt="coverage"/>
   </a>
 </div>
 
 &nbsp;
 
-[Towhee](https://towhee.io) is a cutting-edge framework designed to streamline the processing of unstructured data through the use of Large Language Model (LLM) based pipeline orchestration. It is uniquely positioned to extract invaluable insights from diverse unstructured data types, including lengthy text, images, audio and video files. Leveraging the capabilities of generative AI and the SOTA deep learning models, Towhee is capable of transforming this unprocessed data into specific formats such as text, image, or embeddings. These can then be efficiently loaded into an appropriate storage system like a vector database. Developers can initially build an intuitive data processing pipeline prototype with user friendly Pythonic APU, then optimize it for production environments.
-
-🎨 Multi Modalities: Towhee is capable of handling a wide range of data types. Whether it's image data, video clips, text, audio files, or even molecular structures, Towhee can process them all. 
-
-📃    LLM Pipeline orchestration:  Towhee offers flexibility to adapt to different Large Language Models (LLMs). Additionally, it allows for hosting open-source large models locally. Moreover, Towhee provides features like prompt management and knowledge retrieval, making the interaction with these LLMs more efficient and effective.
-
-🎓 Rich Operators: Towhee provides a wide range of ready-to-use state-of-the-art models across five domains: CV, NLP, multimodal, audio, and medical. With over 140 models like BERT and CLIP and rich functionalities like video decoding, audio slicing, frame sampling,  and dimensionality reduction, it assists in efficiently building data processing pipelines. 
-
-🔌   Prebuilt ETL Pipelines: Towhee offers ready-to-use ETL (Extract, Transform, Load) pipelines for common tasks such as Retrieval-Augmented Generation, Text Image search, and Video copy detection. This means you don't need to be an AI expert to build applications using these features. 
-⚡️  High performance backend: Leveraging the power of the Triton Inference Server, Towhee can speed up model serving on both CPU and GPU using platforms like TensorRT, Pytorch, and ONNX. Moreover, you can transform your Python pipeline into a high-performance docker container with just a few lines of code, enabling efficient deployment and scaling.
-
-🐍 Pythonic API: Towhee includes a Pythonic method-chaining API for describing custom data processing pipelines. We also support schemas, which makes processing unstructured data as easy as handling tabular data.
+[Towhee](https://towhee.io) makes it easy to build neural data processing pipelines for AI applications.
+We provide hundreds of models, algorithms, and transformations that can be used as standard pipeline building blocks.
+You can use Towhee's Pythonic API to build a prototype of your pipeline and
+automatically optimize it for production-ready environments.
+
+:art:&emsp;**Various Modalities:** Towhee supports data processing on a variety of modalities, including images, videos, text, audio, molecular structures, etc.
+
+:mortar_board:&emsp;**SOTA Models:** Towhee provides SOTA models across 5 fields (CV, NLP, Multimodal, Audio, Medical), 15 tasks, and 140+ model architectures. These include BERT, CLIP, ViT, SwinTransformer, MAE, and data2vec, all pretrained and ready to use.
+
+:package:&emsp;**Data Processing:** Towhee also provides traditional methods alongside neural network models to help you build practical data processing pipelines. We have a rich pool of operators available, such as video decoding, audio slicing, frame sampling, feature vector dimension reduction, ensembling, and database operations.
+
+:snake:&emsp;**Pythonic API:** Towhee includes a Pythonic method-chaining API for describing custom data processing pipelines. We also support schemas, which makes processing unstructured data as easy as handling tabular data.
+
+## What's New
+**v1.0.0rc1 May. 4, 2023**
+* Add trainer to operators: 
+[*timm*](https://towhee.io/image-embedding/timm), [*isc*](https://towhee.io/image-embedding/isc), [*transformers*](https://towhee.io/text-embedding/transformers), [*clip*](https://towhee.io/image-text-embedding/clip)
+* Add GPU video decoder: 
+[*VPF*](https://towhee.io/video-decode/VPF)
+* All towhee pipelines can be converted into Nvidia Triton services.
+
+
+**v0.9.0 Dec. 2, 2022**
+* Added one video classification model:
+[*Vis4mer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/vis4mer)
+* Added three visual backbones:
+[*MCProp*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/mcprop), 
+[*RepLKNet*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/replknet), 
+[*Shunted Transformer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/shunted_transformer)
+* Add two code search operators:
+[*code_search.codebert*](https://towhee.io/code-search/codebert), 
+[*code_search.unixcoder*](https://towhee.io/code-search/unixcoder)
+* Add five image captioning operators: 
+[*image_captioning.expansionnet-v2*](https://towhee.io/image-captioning/expansionnet-v2), 
+[*image_captioning.magic*](https://towhee.io/image-captioning/magic),
+[*image_captioning.clip_caption_reward*](https://towhee.io/image-captioning/clip-caption-reward), 
+[*image_captioning.blip*](https://towhee.io/image-captioning/blip), 
+[*image_captioning.clipcap*](https://towhee.io/image-captioning/clipcap)
+* Add five image-text embedding operators: 
+[*image_text_embedding.albef*](https://towhee.io/image-text-embedding/albef), 
+[*image_text_embedding.ru_clip*](https://towhee.io/image-text-embedding/ru-clip), 
+[*image_text_embedding.japanese_clip*](https://towhee.io/image-text-embedding/japanese-clip),
+[*image_text_embedding.taiyi*](https://towhee.io/image-text-embedding/taiyi),
+[*image_text_embedding.slip*](https://towhee.io/image-text-embedding/slip)
+* Add one machine-translation operator: 
+[*machine_translation.opus_mt*](https://towhee.io/machine-translation/opus-mt)
+* Add one filter-tiny-segments operator:
+[*video-copy-detection.filter-tiny-segments*](https://towhee.io/video-copy-detection/filter-tiny-segments)
+* Add an advanced tutorial for audio fingerprinting: 
+[*Audio Fingerprint II: Music Detection with Temporal Localization*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_advanced.ipynb) (increased accuracy from 84% to 90%)
+
+**v0.8.1 Sep. 30, 2022**
+
+* Added four visual backbones:
+[*ISC*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/isc),
+[*MetaFormer*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/metaformer),
+[*ConvNext*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/convnext),
+[*HorNet*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/hornet)
+* Add two video de-copy operators:
+[*select-video*](https://towhee.io/video-copy-detection/select-video), 
+[*temporal-network*](https://towhee.io/video-copy-detection/temporal-network)
+* Add one image embedding operator specifically designed for image retrieval and video de-copy with SOTA performance on VCSL dataset:
+[*isc*](https://towhee.io/image-embedding/isc)
+* Add one audio embedding operator specified for audio fingerprint:
+[*audio_embedding.nnfp*](https://towhee.io/audio-embedding/nnfp) (with pretrained weights)
+* Add one tutorial for video de-copy: 
+[*How to Build a Video Segment Copy Detection System*](https://github.com/towhee-io/examples/blob/main/video/video_deduplication/segment_level/video_deduplication_at_segment_level.ipynb)
+* Add one beginner tutorial for audio fingerprint:
+[*Audio Fingerprint I: Build a Demo with Towhee & Milvus*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_beginner.ipynb)
+
+
+**v0.8.0 Aug. 16, 2022**
+
+* Towhee now supports generating an Nvidia Triton Server from a Towhee pipeline, with aditional support for GPU image decoding.
+* Added one audio fingerprinting model: 
+[*nnfp*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/nnfp)
+* Added two image embedding models: 
+[*RepMLP*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp), [**WaveViT**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/wave_vit)
+
+**v0.7.3 Jul. 27, 2022**
+* Added one multimodal (text/image) model:
+[*CoCa*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coca).
+* Added two video models for grounded situation recognition & repetitive action counting:
+[*CoFormer*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coformer),
+[*TransRAC*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac).
+* Added two SoTA models for image tasks (image retrieval, image classification, etc.):
+[*CVNet*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/cvnet),
+[*MaxViT*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/max_vit)
+
+**v0.7.1 Jul. 1, 2022**
+* Added one image embedding model:
+[*MPViT*](https://towhee.io/image-embedding/mpvit).
+* Added two video retrieval models:
+[*BridgeFormer*](https://towhee.io/video-text-embedding/bridge-former),
+[*collaborative-experts*](https://towhee.io/video-text-embedding/collaborative-experts).
+* Added FAISS-based ANNSearch operators: *to_faiss*, *faiss_search*.
+
+**v0.7.0 Jun. 24, 2022**
+
+* Added six video understanding/classification models:
+[*Video Swin Transformer*](https://towhee.io/action-classification/video-swin-transformer), 
+[*TSM*](https://towhee.io/action-classification/tsm), 
+[*Uniformer*](https://towhee.io/action-classification/uniformer), 
+[*OMNIVORE*](https://towhee.io/action-classification/omnivore), 
+[*TimeSformer*](https://towhee.io/action-classification/timesformer), 
+[*MoViNets*](https://towhee.io/action-classification/movinet).
+* Added four video retrieval models:
+[*CLIP4Clip*](https://towhee.io/video-text-embedding/clip4clip), 
+[*DRL*](https://towhee.io/video-text-embedding/drl), 
+[*Frozen in Time*](https://towhee.io/video-text-embedding/frozen-in-time), 
+[*MDMMT*](https://towhee.io/video-text-embedding/mdmmt).
+
+**v0.6.1  May. 13, 2022**
+
+* Added three text-image retrieval models:
+[*CLIP*](https://towhee.io/image-text-embedding/clip),
+[*BLIP*](https://towhee.io/image-text-embedding/blip),
+[*LightningDOT*](https://towhee.io/image-text-embedding/lightningdot).
+* Added six video understanding/classification models from PyTorchVideo:
+[*I3D*](https://towhee.io/action-classification/pytorchvideo),
+[*C2D*](https://towhee.io/action-classification/pytorchvideo),
+[*Slow*](https://towhee.io/action-classification/pytorchvideo),
+[*SlowFast*](https://towhee.io/action-classification/pytorchvideo),
+[*X3D*](https://towhee.io/action-classification/pytorchvideo),
+[*MViT*](https://towhee.io/action-classification/pytorchvideo).
 
 ## Getting started
 
 Towhee requires Python 3.6+. You can install Towhee via `pip`:
 
 ```bash
 pip install towhee towhee.models
 ```
 
-### Pipeline
+If you run into any pip-related install problems, please try to upgrade pip with `pip install -U pip`.
 
-### Pre-defined Pipeline
+Let's try your first Towhee pipeline. Below is an example for how to create a CLIP-based cross modal retrieval pipeline.
 
-Towhee provides some pre-defined pipelines to help users quickly implement some functions. 
-Currently implemented are: 
-- [Sentence Embedding](https://towhee.io/tasks/detail/pipeline/sentence-similarity)
-- [Image Embedding](https://towhee.io/tasks/detail/pipeline/text-image-search)
-- [Video deduplication](https://towhee.io/tasks/detail/pipeline/video-copy-detection)
-- [Question Answer with Docs](https://towhee.io/tasks/detail/pipeline/retrieval-augmented-generation)
-
-All pipelines can be found on Towhee Hub. Here is an example of using the sentence_embedding pipeline: 
+The example needs towhee 1.0.0, which can be installed with `pip install towhee==1.0.0`, The latest usage [documentation](https://towhee.readthedocs.io/en/main/index.html).
 
 ```python
-from towhee import AutoPipes, AutoConfig
-# get the built-in sentence_similarity pipeline
-config = AutoConfig.load_config('sentence_embedding')
-config.model = 'paraphrase-albert-small-v2'
-config.device = 0
-sentence_embedding = AutoPipes.pipeline('sentence_embedding', config=config)
-
-# generate embedding for one sentence
-embedding = sentence_embedding('how are you?').get()
-# batch generate embeddings for multi-sentences
-embeddings = sentence_embedding.batch(['how are you?', 'how old are you?'])
-embeddings = [e.get() for e in embeddings]
-```
-### Custom pipelines 
-
-If you can't find the pipeline you want in towhee hub, you can also implement custom pipelines through the towhee Python API. In the following example, we will create a cross-modal retrieval pipeline based on CLIP.
-```python
 
+from glob import glob
 from towhee import ops, pipe, DataCollection
+
+
 # create image embeddings and build index
 p = (
     pipe.input('file_name')
     .map('file_name', 'img', ops.image_decode.cv2())
     .map('img', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='image'))
     .map('vec', 'vec', ops.towhee.np_normalize())
     .map(('vec', 'file_name'), (), ops.ann_insert.faiss_index('./faiss', 512))
@@ -128,17 +217,20 @@
 )
 
 for f_name in ['https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog1.png',
                'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog2.png',
                'https://raw.githubusercontent.com/towhee-io/towhee/main/assets/dog3.png']:
     p(f_name)
 
-# Flush faiss data into disk. 
-p.flush()
-# search image by textdecode = ops.image_decode.cv2('rgb')
+# Delete the pipeline object, make sure the faiss data is written to disk. 
+del p
+
+
+# search image by text
+decode = ops.image_decode.cv2('rgb')
 p = (
     pipe.input('text')
     .map('text', 'vec', ops.image_text_embedding.clip(model_name='clip_vit_base_patch32', modality='text'))
     .map('vec', 'vec', ops.towhee.np_normalize())
     # faiss op result format:  [[id, score, [file_name], ...]
     .map('vec', 'row', ops.ann_search.faiss_index('./faiss', 3))
     .map('row', 'images', lambda x: [decode(item[2][0]) for item in x])
@@ -146,32 +238,28 @@
 )
 
 DataCollection(p('a cat')).show()
 
 ```
 <img src="assets/towhee_example.png" style="width: 60%; height: 60%">
 
+Learn more examples from the [Towhee Examples](https://github.com/towhee-io/examples).
 
 ## Core Concepts
 
 Towhee is composed of four main building blocks - `Operators`, `Pipelines`, `DataCollection API` and `Engine`.
 
 - __Operators__: An operator is a single building block of a neural data processing pipeline. Different implementations of operators are categorized by tasks, with each task having a standard interface. An operator can be a deep learning model, a data processing method, or a Python function.
 
 - __Pipelines__: A pipeline is composed of several operators interconnected in the form of a DAG (directed acyclic graph). This DAG can direct complex functionalities, such as embedding feature extraction, data tagging, and cross modal data analysis.
 
-- __DataCollection API__: A Pythonic and method-chaining style API for building custom pipelines, providing multiple data conversion interfaces: map, filter, flat_map, concat, window, time_window, and window_all. Through these interfaces, complex data processing pipelines can be built quickly to process unstructured data such as video, audio, text, images, etc.
+- __DataCollection API__: A Pythonic and method-chaining style API for building custom pipelines. A pipeline defined by the DataColltion API can be run locally on a laptop for fast prototyping and then be converted to a docker image, with end-to-end optimizations, for production-ready environments.
 
 - __Engine__: The engine sits at Towhee's core. Given a pipeline, the engine will drive dataflow among individual operators, schedule tasks, and monitor compute resource usage (CPU/GPU/etc). We provide a basic engine within Towhee to run pipelines on a single-instance machine and a Triton-based engine for docker containers.
 
-## Resource
-- TowheeHub: https://towhee.io/
-- docs: https://towhee.readthedocs.io/en/latest/
-- examples: https://github.com/towhee-io/examples
-
 ## Contributing
 
 Writing code is not the only way to contribute! Submitting issues, answering questions, and improving documentation are just some of the many ways you can help our growing community. Check out our [contributing page](https://github.com/towhee-io/towhee/blob/main/CONTRIBUTING.md) for more information.
 
 Special thanks goes to these folks for contributing to Towhee, either on Github, our Towhee Hub, or elsewhere:
 <br><!-- Do not remove start of hero-bot --><br>
 <img src="https://img.shields.io/badge/all--contributors-33-orange"><br>
```

### html2text {}

```diff
@@ -1,119 +1,180 @@
-Metadata-Version: 2.1 Name: towhee.models Version: 1.1.0 Summary: Towhee is a
+Metadata-Version: 2.1 Name: towhee.models Version: 1.1.1 Summary: Towhee is a
 framework that helps you encode your unstructured data into embeddings. Home-
 page: https://github.com/towhee-io/towhee Author: Towhee Team Author-email:
 towhee-team@zilliz.com License: http://www.apache.org/licenses/LICENSE-2.0
 Platform: unix Platform: linux Platform: osx Platform: win32 Description-
 Content-Type: text/markdown License-File: LICENSE Requires-Dist: requests
 (>=2.12.5) Requires-Dist: tqdm (>=4.59.0) Requires-Dist: tabulate Requires-
 Dist: numpy Requires-Dist: twine Requires-Dist: tenacity Requires-Dist:
-pydantic Requires-Dist: contextvars ; python_version <= "3.6" Requires-Dist:
-importlib-resources ; python_version<'3.7'  
+pydantic (<2) Requires-Dist: contextvars ; python_version <= "3.6" Requires-
+Dist: importlib-resources ; python_version<'3.7'  
 [towhee_logo.png#gh-light-mode-only] [assets/towhee_logo_dark.png#gh-dark-mode-
                                      only]
                    **** x2vec, Towhee is all you need! ****
                        **** ENGLISH | ä¸­æææ¡£ ****
-[join-slack] [twitter] [license] [github_actions] [github_actions] [coverage]
-  [Towhee](https://towhee.io) is a cutting-edge framework designed to
-streamline the processing of unstructured data through the use of Large
-Language Model (LLM) based pipeline orchestration. It is uniquely positioned to
-extract invaluable insights from diverse unstructured data types, including
-lengthy text, images, audio and video files. Leveraging the capabilities of
-generative AI and the SOTA deep learning models, Towhee is capable of
-transforming this unprocessed data into specific formats such as text, image,
-or embeddings. These can then be efficiently loaded into an appropriate storage
-system like a vector database. Developers can initially build an intuitive data
-processing pipeline prototype with user friendly Pythonic APU, then optimize it
-for production environments. ð¨âMulti Modalities: Towhee is capable of
-handling a wide range of data types. Whether it's image data, video clips,
-text, audio files, or even molecular structures, Towhee can process them all.
-ð LLM Pipeline orchestration: Towhee offers flexibility to adapt to
-different Large Language Models (LLMs). Additionally, it allows for hosting
-open-source large models locally. Moreover, Towhee provides features like
-prompt management and knowledge retrieval, making the interaction with these
-LLMs more efficient and effective. ðâRich Operators: Towhee provides a
-wide range of ready-to-use state-of-the-art models across five domains: CV,
-NLP, multimodal, audio, and medical. With over 140 models like BERT and CLIP
-and rich functionalities like video decoding, audio slicing, frame sampling,
-and dimensionality reduction, it assists in efficiently building data
-processing pipelines. ð Prebuilt ETL Pipelines: Towhee offers ready-to-use
-ETL (Extract, Transform, Load) pipelines for common tasks such as Retrieval-
-Augmented Generation, Text Image search, and Video copy detection. This means
-you don't need to be an AI expert to build applications using these features.
-â¡ï¸ High performance backend: Leveraging the power of the Triton Inference
-Server, Towhee can speed up model serving on both CPU and GPU using platforms
-like TensorRT, Pytorch, and ONNX. Moreover, you can transform your Python
-pipeline into a high-performance docker container with just a few lines of
-code, enabling efficient deployment and scaling. ðâPythonic API: Towhee
-includes a Pythonic method-chaining API for describing custom data processing
-pipelines. We also support schemas, which makes processing unstructured data as
-easy as handling tabular data. ## Getting started Towhee requires Python 3.6+.
-You can install Towhee via `pip`: ```bash pip install towhee towhee.models ```
-### Pipeline ### Pre-defined Pipeline Towhee provides some pre-defined
-pipelines to help users quickly implement some functions. Currently implemented
-are: - [Sentence Embedding](https://towhee.io/tasks/detail/pipeline/sentence-
-similarity) - [Image Embedding](https://towhee.io/tasks/detail/pipeline/text-
-image-search) - [Video deduplication](https://towhee.io/tasks/detail/pipeline/
-video-copy-detection) - [Question Answer with Docs](https://towhee.io/tasks/
-detail/pipeline/retrieval-augmented-generation) All pipelines can be found on
-Towhee Hub. Here is an example of using the sentence_embedding pipeline:
-```python from towhee import AutoPipes, AutoConfig # get the built-in
-sentence_similarity pipeline config = AutoConfig.load_config
-('sentence_embedding') config.model = 'paraphrase-albert-small-v2'
-config.device = 0 sentence_embedding = AutoPipes.pipeline('sentence_embedding',
-config=config) # generate embedding for one sentence embedding =
-sentence_embedding('how are you?').get() # batch generate embeddings for multi-
-sentences embeddings = sentence_embedding.batch(['how are you?', 'how old are
-you?']) embeddings = [e.get() for e in embeddings] ``` ### Custom pipelines If
-you can't find the pipeline you want in towhee hub, you can also implement
-custom pipelines through the towhee Python API. In the following example, we
-will create a cross-modal retrieval pipeline based on CLIP. ```python from
+[join-slack] [twitter] [license] [github_actions] [coverage]
+  [Towhee](https://towhee.io) makes it easy to build neural data processing
+pipelines for AI applications. We provide hundreds of models, algorithms, and
+transformations that can be used as standard pipeline building blocks. You can
+use Towhee's Pythonic API to build a prototype of your pipeline and
+automatically optimize it for production-ready environments. :art:
+&emsp;**Various Modalities:** Towhee supports data processing on a variety of
+modalities, including images, videos, text, audio, molecular structures, etc. :
+mortar_board:&emsp;**SOTA Models:** Towhee provides SOTA models across 5 fields
+(CV, NLP, Multimodal, Audio, Medical), 15 tasks, and 140+ model architectures.
+These include BERT, CLIP, ViT, SwinTransformer, MAE, and data2vec, all
+pretrained and ready to use. :package:&emsp;**Data Processing:** Towhee also
+provides traditional methods alongside neural network models to help you build
+practical data processing pipelines. We have a rich pool of operators
+available, such as video decoding, audio slicing, frame sampling, feature
+vector dimension reduction, ensembling, and database operations. :snake:
+&emsp;**Pythonic API:** Towhee includes a Pythonic method-chaining API for
+describing custom data processing pipelines. We also support schemas, which
+makes processing unstructured data as easy as handling tabular data. ## What's
+New **v1.0.0rc1 May. 4, 2023** * Add trainer to operators: [*timm*](https://
+towhee.io/image-embedding/timm), [*isc*](https://towhee.io/image-embedding/
+isc), [*transformers*](https://towhee.io/text-embedding/transformers), [*clip*]
+(https://towhee.io/image-text-embedding/clip) * Add GPU video decoder: [*VPF*]
+(https://towhee.io/video-decode/VPF) * All towhee pipelines can be converted
+into Nvidia Triton services. **v0.9.0 Dec. 2, 2022** * Added one video
+classification model: [*Vis4mer*](https://github.com/towhee-io/towhee/tree/
+branch0.9.0/towhee/models/vis4mer) * Added three visual backbones: [*MCProp*]
+(https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/mcprop),
+[*RepLKNet*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/
+models/replknet), [*Shunted Transformer*](https://github.com/towhee-io/towhee/
+tree/branch0.9.0/towhee/models/shunted_transformer) * Add two code search
+operators: [*code_search.codebert*](https://towhee.io/code-search/codebert),
+[*code_search.unixcoder*](https://towhee.io/code-search/unixcoder) * Add five
+image captioning operators: [*image_captioning.expansionnet-v2*](https://
+towhee.io/image-captioning/expansionnet-v2), [*image_captioning.magic*](https:/
+/towhee.io/image-captioning/magic), [*image_captioning.clip_caption_reward*]
+(https://towhee.io/image-captioning/clip-caption-reward),
+[*image_captioning.blip*](https://towhee.io/image-captioning/blip),
+[*image_captioning.clipcap*](https://towhee.io/image-captioning/clipcap) * Add
+five image-text embedding operators: [*image_text_embedding.albef*](https://
+towhee.io/image-text-embedding/albef), [*image_text_embedding.ru_clip*](https:/
+/towhee.io/image-text-embedding/ru-clip),
+[*image_text_embedding.japanese_clip*](https://towhee.io/image-text-embedding/
+japanese-clip), [*image_text_embedding.taiyi*](https://towhee.io/image-text-
+embedding/taiyi), [*image_text_embedding.slip*](https://towhee.io/image-text-
+embedding/slip) * Add one machine-translation operator:
+[*machine_translation.opus_mt*](https://towhee.io/machine-translation/opus-mt)
+* Add one filter-tiny-segments operator: [*video-copy-detection.filter-tiny-
+segments*](https://towhee.io/video-copy-detection/filter-tiny-segments) * Add
+an advanced tutorial for audio fingerprinting: [*Audio Fingerprint II: Music
+Detection with Temporal Localization*](https://github.com/towhee-io/examples/
+blob/main/audio/audio_fingerprint/audio_fingerprint_advanced.ipynb) (increased
+accuracy from 84% to 90%) **v0.8.1 Sep. 30, 2022** * Added four visual
+backbones: [*ISC*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/
+models/isc), [*MetaFormer*](https://github.com/towhee-io/towhee/tree/
+branch0.8.1/towhee/models/metaformer), [*ConvNext*](https://github.com/towhee-
+io/towhee/tree/branch0.8.1/towhee/models/convnext), [*HorNet*](https://
+github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/hornet) * Add two
+video de-copy operators: [*select-video*](https://towhee.io/video-copy-
+detection/select-video), [*temporal-network*](https://towhee.io/video-copy-
+detection/temporal-network) * Add one image embedding operator specifically
+designed for image retrieval and video de-copy with SOTA performance on VCSL
+dataset: [*isc*](https://towhee.io/image-embedding/isc) * Add one audio
+embedding operator specified for audio fingerprint: [*audio_embedding.nnfp*]
+(https://towhee.io/audio-embedding/nnfp) (with pretrained weights) * Add one
+tutorial for video de-copy: [*How to Build a Video Segment Copy Detection
+System*](https://github.com/towhee-io/examples/blob/main/video/
+video_deduplication/segment_level/video_deduplication_at_segment_level.ipynb) *
+Add one beginner tutorial for audio fingerprint: [*Audio Fingerprint I: Build a
+Demo with Towhee & Milvus*](https://github.com/towhee-io/examples/blob/main/
+audio/audio_fingerprint/audio_fingerprint_beginner.ipynb) **v0.8.0 Aug. 16,
+2022** * Towhee now supports generating an Nvidia Triton Server from a Towhee
+pipeline, with aditional support for GPU image decoding. * Added one audio
+fingerprinting model: [*nnfp*](https://github.com/towhee-io/towhee/tree/
+branch0.8.0/towhee/models/nnfp) * Added two image embedding models: [*RepMLP*]
+(https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp),
+[**WaveViT**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/
+models/wave_vit) **v0.7.3 Jul. 27, 2022** * Added one multimodal (text/image)
+model: [*CoCa*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/
+models/coca). * Added two video models for grounded situation recognition &
+repetitive action counting: [*CoFormer*](https://github.com/towhee-io/towhee/
+tree/branch0.7.3/towhee/models/coformer), [*TransRAC*](https://github.com/
+towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac). * Added two SoTA
+models for image tasks (image retrieval, image classification, etc.): [*CVNet*]
+(https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/cvnet),
+[*MaxViT*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/
+max_vit) **v0.7.1 Jul. 1, 2022** * Added one image embedding model: [*MPViT*]
+(https://towhee.io/image-embedding/mpvit). * Added two video retrieval models:
+[*BridgeFormer*](https://towhee.io/video-text-embedding/bridge-former),
+[*collaborative-experts*](https://towhee.io/video-text-embedding/collaborative-
+experts). * Added FAISS-based ANNSearch operators: *to_faiss*, *faiss_search*.
+**v0.7.0 Jun. 24, 2022** * Added six video understanding/classification models:
+[*Video Swin Transformer*](https://towhee.io/action-classification/video-swin-
+transformer), [*TSM*](https://towhee.io/action-classification/tsm),
+[*Uniformer*](https://towhee.io/action-classification/uniformer), [*OMNIVORE*]
+(https://towhee.io/action-classification/omnivore), [*TimeSformer*](https://
+towhee.io/action-classification/timesformer), [*MoViNets*](https://towhee.io/
+action-classification/movinet). * Added four video retrieval models:
+[*CLIP4Clip*](https://towhee.io/video-text-embedding/clip4clip), [*DRL*](https:
+//towhee.io/video-text-embedding/drl), [*Frozen in Time*](https://towhee.io/
+video-text-embedding/frozen-in-time), [*MDMMT*](https://towhee.io/video-text-
+embedding/mdmmt). **v0.6.1 May. 13, 2022** * Added three text-image retrieval
+models: [*CLIP*](https://towhee.io/image-text-embedding/clip), [*BLIP*](https:/
+/towhee.io/image-text-embedding/blip), [*LightningDOT*](https://towhee.io/
+image-text-embedding/lightningdot). * Added six video understanding/
+classification models from PyTorchVideo: [*I3D*](https://towhee.io/action-
+classification/pytorchvideo), [*C2D*](https://towhee.io/action-classification/
+pytorchvideo), [*Slow*](https://towhee.io/action-classification/pytorchvideo),
+[*SlowFast*](https://towhee.io/action-classification/pytorchvideo), [*X3D*]
+(https://towhee.io/action-classification/pytorchvideo), [*MViT*](https://
+towhee.io/action-classification/pytorchvideo). ## Getting started Towhee
+requires Python 3.6+. You can install Towhee via `pip`: ```bash pip install
+towhee towhee.models ``` If you run into any pip-related install problems,
+please try to upgrade pip with `pip install -U pip`. Let's try your first
+Towhee pipeline. Below is an example for how to create a CLIP-based cross modal
+retrieval pipeline. The example needs towhee 1.0.0, which can be installed with
+`pip install towhee==1.0.0`, The latest usage [documentation](https://
+towhee.readthedocs.io/en/main/index.html). ```python from glob import glob from
 towhee import ops, pipe, DataCollection # create image embeddings and build
 index p = ( pipe.input('file_name') .map('file_name', 'img',
 ops.image_decode.cv2()) .map('img', 'vec', ops.image_text_embedding.clip
 (model_name='clip_vit_base_patch32', modality='image')) .map('vec', 'vec',
 ops.towhee.np_normalize()) .map(('vec', 'file_name'), (),
 ops.ann_insert.faiss_index('./faiss', 512)) .output() ) for f_name in ['https:/
 /raw.githubusercontent.com/towhee-io/towhee/main/assets/dog1.png', 'https://
 raw.githubusercontent.com/towhee-io/towhee/main/assets/dog2.png', 'https://
 raw.githubusercontent.com/towhee-io/towhee/main/assets/dog3.png']: p(f_name) #
-Flush faiss data into disk. p.flush() # search image by textdecode =
-ops.image_decode.cv2('rgb') p = ( pipe.input('text') .map('text', 'vec',
-ops.image_text_embedding.clip(model_name='clip_vit_base_patch32',
-modality='text')) .map('vec', 'vec', ops.towhee.np_normalize()) # faiss op
-result format: [[id, score, [file_name], ...] .map('vec', 'row',
-ops.ann_search.faiss_index('./faiss', 3)) .map('row', 'images', lambda x:
-[decode(item[2][0]) for item in x]) .output('text', 'images') ) DataCollection
-(p('a cat')).show() ``` [assets/towhee_example.png] ## Core Concepts Towhee is
-composed of four main building blocks - `Operators`, `Pipelines`,
-`DataCollection API` and `Engine`. - __Operators__: An operator is a single
-building block of a neural data processing pipeline. Different implementations
-of operators are categorized by tasks, with each task having a standard
-interface. An operator can be a deep learning model, a data processing method,
-or a Python function. - __Pipelines__: A pipeline is composed of several
-operators interconnected in the form of a DAG (directed acyclic graph). This
-DAG can direct complex functionalities, such as embedding feature extraction,
-data tagging, and cross modal data analysis. - __DataCollection API__: A
-Pythonic and method-chaining style API for building custom pipelines, providing
-multiple data conversion interfaces: map, filter, flat_map, concat, window,
-time_window, and window_all. Through these interfaces, complex data processing
-pipelines can be built quickly to process unstructured data such as video,
-audio, text, images, etc. - __Engine__: The engine sits at Towhee's core. Given
-a pipeline, the engine will drive dataflow among individual operators, schedule
-tasks, and monitor compute resource usage (CPU/GPU/etc). We provide a basic
-engine within Towhee to run pipelines on a single-instance machine and a
-Triton-based engine for docker containers. ## Resource - TowheeHub: https://
-towhee.io/ - docs: https://towhee.readthedocs.io/en/latest/ - examples: https:/
-/github.com/towhee-io/examples ## Contributing Writing code is not the only way
-to contribute! Submitting issues, answering questions, and improving
-documentation are just some of the many ways you can help our growing
-community. Check out our [contributing page](https://github.com/towhee-io/
-towhee/blob/main/CONTRIBUTING.md) for more information. Special thanks goes to
-these folks for contributing to Towhee, either on Github, our Towhee Hub, or
-elsewhere:
+Delete the pipeline object, make sure the faiss data is written to disk. del p
+# search image by text decode = ops.image_decode.cv2('rgb') p = ( pipe.input
+('text') .map('text', 'vec', ops.image_text_embedding.clip
+(model_name='clip_vit_base_patch32', modality='text')) .map('vec', 'vec',
+ops.towhee.np_normalize()) # faiss op result format: [[id, score, [file_name],
+...] .map('vec', 'row', ops.ann_search.faiss_index('./faiss', 3)) .map('row',
+'images', lambda x: [decode(item[2][0]) for item in x]) .output('text',
+'images') ) DataCollection(p('a cat')).show() ``` [assets/towhee_example.png]
+Learn more examples from the [Towhee Examples](https://github.com/towhee-io/
+examples). ## Core Concepts Towhee is composed of four main building blocks -
+`Operators`, `Pipelines`, `DataCollection API` and `Engine`. - __Operators__:
+An operator is a single building block of a neural data processing pipeline.
+Different implementations of operators are categorized by tasks, with each task
+having a standard interface. An operator can be a deep learning model, a data
+processing method, or a Python function. - __Pipelines__: A pipeline is
+composed of several operators interconnected in the form of a DAG (directed
+acyclic graph). This DAG can direct complex functionalities, such as embedding
+feature extraction, data tagging, and cross modal data analysis. -
+__DataCollection API__: A Pythonic and method-chaining style API for building
+custom pipelines. A pipeline defined by the DataColltion API can be run locally
+on a laptop for fast prototyping and then be converted to a docker image, with
+end-to-end optimizations, for production-ready environments. - __Engine__: The
+engine sits at Towhee's core. Given a pipeline, the engine will drive dataflow
+among individual operators, schedule tasks, and monitor compute resource usage
+(CPU/GPU/etc). We provide a basic engine within Towhee to run pipelines on a
+single-instance machine and a Triton-based engine for docker containers. ##
+Contributing Writing code is not the only way to contribute! Submitting issues,
+answering questions, and improving documentation are just some of the many ways
+you can help our growing community. Check out our [contributing page](https://
+github.com/towhee-io/towhee/blob/main/CONTRIBUTING.md) for more information.
+Special thanks goes to these folks for contributing to Towhee, either on
+Github, our Towhee Hub, or elsewhere:
 
 [https://img.shields.io/badge/all--contributors-33-orange]
 [https://avatars.githubusercontent.com/u/34787227?v=4] [https://
 avatars.githubusercontent.com/u/72550076?v=4] [https://
 avatars.githubusercontent.com/u/57477222?v=4] [https://
 avatars.githubusercontent.com/u/109071306?v=4] [https://
 avatars.githubusercontent.com/u/21202514?v=4] [https://
```

## Comparing `towhee.models-1.1.0.dist-info/RECORD` & `towhee.models-1.1.1.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -268,13 +268,13 @@
 towhee/models/vit/vit.py,sha256=JCpAKp-e2U3ss45Ji-mJ9qRaAJpN7YBRdAHD98mTkaY,10295
 towhee/models/vit/vit_block.py,sha256=zprOJCOX2z273iMenk-ex9-Rr53SIvzQbCHCrJCqsm4,3878
 towhee/models/vit/vit_utils.py,sha256=y5PwJKR57TIwiO3ZssRO2r0esJLCiguYekhVkLUfZSI,1868
 towhee/models/wave_vit/__init__.py,sha256=oyVqphD9VR93Kj--l4TvVS-k-d3fqGuvEJcMJzohHaQ,617
 towhee/models/wave_vit/wave_vit.py,sha256=KqsLjc2DOQpjaIHvD909u6YNXSb7gGpv_VjbAYWC2aw,10014
 towhee/models/wave_vit/wave_vit_block.py,sha256=15DAbuYWb8SKr2crC4uS18W6bXmD_yClrPshaPYpriQ,9731
 towhee/models/wave_vit/wave_vit_utils.py,sha256=8nYx23xalB26swYnut2WlzWCj45md-1-hVHFoAydHPE,6361
-towhee.models-1.1.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-towhee.models-1.1.0.dist-info/METADATA,sha256=OWqBV3QZhOAJhjYIm64DQSHWc9ezweQuAuwGlz5Akms,13974
-towhee.models-1.1.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-towhee.models-1.1.0.dist-info/entry_points.txt,sha256=tGMn2QCTr-tOrFxMYYTyubVJ-ZOXIP-qf6P5GXOJWxI,55
-towhee.models-1.1.0.dist-info/top_level.txt,sha256=s8O0-CAA8lmENHKY-FR5ojkSAmqEOEkrpg6ITjplm4A,7
-towhee.models-1.1.0.dist-info/RECORD,,
+towhee.models-1.1.1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+towhee.models-1.1.1.dist-info/METADATA,sha256=7s746FW5sPpQv-VtZWIipckAd_lWFTiORDglF5DO7EY,18516
+towhee.models-1.1.1.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+towhee.models-1.1.1.dist-info/entry_points.txt,sha256=tGMn2QCTr-tOrFxMYYTyubVJ-ZOXIP-qf6P5GXOJWxI,55
+towhee.models-1.1.1.dist-info/top_level.txt,sha256=s8O0-CAA8lmENHKY-FR5ojkSAmqEOEkrpg6ITjplm4A,7
+towhee.models-1.1.1.dist-info/RECORD,,
```

