# Comparing `tmp/llama_index-0.6.9.tar.gz` & `tmp/llama_index-0.7.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "llama_index-0.6.9.tar", last modified: Fri May 19 20:30:37 2023, max compression
+gzip compressed data, was "llama_index-0.7.0.tar", last modified: Tue Jul  4 15:28:38 2023, max compression
```

## Comparing `llama_index-0.6.9.tar` & `llama_index-0.7.0.tar`

### file list

```diff
@@ -1,496 +1,644 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.940061 llama_index-0.6.9/
--rw-r--r--   0 runner    (1001) docker     (123)     1064 2023-05-19 20:30:23.000000 llama_index-0.6.9/LICENSE
--rw-r--r--   0 runner    (1001) docker     (123)       72 2023-05-19 20:30:23.000000 llama_index-0.6.9/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (123)     4255 2023-05-19 20:30:37.940061 llama_index-0.6.9/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)     4026 2023-05-19 20:30:23.000000 llama_index-0.6.9/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.892060 llama_index-0.6.9/llama_index/
--rw-r--r--   0 runner    (1001) docker     (123)        6 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/VERSION
--rw-r--r--   0 runner    (1001) docker     (123)     4719 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      322 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/async_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.896060 llama_index-0.6.9/llama_index/callbacks/
--rw-r--r--   0 runner    (1001) docker     (123)      263 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/callbacks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6096 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/callbacks/aim.py
--rw-r--r--   0 runner    (1001) docker     (123)     3066 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/callbacks/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5526 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/callbacks/llama_debug.py
--rw-r--r--   0 runner    (1001) docker     (123)     1668 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/callbacks/schema.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.896060 llama_index-0.6.9/llama_index/composability/
--rw-r--r--   0 runner    (1001) docker     (123)      232 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/composability/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      171 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/composability/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     3424 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/composability/joint_qa_summary.py
--rw-r--r--   0 runner    (1001) docker     (123)      313 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.896060 llama_index-0.6.9/llama_index/data_structs/
--rw-r--r--   0 runner    (1001) docker     (123)      400 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/data_structs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9458 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/data_structs/data_structs.py
--rw-r--r--   0 runner    (1001) docker     (123)     2292 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/data_structs/document_summary.py
--rw-r--r--   0 runner    (1001) docker     (123)     6221 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/data_structs/node.py
--rw-r--r--   0 runner    (1001) docker     (123)      904 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/data_structs/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     3638 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/data_structs/struct_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     1025 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/data_structs/table.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.896060 llama_index-0.6.9/llama_index/embeddings/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/embeddings/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8385 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/embeddings/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     1164 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/embeddings/google.py
--rw-r--r--   0 runner    (1001) docker     (123)     1109 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/embeddings/langchain.py
--rw-r--r--   0 runner    (1001) docker     (123)     9825 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/embeddings/openai.py
--rw-r--r--   0 runner    (1001) docker     (123)      559 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/embeddings/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.896060 llama_index-0.6.9/llama_index/evaluation/
--rw-r--r--   0 runner    (1001) docker     (123)      259 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/evaluation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12080 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/evaluation/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5341 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/evaluation/dataset_generation.py
--rw-r--r--   0 runner    (1001) docker     (123)      544 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/img_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.896060 llama_index-0.6.9/llama_index/indices/
--rw-r--r--   0 runner    (1001) docker     (123)      542 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9392 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/base.py
--rw-r--r--   0 runner    (1001) docker     (123)      856 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/base_retriever.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.896060 llama_index-0.6.9/llama_index/indices/common/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/common/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.900060 llama_index-0.6.9/llama_index/indices/common/struct_store/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/common/struct_store/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8454 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/common/struct_store/base.py
--rw-r--r--   0 runner    (1001) docker     (123)      776 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/common/struct_store/schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     2676 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/common/struct_store/sql.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.900060 llama_index-0.6.9/llama_index/indices/common_tree/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/common_tree/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7971 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/common_tree/base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.900060 llama_index-0.6.9/llama_index/indices/composability/
--rw-r--r--   0 runner    (1001) docker     (123)      180 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/composability/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3950 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/composability/graph.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.900060 llama_index-0.6.9/llama_index/indices/document_summary/
--rw-r--r--   0 runner    (1001) docker     (123)      382 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/document_summary/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6018 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/document_summary/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     6871 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/document_summary/retrievers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.900060 llama_index-0.6.9/llama_index/indices/empty/
--rw-r--r--   0 runner    (1001) docker     (123)      198 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/empty/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2145 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/empty/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     1173 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/empty/retrievers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.900060 llama_index-0.6.9/llama_index/indices/keyword_table/
--rw-r--r--   0 runner    (1001) docker     (123)      656 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/keyword_table/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7676 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/keyword_table/base.py
--rw-r--r--   0 runner    (1001) docker     (123)      650 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/keyword_table/rake_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5903 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/keyword_table/retrievers.py
--rw-r--r--   0 runner    (1001) docker     (123)      844 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/keyword_table/simple_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2264 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/keyword_table/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.900060 llama_index-0.6.9/llama_index/indices/knowledge_graph/
--rw-r--r--   0 runner    (1001) docker     (123)      254 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/knowledge_graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7796 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/knowledge_graph/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     9540 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/knowledge_graph/retrievers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.900060 llama_index-0.6.9/llama_index/indices/list/
--rw-r--r--   0 runner    (1001) docker     (123)      295 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/list/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3461 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/list/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     6803 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/list/retrievers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3608 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/loading.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.904060 llama_index-0.6.9/llama_index/indices/postprocessor/
--rw-r--r--   0 runner    (1001) docker     (123)      972 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/postprocessor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1749 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/postprocessor/cohere_rerank.py
--rw-r--r--   0 runner    (1001) docker     (123)     3115 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/postprocessor/llm_rerank.py
--rw-r--r--   0 runner    (1001) docker     (123)    11698 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/postprocessor/node.py
--rw-r--r--   0 runner    (1001) docker     (123)     8923 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/postprocessor/node_recency.py
--rw-r--r--   0 runner    (1001) docker     (123)     5026 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/postprocessor/pii.py
--rw-r--r--   0 runner    (1001) docker     (123)      445 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/postprocessor/types.py
--rw-r--r--   0 runner    (1001) docker     (123)     8489 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/prompt_helper.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.904060 llama_index-0.6.9/llama_index/indices/query/
--rw-r--r--   0 runner    (1001) docker     (123)      137 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/query/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1925 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/query/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     3386 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/query/embedding_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.904060 llama_index-0.6.9/llama_index/indices/query/query_transform/
--rw-r--r--   0 runner    (1001) docker     (123)      281 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/query/query_transform/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8904 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/query/query_transform/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5920 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/query/query_transform/prompts.py
--rw-r--r--   0 runner    (1001) docker     (123)     7995 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/query/response_synthesis.py
--rw-r--r--   0 runner    (1001) docker     (123)     1248 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/query/schema.py
--rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.904060 llama_index-0.6.9/llama_index/indices/response/
--rw-r--r--   0 runner    (1001) docker     (123)      831 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3783 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/accumulate.py
--rw-r--r--   0 runner    (1001) docker     (123)     2204 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/base_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2258 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/compact_and_refine.py
--rw-r--r--   0 runner    (1001) docker     (123)     2996 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     2442 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/generation.py
--rw-r--r--   0 runner    (1001) docker     (123)     6136 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/refine.py
--rw-r--r--   0 runner    (1001) docker     (123)      848 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/response_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3137 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/simple_summarize.py
--rw-r--r--   0 runner    (1001) docker     (123)     5537 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/tree_summarize.py
--rw-r--r--   0 runner    (1001) docker     (123)     1640 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/response/type.py
--rw-r--r--   0 runner    (1001) docker     (123)     4083 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/service_context.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.904060 llama_index-0.6.9/llama_index/indices/struct_store/
--rw-r--r--   0 runner    (1001) docker     (123)      622 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/struct_store/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2109 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/struct_store/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     5765 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/struct_store/container_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2208 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/struct_store/pandas.py
--rw-r--r--   0 runner    (1001) docker     (123)     4741 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/struct_store/pandas_query.py
--rw-r--r--   0 runner    (1001) docker     (123)     6142 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/struct_store/sql.py
--rw-r--r--   0 runner    (1001) docker     (123)     5950 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/struct_store/sql_query.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.904060 llama_index-0.6.9/llama_index/indices/tree/
--rw-r--r--   0 runner    (1001) docker     (123)      616 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/tree/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1606 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/tree/all_leaf_retriever.py
--rw-r--r--   0 runner    (1001) docker     (123)     5853 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/tree/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     7188 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/tree/inserter.py
--rw-r--r--   0 runner    (1001) docker     (123)     4660 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/tree/select_leaf_embedding_retriever.py
--rw-r--r--   0 runner    (1001) docker     (123)    15292 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/tree/select_leaf_retriever.py
--rw-r--r--   0 runner    (1001) docker     (123)     1399 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/tree/tree_root_retriever.py
--rw-r--r--   0 runner    (1001) docker     (123)     3523 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.908060 llama_index-0.6.9/llama_index/indices/vector_store/
--rw-r--r--   0 runner    (1001) docker     (123)      331 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/vector_store/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8450 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/vector_store/base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.908060 llama_index-0.6.9/llama_index/indices/vector_store/retrievers/
--rw-r--r--   0 runner    (1001) docker     (123)      267 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/vector_store/retrievers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.908060 llama_index-0.6.9/llama_index/indices/vector_store/retrievers/auto_retriever/
--rw-r--r--   0 runner    (1001) docker     (123)      167 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/vector_store/retrievers/auto_retriever/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4476 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/vector_store/retrievers/auto_retriever/auto_retriever.py
--rw-r--r--   0 runner    (1001) docker     (123)      649 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/vector_store/retrievers/auto_retriever/output_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2875 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/vector_store/retrievers/auto_retriever/prompts.py
--rw-r--r--   0 runner    (1001) docker     (123)     4719 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/indices/vector_store/retrievers/retriever.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.908060 llama_index-0.6.9/llama_index/langchain_helpers/
--rw-r--r--   0 runner    (1001) docker     (123)       39 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.908060 llama_index-0.6.9/llama_index/langchain_helpers/agents/
--rw-r--r--   0 runner    (1001) docker     (123)      514 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/agents/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2994 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/agents/agents.py
--rw-r--r--   0 runner    (1001) docker     (123)      837 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/agents/toolkits.py
--rw-r--r--   0 runner    (1001) docker     (123)     2211 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/agents/tools.py
--rw-r--r--   0 runner    (1001) docker     (123)      252 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/chain_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     7675 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/memory_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     3287 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/sql_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1224 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/streaming.py
--rw-r--r--   0 runner    (1001) docker     (123)    19719 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/langchain_helpers/text_splitter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.908060 llama_index-0.6.9/llama_index/llm_predictor/
--rw-r--r--   0 runner    (1001) docker     (123)      359 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/llm_predictor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12281 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/llm_predictor/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     4282 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/llm_predictor/chatgpt.py
--rw-r--r--   0 runner    (1001) docker     (123)     8731 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/llm_predictor/huggingface.py
--rw-r--r--   0 runner    (1001) docker     (123)     2274 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/llm_predictor/structured.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.908060 llama_index-0.6.9/llama_index/logger/
--rw-r--r--   0 runner    (1001) docker     (123)       95 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/logger/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      995 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/logger/base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.908060 llama_index-0.6.9/llama_index/node_parser/
--rw-r--r--   0 runner    (1001) docker     (123)      184 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/node_parser/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      527 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/node_parser/interface.py
--rw-r--r--   0 runner    (1001) docker     (123)     3582 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/node_parser/node_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     2396 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/node_parser/simple.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.908060 llama_index-0.6.9/llama_index/optimization/
--rw-r--r--   0 runner    (1001) docker     (123)      144 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/optimization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4301 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/optimization/optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.912060 llama_index-0.6.9/llama_index/output_parsers/
--rw-r--r--   0 runner    (1001) docker     (123)      230 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/output_parsers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      662 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/output_parsers/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2742 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/output_parsers/guardrails.py
--rw-r--r--   0 runner    (1001) docker     (123)     1828 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/output_parsers/langchain.py
--rw-r--r--   0 runner    (1001) docker     (123)     1963 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/output_parsers/selection.py
--rw-r--r--   0 runner    (1001) docker     (123)      608 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/output_parsers/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.912060 llama_index-0.6.9/llama_index/playground/
--rw-r--r--   0 runner    (1001) docker     (123)      202 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/playground/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6471 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/playground/base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.912060 llama_index-0.6.9/llama_index/prompts/
--rw-r--r--   0 runner    (1001) docker     (123)       87 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/prompts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6633 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/prompts/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     1969 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/prompts/chat_prompts.py
--rw-r--r--   0 runner    (1001) docker     (123)     1664 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/prompts/choice_select.py
--rw-r--r--   0 runner    (1001) docker     (123)     1153 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/prompts/default_choice_select.py
--rw-r--r--   0 runner    (1001) docker     (123)     1134 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/prompts/default_prompt_selectors.py
--rw-r--r--   0 runner    (1001) docker     (123)    10843 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/prompts/default_prompts.py
--rw-r--r--   0 runner    (1001) docker     (123)     1228 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/prompts/prompt_type.py
--rw-r--r--   0 runner    (1001) docker     (123)     7685 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/prompts/prompts.py
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/py.typed
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.912060 llama_index-0.6.9/llama_index/query_engine/
--rw-r--r--   0 runner    (1001) docker     (123)      750 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/query_engine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3569 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/query_engine/graph_query_engine.py
--rw-r--r--   0 runner    (1001) docker     (123)     5811 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/query_engine/multistep_query_engine.py
--rw-r--r--   0 runner    (1001) docker     (123)     6984 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/query_engine/retriever_query_engine.py
--rw-r--r--   0 runner    (1001) docker     (123)     4570 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/query_engine/router_query_engine.py
--rw-r--r--   0 runner    (1001) docker     (123)     5431 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/query_engine/sub_question_query_engine.py
--rw-r--r--   0 runner    (1001) docker     (123)     2964 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/query_engine/transform_query_engine.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.912060 llama_index-0.6.9/llama_index/question_gen/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/question_gen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2840 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/question_gen/llm_generators.py
--rw-r--r--   0 runner    (1001) docker     (123)      635 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/question_gen/output_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)     2185 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/question_gen/prompts.py
--rw-r--r--   0 runner    (1001) docker     (123)      612 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/question_gen/types.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.916061 llama_index-0.6.9/llama_index/readers/
--rw-r--r--   0 runner    (1001) docker     (123)     3043 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      659 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.916061 llama_index-0.6.9/llama_index/readers/chatgpt_plugin/
--rw-r--r--   0 runner    (1001) docker     (123)      145 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/chatgpt_plugin/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2111 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/chatgpt_plugin/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     3755 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/chroma.py
--rw-r--r--   0 runner    (1001) docker     (123)     3308 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/database.py
--rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/deeplake.py
--rw-r--r--   0 runner    (1001) docker     (123)     4981 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/discord_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     7769 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/download.py
--rw-r--r--   0 runner    (1001) docker     (123)     2392 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/elasticsearch.py
--rw-r--r--   0 runner    (1001) docker     (123)     2510 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/faiss.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.916061 llama_index-0.6.9/llama_index/readers/file/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7637 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     1813 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/docs_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     1267 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/epub_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     2984 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/image_caption_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     3772 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/image_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     2987 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/image_vision_llm_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     1292 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/ipynb_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     3734 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/markdown_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     3204 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/mbox_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     3583 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/slides_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     3445 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/tabular_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/file/video_audio_reader.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/readers/github_readers/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/github_readers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11730 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/github_readers/github_api_client.py
--rw-r--r--   0 runner    (1001) docker     (123)    15726 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/github_readers/github_repository_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     5473 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/github_readers/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/readers/google_readers/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/google_readers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5659 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/google_readers/gdocs.py
--rw-r--r--   0 runner    (1001) docker     (123)     4989 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/google_readers/gsheets.py
--rw-r--r--   0 runner    (1001) docker     (123)     3708 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/json.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/readers/make_com/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/make_com/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1693 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/make_com/wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1236 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/mbox.py
--rw-r--r--   0 runner    (1001) docker     (123)     2307 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/metal.py
--rw-r--r--   0 runner    (1001) docker     (123)     4588 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/milvus.py
--rw-r--r--   0 runner    (1001) docker     (123)     2608 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/mongo.py
--rw-r--r--   0 runner    (1001) docker     (123)     5541 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/myscale.py
--rw-r--r--   0 runner    (1001) docker     (123)     5679 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/notion.py
--rw-r--r--   0 runner    (1001) docker     (123)     1582 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/obsidian.py
--rw-r--r--   0 runner    (1001) docker     (123)     2766 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/pinecone.py
--rw-r--r--   0 runner    (1001) docker     (123)     6920 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/qdrant.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/readers/redis/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/redis/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3492 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/redis/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/readers/schema/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/schema/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1214 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/schema/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     7892 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/slack.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/readers/steamship/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/steamship/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3498 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/steamship/file_reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     1042 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/string_iterable.py
--rw-r--r--   0 runner    (1001) docker     (123)     1918 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/twitter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/readers/weaviate/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/weaviate/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7105 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/weaviate/client.py
--rw-r--r--   0 runner    (1001) docker     (123)     3986 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/weaviate/reader.py
--rw-r--r--   0 runner    (1001) docker     (123)     1867 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/weaviate/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     7987 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/web.py
--rw-r--r--   0 runner    (1001) docker     (123)      981 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/wikipedia.py
--rw-r--r--   0 runner    (1001) docker     (123)     1255 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/readers/youtube_transcript.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/response/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/response/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2211 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/response/notebook_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1574 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/response/pprint_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3080 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/response/schema.py
--rw-r--r--   0 runner    (1001) docker     (123)      262 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/response/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/retrievers/
--rw-r--r--   0 runner    (1001) docker     (123)     1329 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/retrievers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1063 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/retrievers/transform_retriever.py
--rw-r--r--   0 runner    (1001) docker     (123)     3542 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/schema.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/selectors/
--rw-r--r--   0 runner    (1001) docker     (123)      259 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/selectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7068 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/selectors/llm_selectors.py
--rw-r--r--   0 runner    (1001) docker     (123)     2438 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/selectors/prompts.py
--rw-r--r--   0 runner    (1001) docker     (123)     2235 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/selectors/types.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/storage/
--rw-r--r--   0 runner    (1001) docker     (123)      124 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.920061 llama_index-0.6.9/llama_index/storage/docstore/
--rw-r--r--   0 runner    (1001) docker     (123)      536 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/docstore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4825 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/docstore/keyval_docstore.py
--rw-r--r--   0 runner    (1001) docker     (123)     1408 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/docstore/mongo_docstore.py
--rw-r--r--   0 runner    (1001) docker     (123)      762 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/docstore/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     3005 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/docstore/simple_docstore.py
--rw-r--r--   0 runner    (1001) docker     (123)     2668 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/docstore/types.py
--rw-r--r--   0 runner    (1001) docker     (123)      915 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/docstore/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.924061 llama_index-0.6.9/llama_index/storage/index_store/
--rw-r--r--   0 runner    (1001) docker     (123)      301 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/index_store/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2209 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/index_store/keyval_index_store.py
--rw-r--r--   0 runner    (1001) docker     (123)     1341 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/index_store/mongo_index_store.py
--rw-r--r--   0 runner    (1001) docker     (123)     2176 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/index_store/simple_index_store.py
--rw-r--r--   0 runner    (1001) docker     (123)      966 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/index_store/types.py
--rw-r--r--   0 runner    (1001) docker     (123)      635 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/index_store/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.924061 llama_index-0.6.9/llama_index/storage/kvstore/
--rw-r--r--   0 runner    (1001) docker     (123)      187 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/kvstore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4061 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/kvstore/mongodb_kvstore.py
--rw-r--r--   0 runner    (1001) docker     (123)     3862 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/kvstore/s3_kvstore.py
--rw-r--r--   0 runner    (1001) docker     (123)     2728 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/kvstore/simple_kvstore.py
--rw-r--r--   0 runner    (1001) docker     (123)     1049 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/kvstore/types.py
--rw-r--r--   0 runner    (1001) docker     (123)     4714 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/storage/storage_context.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.924061 llama_index-0.6.9/llama_index/token_counter/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/token_counter/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4664 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/token_counter/mock_chain_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)      722 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/token_counter/mock_embed_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     2874 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/token_counter/token_counter.py
--rw-r--r--   0 runner    (1001) docker     (123)      908 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/token_counter/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.924061 llama_index-0.6.9/llama_index/tools/
--rw-r--r--   0 runner    (1001) docker     (123)      191 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/tools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1557 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/tools/query_engine.py
--rw-r--r--   0 runner    (1001) docker     (123)      366 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/tools/types.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.924061 llama_index-0.6.9/llama_index/tts/
--rw-r--r--   0 runner    (1001) docker     (123)      154 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/tts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2589 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/tts/bark.py
--rw-r--r--   0 runner    (1001) docker     (123)      631 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/tts/base.py
--rw-r--r--   0 runner    (1001) docker     (123)     1282 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/tts/elevenlabs.py
--rw-r--r--   0 runner    (1001) docker     (123)       81 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/types.py
--rw-r--r--   0 runner    (1001) docker     (123)     5847 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.928061 llama_index-0.6.9/llama_index/vector_stores/
--rw-r--r--   0 runner    (1001) docker     (123)     1393 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5332 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/chatgpt_plugin.py
--rw-r--r--   0 runner    (1001) docker     (123)     5336 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/chroma.py
--rw-r--r--   0 runner    (1001) docker     (123)     8767 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/deeplake.py
--rw-r--r--   0 runner    (1001) docker     (123)     5639 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/faiss.py
--rw-r--r--   0 runner    (1001) docker     (123)     4047 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/lancedb.py
--rw-r--r--   0 runner    (1001) docker     (123)     4882 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/metal.py
--rw-r--r--   0 runner    (1001) docker     (123)    15963 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/milvus.py
--rw-r--r--   0 runner    (1001) docker     (123)     8535 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/myscale.py
--rw-r--r--   0 runner    (1001) docker     (123)     7323 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/opensearch.py
--rw-r--r--   0 runner    (1001) docker     (123)    10362 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/pinecone.py
--rw-r--r--   0 runner    (1001) docker     (123)     7363 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/qdrant.py
--rw-r--r--   0 runner    (1001) docker     (123)    14558 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/redis.py
--rw-r--r--   0 runner    (1001) docker     (123)     2237 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/registry.py
--rw-r--r--   0 runner    (1001) docker     (123)     5970 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/simple.py
--rw-r--r--   0 runner    (1001) docker     (123)     3524 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/types.py
--rw-r--r--   0 runner    (1001) docker     (123)     2286 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     3516 2023-05-19 20:30:23.000000 llama_index-0.6.9/llama_index/vector_stores/weaviate.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.896060 llama_index-0.6.9/llama_index.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)     4255 2023-05-19 20:30:37.000000 llama_index-0.6.9/llama_index.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    15530 2023-05-19 20:30:37.000000 llama_index-0.6.9/llama_index.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-19 20:30:37.000000 llama_index-0.6.9/llama_index.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      129 2023-05-19 20:30:37.000000 llama_index-0.6.9/llama_index.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       18 2023-05-19 20:30:37.000000 llama_index-0.6.9/llama_index.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)      276 2023-05-19 20:30:23.000000 llama_index-0.6.9/pyproject.toml
--rw-r--r--   0 runner    (1001) docker     (123)       38 2023-05-19 20:30:37.940061 llama_index-0.6.9/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     1216 2023-05-19 20:30:23.000000 llama_index-0.6.9/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.928061 llama_index-0.6.9/tests/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.928061 llama_index-0.6.9/tests/callbacks/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/callbacks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2975 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/callbacks/test_llama_debug.py
--rw-r--r--   0 runner    (1001) docker     (123)     1966 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.928061 llama_index-0.6.9/tests/embeddings/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/embeddings/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3002 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/embeddings/test_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.928061 llama_index-0.6.9/tests/indices/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.928061 llama_index-0.6.9/tests/indices/composability/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/composability/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1008 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/composability/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     1023 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.928061 llama_index-0.6.9/tests/indices/document_summary/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/document_summary/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1340 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/document_summary/test_index.py
--rw-r--r--   0 runner    (1001) docker     (123)      367 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/document_summary/test_retrievers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.932061 llama_index-0.6.9/tests/indices/empty/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/empty/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      548 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/empty/test_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.932061 llama_index-0.6.9/tests/indices/keyword_table/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/keyword_table/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6259 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/keyword_table/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     1185 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/keyword_table/test_retrievers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1010 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/keyword_table/test_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.932061 llama_index-0.6.9/tests/indices/knowledge_graph/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/knowledge_graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      381 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/knowledge_graph/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)     6415 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/knowledge_graph/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     3900 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/knowledge_graph/test_retrievers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.932061 llama_index-0.6.9/tests/indices/list/
--rw-r--r--   0 runner    (1001) docker     (123)       34 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/list/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6487 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/list/test_index.py
--rw-r--r--   0 runner    (1001) docker     (123)     2749 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/list/test_retrievers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.932061 llama_index-0.6.9/tests/indices/postprocessor/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/postprocessor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13872 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/postprocessor/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     2556 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/postprocessor/test_llm_rerank.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.932061 llama_index-0.6.9/tests/indices/query/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/query/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1832 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/query/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.932061 llama_index-0.6.9/tests/indices/query/query_transform/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/query/query_transform/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      267 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/query/query_transform/mock_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)      787 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/query/query_transform/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     6649 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/query/test_compose.py
--rw-r--r--   0 runner    (1001) docker     (123)    12483 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/query/test_compose_vector.py
--rw-r--r--   0 runner    (1001) docker     (123)     2196 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/query/test_query_bundle.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.932061 llama_index-0.6.9/tests/indices/struct_store/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/struct_store/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1195 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/struct_store/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)    11931 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/struct_store/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     1234 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/struct_store/test_pandas.py
--rw-r--r--   0 runner    (1001) docker     (123)     3901 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/struct_store/test_sql_query.py
--rw-r--r--   0 runner    (1001) docker     (123)     5540 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/test_loading.py
--rw-r--r--   0 runner    (1001) docker     (123)     2007 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/test_loading_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     1940 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/test_node_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     7178 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/test_prompt_helper.py
--rw-r--r--   0 runner    (1001) docker     (123)      463 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/test_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/indices/tree/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/tree/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      992 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/tree/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)     4444 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/tree/test_embedding_retriever.py
--rw-r--r--   0 runner    (1001) docker     (123)     7789 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/tree/test_index.py
--rw-r--r--   0 runner    (1001) docker     (123)     1332 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/tree/test_retrievers.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/indices/vector_store/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/indices/vector_store/auto_retriever/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/auto_retriever/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1179 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/auto_retriever/test_output_parser.py
--rw-r--r--   0 runner    (1001) docker     (123)      767 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/conftest.py
--rw-r--r--   0 runner    (1001) docker     (123)     1153 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/mock_faiss.py
--rw-r--r--   0 runner    (1001) docker     (123)     1020 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/mock_services.py
--rw-r--r--   0 runner    (1001) docker     (123)     2960 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/test_faiss.py
--rw-r--r--   0 runner    (1001) docker     (123)     4134 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/test_myscale.py
--rw-r--r--   0 runner    (1001) docker     (123)     1931 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/test_pinecone.py
--rw-r--r--   0 runner    (1001) docker     (123)     5027 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/test_retrievers.py
--rw-r--r--   0 runner    (1001) docker     (123)     6554 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/test_simple.py
--rw-r--r--   0 runner    (1001) docker     (123)     2125 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/indices/vector_store/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/langchain_helpers/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/langchain_helpers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3426 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/langchain_helpers/test_text_splitter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/llm_predictor/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/llm_predictor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3174 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/llm_predictor/test_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/logger/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/logger/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1244 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/logger/test_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/mock_utils/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/mock_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9066 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/mock_utils/mock_predict.py
--rw-r--r--   0 runner    (1001) docker     (123)     2253 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/mock_utils/mock_prompts.py
--rw-r--r--   0 runner    (1001) docker     (123)     1141 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/mock_utils/mock_text_splitter.py
--rw-r--r--   0 runner    (1001) docker     (123)      739 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/mock_utils/mock_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/optimization/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/optimization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2466 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/optimization/test_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/output_parsers/
--rw-r--r--   0 runner    (1001) docker     (123)       19 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/output_parsers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1507 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/output_parsers/test_base.py
--rw-r--r--   0 runner    (1001) docker     (123)     1555 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/output_parsers/test_selection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/playground/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/playground/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3494 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/playground/test_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/prompts/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/prompts/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4655 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/prompts/test_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.936061 llama_index-0.6.9/tests/question_gen/
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/question_gen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      817 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/question_gen/test_llm_generators.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.940061 llama_index-0.6.9/tests/readers/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/readers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11137 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/readers/test_file.py
--rw-r--r--   0 runner    (1001) docker     (123)     1773 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/readers/test_json.py
--rw-r--r--   0 runner    (1001) docker     (123)     1732 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/readers/test_mongo.py
--rw-r--r--   0 runner    (1001) docker     (123)      339 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/readers/test_string_iterable.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.940061 llama_index-0.6.9/tests/selectors/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/selectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1261 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/selectors/test_llm_selectors.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.940061 llama_index-0.6.9/tests/storage/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/storage/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      548 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/storage/conftest.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.940061 llama_index-0.6.9/tests/storage/docstore/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/storage/docstore/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2115 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/storage/docstore/test_mongo_docstore.py
--rw-r--r--   0 runner    (1001) docker     (123)     2137 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/storage/docstore/test_simple_docstore.py
--rw-r--r--   0 runner    (1001) docker     (123)      959 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/storage/test_storage_context.py
--rw-r--r--   0 runner    (1001) docker     (123)     3016 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/test_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.940061 llama_index-0.6.9/tests/token_predictor/
--rw-r--r--   0 runner    (1001) docker     (123)       17 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/token_predictor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1940 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/token_predictor/test_base.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:37.940061 llama_index-0.6.9/tests/vector_stores/
--rw-r--r--   0 runner    (1001) docker     (123)        0 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/vector_stores/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4975 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/vector_stores/test_qdrant.py
--rw-r--r--   0 runner    (1001) docker     (123)     1002 2023-05-19 20:30:23.000000 llama_index-0.6.9/tests/vector_stores/test_weaviate.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.479011 llama_index-0.7.0/
+-rw-r--r--   0 runner    (1001) docker     (123)     1064 2023-07-04 15:28:19.000000 llama_index-0.7.0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (123)       72 2023-07-04 15:28:19.000000 llama_index-0.7.0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (123)     4073 2023-07-04 15:28:38.479011 llama_index-0.7.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)     3826 2023-07-04 15:28:19.000000 llama_index-0.7.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.411008 llama_index-0.7.0/llama_index/
+-rw-r--r--   0 runner    (1001) docker     (123)        6 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/VERSION
+-rw-r--r--   0 runner    (1001) docker     (123)     5972 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.415008 llama_index-0.7.0/llama_index/agent/
+-rw-r--r--   0 runner    (1001) docker     (123)      362 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/agent/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5600 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/agent/context_retriever_agent.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10358 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/agent/openai_agent.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2373 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/agent/retriever_openai_agent.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1217 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/async_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.415008 llama_index-0.7.0/llama_index/bridge/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/bridge/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2778 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/bridge/langchain.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.415008 llama_index-0.7.0/llama_index/callbacks/
+-rw-r--r--   0 runner    (1001) docker     (123)      417 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/callbacks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6374 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/callbacks/aim.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6010 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/callbacks/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7826 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/callbacks/llama_debug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2327 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/callbacks/schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5202 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/callbacks/token_counting.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20607 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/callbacks/wandb_callback.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.415008 llama_index-0.7.0/llama_index/chat_engine/
+-rw-r--r--   0 runner    (1001) docker     (123)      295 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/chat_engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5428 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/chat_engine/condense_question.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5328 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/chat_engine/react.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2527 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/chat_engine/simple.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1878 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/chat_engine/types.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.415008 llama_index-0.7.0/llama_index/composability/
+-rw-r--r--   0 runner    (1001) docker     (123)      232 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/composability/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      171 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/composability/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3398 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/composability/joint_qa_summary.py
+-rw-r--r--   0 runner    (1001) docker     (123)      501 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.415008 llama_index-0.7.0/llama_index/data_structs/
+-rw-r--r--   0 runner    (1001) docker     (123)      328 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/data_structs/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7832 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/data_structs/data_structs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2283 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/data_structs/document_summary.py
+-rw-r--r--   0 runner    (1001) docker     (123)      916 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/data_structs/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3698 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/data_structs/struct_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1025 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/data_structs/table.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.419008 llama_index-0.7.0/llama_index/embeddings/
+-rw-r--r--   0 runner    (1001) docker     (123)      315 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/embeddings/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10009 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/embeddings/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1164 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/embeddings/google.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1112 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/embeddings/langchain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9825 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/embeddings/openai.py
+-rw-r--r--   0 runner    (1001) docker     (123)      559 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/embeddings/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.419008 llama_index-0.7.0/llama_index/evaluation/
+-rw-r--r--   0 runner    (1001) docker     (123)      369 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/evaluation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13358 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/evaluation/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5684 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/evaluation/dataset_generation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2985 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/evaluation/guideline_eval.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.419008 llama_index-0.7.0/llama_index/graph_stores/
+-rw-r--r--   0 runner    (1001) docker     (123)      211 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/graph_stores/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16582 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/graph_stores/nebulagraph.py
+-rw-r--r--   0 runner    (1001) docker     (123)      663 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/graph_stores/registery.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5047 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/graph_stores/simple.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1704 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/graph_stores/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)      544 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/img_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.419008 llama_index-0.7.0/llama_index/indices/
+-rw-r--r--   0 runner    (1001) docker     (123)      802 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14414 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      845 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/base_retriever.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.419008 llama_index-0.7.0/llama_index/indices/common/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.419008 llama_index-0.7.0/llama_index/indices/common/struct_store/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/common/struct_store/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8806 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/common/struct_store/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      776 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/common/struct_store/schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2671 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/common/struct_store/sql.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.423008 llama_index-0.7.0/llama_index/indices/common_tree/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/common_tree/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8846 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/common_tree/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.423008 llama_index-0.7.0/llama_index/indices/composability/
+-rw-r--r--   0 runner    (1001) docker     (123)      180 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/composability/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4874 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/composability/graph.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.423008 llama_index-0.7.0/llama_index/indices/document_summary/
+-rw-r--r--   0 runner    (1001) docker     (123)      458 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/document_summary/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7105 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/document_summary/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7056 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/document_summary/retrievers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.423008 llama_index-0.7.0/llama_index/indices/empty/
+-rw-r--r--   0 runner    (1001) docker     (123)      224 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/empty/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2942 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/empty/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1149 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/empty/retrievers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.423008 llama_index-0.7.0/llama_index/indices/keyword_table/
+-rw-r--r--   0 runner    (1001) docker     (123)      860 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/keyword_table/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8813 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/keyword_table/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)      694 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/keyword_table/rake_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5858 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/keyword_table/retrievers.py
+-rw-r--r--   0 runner    (1001) docker     (123)      892 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/keyword_table/simple_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2264 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/keyword_table/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.423008 llama_index-0.7.0/llama_index/indices/knowledge_graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      327 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/knowledge_graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10679 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/knowledge_graph/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12275 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/knowledge_graph/retriever.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.423008 llama_index-0.7.0/llama_index/indices/list/
+-rw-r--r--   0 runner    (1001) docker     (123)      392 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/list/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4494 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/list/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6828 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/list/retrievers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3599 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/loading.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.423008 llama_index-0.7.0/llama_index/indices/postprocessor/
+-rw-r--r--   0 runner    (1001) docker     (123)     1089 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/postprocessor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1752 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/postprocessor/cohere_rerank.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3098 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/postprocessor/llm_rerank.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12587 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/postprocessor/node.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8653 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/postprocessor/node_recency.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4655 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/postprocessor/optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5388 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/postprocessor/pii.py
+-rw-r--r--   0 runner    (1001) docker     (123)      434 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/postprocessor/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7137 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/prompt_helper.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.427008 llama_index-0.7.0/llama_index/indices/query/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/query/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2315 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/query/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6059 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/query/embedding_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.427008 llama_index-0.7.0/llama_index/indices/query/query_transform/
+-rw-r--r--   0 runner    (1001) docker     (123)      281 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/query/query_transform/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8829 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/query/query_transform/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4038 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/query/query_transform/feedback_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5110 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/query/query_transform/prompts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1248 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/query/schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1232 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8840 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/service_context.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.427008 llama_index-0.7.0/llama_index/indices/struct_store/
+-rw-r--r--   0 runner    (1001) docker     (123)      955 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/struct_store/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2394 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/struct_store/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5796 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/struct_store/container_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6233 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/struct_store/json_query.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2481 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/struct_store/pandas.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6192 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/struct_store/sql.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15599 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/struct_store/sql_query.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.427008 llama_index-0.7.0/llama_index/indices/tree/
+-rw-r--r--   0 runner    (1001) docker     (123)      657 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/tree/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1541 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/tree/all_leaf_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6802 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/tree/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7912 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/tree/inserter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4729 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/tree/select_leaf_embedding_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15368 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/tree/select_leaf_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1324 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/tree/tree_root_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (123)      733 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/tree/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3616 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.427008 llama_index-0.7.0/llama_index/indices/vector_store/
+-rw-r--r--   0 runner    (1001) docker     (123)      386 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/vector_store/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12051 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/vector_store/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.427008 llama_index-0.7.0/llama_index/indices/vector_store/retrievers/
+-rw-r--r--   0 runner    (1001) docker     (123)      267 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/vector_store/retrievers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.427008 llama_index-0.7.0/llama_index/indices/vector_store/retrievers/auto_retriever/
+-rw-r--r--   0 runner    (1001) docker     (123)      167 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/vector_store/retrievers/auto_retriever/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4453 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/vector_store/retrievers/auto_retriever/auto_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (123)      678 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/vector_store/retrievers/auto_retriever/output_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2784 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/vector_store/retrievers/auto_retriever/prompts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5099 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/indices/vector_store/retrievers/retriever.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.431008 llama_index-0.7.0/llama_index/langchain_helpers/
+-rw-r--r--   0 runner    (1001) docker     (123)       39 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/langchain_helpers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.431008 llama_index-0.7.0/llama_index/langchain_helpers/agents/
+-rw-r--r--   0 runner    (1001) docker     (123)      514 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/langchain_helpers/agents/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2924 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/langchain_helpers/agents/agents.py
+-rw-r--r--   0 runner    (1001) docker     (123)      802 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/langchain_helpers/agents/toolkits.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2485 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/langchain_helpers/agents/tools.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7595 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/langchain_helpers/memory_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3293 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/langchain_helpers/sql_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1200 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/langchain_helpers/streaming.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19862 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/langchain_helpers/text_splitter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.431008 llama_index-0.7.0/llama_index/llm_predictor/
+-rw-r--r--   0 runner    (1001) docker     (123)      390 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6604 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6556 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/mock.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2186 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/structured.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1478 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.431008 llama_index-0.7.0/llama_index/llm_predictor/vellum/
+-rw-r--r--   0 runner    (1001) docker     (123)      386 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/vellum/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      151 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/vellum/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6554 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/vellum/predictor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9755 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/vellum/prompt_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1078 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/vellum/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)      278 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llm_predictor/vellum/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.435009 llama_index-0.7.0/llama_index/llms/
+-rw-r--r--   0 runner    (1001) docker     (123)      412 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3317 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1958 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/custom.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8920 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/generic_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6232 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/huggingface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4239 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/langchain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4558 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/langchain_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      718 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/mock.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13959 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/openai.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6685 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/openai_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2761 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/palm.py
+-rw-r--r--   0 runner    (1001) docker     (123)      498 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/llms/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.435009 llama_index-0.7.0/llama_index/logger/
+-rw-r--r--   0 runner    (1001) docker     (123)       95 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/logger/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      995 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/logger/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.435009 llama_index-0.7.0/llama_index/node_parser/
+-rw-r--r--   0 runner    (1001) docker     (123)      184 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/node_parser/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      548 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/node_parser/interface.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4114 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/node_parser/node_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3701 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/node_parser/simple.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.435009 llama_index-0.7.0/llama_index/objects/
+-rw-r--r--   0 runner    (1001) docker     (123)      567 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/objects/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2116 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/objects/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2550 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/objects/base_node_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2170 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/objects/table_node_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3343 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/objects/tool_node_mapping.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.435009 llama_index-0.7.0/llama_index/output_parsers/
+-rw-r--r--   0 runner    (1001) docker     (123)      241 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/output_parsers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      284 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/output_parsers/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2737 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/output_parsers/guardrails.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1826 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/output_parsers/langchain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1991 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/output_parsers/pydantic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2319 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/output_parsers/selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1014 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/output_parsers/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.435009 llama_index-0.7.0/llama_index/playground/
+-rw-r--r--   0 runner    (1001) docker     (123)      202 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/playground/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6118 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/playground/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.435009 llama_index-0.7.0/llama_index/program/
+-rw-r--r--   0 runner    (1001) docker     (123)      528 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      469 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/base_program.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2187 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/guidance_program.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2322 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/llm_program.py
+-rw-r--r--   0 runner    (1001) docker     (123)      875 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/llm_prompt_program.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3404 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/openai_program.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.435009 llama_index-0.7.0/llama_index/program/predefined/
+-rw-r--r--   0 runner    (1001) docker     (123)      321 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/predefined/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7864 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/predefined/df.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.439009 llama_index-0.7.0/llama_index/program/predefined/evaporate/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/predefined/evaporate/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9960 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/predefined/evaporate/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9569 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/predefined/evaporate/extractor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4701 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/program/predefined/evaporate/prompts.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.439009 llama_index-0.7.0/llama_index/prompts/
+-rw-r--r--   0 runner    (1001) docker     (123)       87 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6542 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1909 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/chat_prompts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1296 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/choice_select.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1237 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/default_prompt_selectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11478 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/default_prompts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5562 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/guidance_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      741 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/prompt_selector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1543 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/prompt_type.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3530 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/prompts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1130 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/prompts/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/py.typed
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.439009 llama_index-0.7.0/llama_index/query_engine/
+-rw-r--r--   0 runner    (1001) docker     (123)     1643 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12317 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/citation_query_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.439009 llama_index-0.7.0/llama_index/query_engine/flare/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/flare/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6167 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/flare/answer_inserter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9832 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/flare/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2078 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/flare/output_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)      163 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/flare/schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4339 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/graph_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6677 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/multistep_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5309 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/pandas_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7073 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/retriever_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4875 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/retry_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2743 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/retry_source_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12552 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/router_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12346 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/sql_join_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5981 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/sql_vector_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7259 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/sub_question_query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3156 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/query_engine/transform_query_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.443009 llama_index-0.7.0/llama_index/question_gen/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/question_gen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2292 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/question_gen/guidance_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2987 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/question_gen/llm_generators.py
+-rw-r--r--   0 runner    (1001) docker     (123)      664 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/question_gen/output_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2049 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/question_gen/prompts.py
+-rw-r--r--   0 runner    (1001) docker     (123)      612 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/question_gen/types.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.443009 llama_index-0.7.0/llama_index/readers/
+-rw-r--r--   0 runner    (1001) docker     (123)     3105 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      647 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.447009 llama_index-0.7.0/llama_index/readers/chatgpt_plugin/
+-rw-r--r--   0 runner    (1001) docker     (123)      145 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/chatgpt_plugin/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2098 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/chatgpt_plugin/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3909 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/chroma.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3300 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/database.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3440 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/deeplake.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4971 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/discord_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7862 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/download.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2377 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/elasticsearch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2497 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/faiss.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.447009 llama_index-0.7.0/llama_index/readers/file/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8106 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1953 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/docs_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1263 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/epub_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2975 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/image_caption_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3785 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/image_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2977 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/image_vision_llm_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1287 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/ipynb_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3742 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/markdown_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3500 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/mbox_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3579 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/slides_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3554 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/tabular_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1844 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/file/video_audio_reader.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.447009 llama_index-0.7.0/llama_index/readers/github_readers/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/github_readers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11732 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/github_readers/github_api_client.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15684 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/github_readers/github_repository_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5473 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/github_readers/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.447009 llama_index-0.7.0/llama_index/readers/google_readers/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/google_readers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5649 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/google_readers/gdocs.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4979 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/google_readers/gsheets.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3719 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/json.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.447009 llama_index-0.7.0/llama_index/readers/make_com/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/make_com/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1670 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/make_com/wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1223 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/mbox.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2297 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/metal.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4572 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/milvus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2600 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/mongo.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5519 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/myscale.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5672 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/notion.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1570 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/obsidian.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2756 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/pinecone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2755 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/psychic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6900 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/qdrant.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.447009 llama_index-0.7.0/llama_index/readers/redis/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/redis/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3492 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/redis/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.447009 llama_index-0.7.0/llama_index/readers/schema/
+-rw-r--r--   0 runner    (1001) docker     (123)      204 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/schema/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/schema/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7882 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/slack.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.447009 llama_index-0.7.0/llama_index/readers/steamship/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/steamship/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3429 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/steamship/file_reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1028 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/string_iterable.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1910 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/twitter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.447009 llama_index-0.7.0/llama_index/readers/weaviate/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/weaviate/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3999 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/weaviate/reader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7979 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/web.py
+-rw-r--r--   0 runner    (1001) docker     (123)      973 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/wikipedia.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1247 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/readers/youtube_transcript.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.451010 llama_index-0.7.0/llama_index/response/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2196 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response/notebook_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1569 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response/pprint_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3147 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response/schema.py
+-rw-r--r--   0 runner    (1001) docker     (123)      262 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.451010 llama_index-0.7.0/llama_index/response_synthesizers/
+-rw-r--r--   0 runner    (1001) docker     (123)      868 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3662 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/accumulate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5640 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1852 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/compact_and_accumulate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1823 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/compact_and_refine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3702 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2092 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/generation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9181 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/refine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2828 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/simple_summarize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5508 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/tree_summarize.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2017 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/response_synthesizers/type.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.451010 llama_index-0.7.0/llama_index/retrievers/
+-rw-r--r--   0 runner    (1001) docker     (123)     1328 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/retrievers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1043 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/retrievers/transform_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11451 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/schema.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.451010 llama_index-0.7.0/llama_index/selectors/
+-rw-r--r--   0 runner    (1001) docker     (123)      259 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/selectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7325 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/selectors/llm_selectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2917 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/selectors/prompts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4392 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/selectors/pydantic_selectors.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2716 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/selectors/types.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.451010 llama_index-0.7.0/llama_index/storage/
+-rw-r--r--   0 runner    (1001) docker     (123)      124 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.455010 llama_index-0.7.0/llama_index/storage/docstore/
+-rw-r--r--   0 runner    (1001) docker     (123)      637 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/docstore/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      738 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/docstore/dynamodb_docstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9136 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/docstore/keyval_docstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1408 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/docstore/mongo_docstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1455 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/docstore/redis_docstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)      762 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/docstore/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3166 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/docstore/simple_docstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3445 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/docstore/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2499 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/docstore/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.455010 llama_index-0.7.0/llama_index/storage/index_store/
+-rw-r--r--   0 runner    (1001) docker     (123)      398 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/index_store/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      822 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/index_store/dynamodb_index_store.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2221 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/index_store/keyval_index_store.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1341 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/index_store/mongo_index_store.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1384 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/index_store/redis_index_store.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2338 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/index_store/simple_index_store.py
+-rw-r--r--   0 runner    (1001) docker     (123)      966 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/index_store/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)      710 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/index_store/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.455010 llama_index-0.7.0/llama_index/storage/kvstore/
+-rw-r--r--   0 runner    (1001) docker     (123)      270 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/kvstore/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5359 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/kvstore/dynamodb_kvstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4061 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/kvstore/mongodb_kvstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3645 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/kvstore/redis_kvstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3862 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/kvstore/s3_kvstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2728 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/kvstore/simple_kvstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1049 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/kvstore/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6205 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/storage/storage_context.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.455010 llama_index-0.7.0/llama_index/token_counter/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/token_counter/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      722 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/token_counter/mock_embed_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)      908 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/token_counter/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.455010 llama_index-0.7.0/llama_index/tools/
+-rw-r--r--   0 runner    (1001) docker     (123)      359 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2306 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/function_tool.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4881 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/ondemand_loader_tool.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1949 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/query_engine.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7480 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/query_plan.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.455010 llama_index-0.7.0/llama_index/tools/tool_spec/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/tool_spec/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2074 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/tool_spec/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.459010 llama_index-0.7.0/llama_index/tools/tool_spec/notion/
+-rw-r--r--   0 runner    (1001) docker     (123)       24 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/tool_spec/notion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3202 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/tool_spec/notion/base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.459010 llama_index-0.7.0/llama_index/tools/tool_spec/slack/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/tool_spec/slack/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2264 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/tool_spec/slack/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2418 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1064 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tools/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.459010 llama_index-0.7.0/llama_index/tts/
+-rw-r--r--   0 runner    (1001) docker     (123)      154 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2589 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tts/bark.py
+-rw-r--r--   0 runner    (1001) docker     (123)      631 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tts/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1282 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/tts/elevenlabs.py
+-rw-r--r--   0 runner    (1001) docker     (123)      675 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6784 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/llama_index/vector_stores/
+-rw-r--r--   0 runner    (1001) docker     (123)     1830 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5776 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/chatgpt_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5492 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/chroma.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8903 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/deeplake.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/llama_index/vector_stores/docarray/
+-rw-r--r--   0 runner    (1001) docker     (123)      242 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/docarray/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6802 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/docarray/base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4349 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/docarray/hnsw.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2844 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/docarray/in_memory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5196 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/dynamodb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5668 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/faiss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4183 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/lancedb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4823 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/metal.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17241 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/milvus.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6729 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/mongodb.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9039 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/myscale.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8820 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/opensearch.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10278 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/pinecone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5851 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/postgres.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8457 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/qdrant.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16882 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/redis.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2381 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7003 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/simple.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5071 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/supabase.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8975 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/tair.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3997 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/types.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8963 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/typesense.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3530 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7073 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/weaviate.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4885 2023-07-04 15:28:20.000000 llama_index-0.7.0/llama_index/vector_stores/weaviate_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.411008 llama_index-0.7.0/llama_index.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)     4073 2023-07-04 15:28:38.000000 llama_index-0.7.0/llama_index.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    20504 2023-07-04 15:28:38.000000 llama_index-0.7.0/llama_index.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-04 15:28:38.000000 llama_index-0.7.0/llama_index.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      217 2023-07-04 15:28:38.000000 llama_index-0.7.0/llama_index.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       18 2023-07-04 15:28:38.000000 llama_index-0.7.0/llama_index.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      276 2023-07-04 15:28:20.000000 llama_index-0.7.0/pyproject.toml
+-rw-r--r--   0 runner    (1001) docker     (123)       38 2023-07-04 15:28:38.479011 llama_index-0.7.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     1199 2023-07-04 15:28:20.000000 llama_index-0.7.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/tests/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/tests/callbacks/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/callbacks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3067 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/callbacks/test_llama_debug.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1556 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/callbacks/test_token_counter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/tests/chat_engine/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/chat_engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2032 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/chat_engine/test_condense_question.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1407 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/chat_engine/test_simple.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2144 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/tests/embeddings/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/embeddings/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3612 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/embeddings/test_base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/tests/indices/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/tests/indices/composability/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/composability/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1012 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/composability/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1313 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/tests/indices/document_summary/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/document_summary/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1599 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/document_summary/test_index.py
+-rw-r--r--   0 runner    (1001) docker     (123)      354 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/document_summary/test_retrievers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.463010 llama_index-0.7.0/tests/indices/empty/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/empty/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      554 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/empty/test_base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.467011 llama_index-0.7.0/tests/indices/keyword_table/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/keyword_table/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6518 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/keyword_table/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1175 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/keyword_table/test_retrievers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1010 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/keyword_table/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.467011 llama_index-0.7.0/tests/indices/knowledge_graph/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/knowledge_graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      373 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/knowledge_graph/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6669 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/knowledge_graph/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5243 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/knowledge_graph/test_retrievers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.467011 llama_index-0.7.0/tests/indices/list/
+-rw-r--r--   0 runner    (1001) docker     (123)       34 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/list/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6941 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/list/test_index.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2744 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/list/test_retrievers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.467011 llama_index-0.7.0/tests/indices/postprocessor/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/postprocessor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12320 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/postprocessor/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2641 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/postprocessor/test_llm_rerank.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5029 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/postprocessor/test_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.467011 llama_index-0.7.0/tests/indices/query/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/query/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1859 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/query/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.467011 llama_index-0.7.0/tests/indices/query/query_transform/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/query/query_transform/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      362 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/query/query_transform/mock_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)      787 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/query/query_transform/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6579 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/query/test_compose.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12399 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/query/test_compose_vector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2601 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/query/test_embedding_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2191 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/query/test_query_bundle.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.467011 llama_index-0.7.0/tests/indices/struct_store/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/struct_store/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1195 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/struct_store/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11906 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/struct_store/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2479 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/struct_store/test_json_query.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4283 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/struct_store/test_sql_query.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5505 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/test_loading.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2255 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/test_loading_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2050 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/test_node_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6921 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/test_prompt_helper.py
+-rw-r--r--   0 runner    (1001) docker     (123)      463 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.471011 llama_index-0.7.0/tests/indices/tree/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/tree/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      984 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/tree/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2500 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/tree/test_embedding_retriever.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8219 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/tree/test_index.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1310 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/tree/test_retrievers.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.471011 llama_index-0.7.0/tests/indices/vector_store/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.471011 llama_index-0.7.0/tests/indices/vector_store/auto_retriever/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/auto_retriever/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1179 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/auto_retriever/test_output_parser.py
+-rw-r--r--   0 runner    (1001) docker     (123)      767 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1153 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/mock_faiss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1020 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/mock_services.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2951 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/test_faiss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4070 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/test_myscale.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1916 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/test_pinecone.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4986 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/test_retrievers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7416 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/test_simple.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2158 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/indices/vector_store/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.471011 llama_index-0.7.0/tests/langchain_helpers/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/langchain_helpers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3414 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/langchain_helpers/test_text_splitter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.471011 llama_index-0.7.0/tests/llm_predictor/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llm_predictor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2880 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llm_predictor/test_base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.471011 llama_index-0.7.0/tests/llm_predictor/vellum/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llm_predictor/vellum/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4169 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llm_predictor/vellum/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7343 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llm_predictor/vellum/test_predictor.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2969 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llm_predictor/vellum/test_prompt_registry.py
+-rw-r--r--   0 runner    (1001) docker     (123)      460 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llm_predictor/vellum/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/llms/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llms/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1485 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llms/test_custom.py
+-rw-r--r--   0 runner    (1001) docker     (123)      430 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llms/test_langchain.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7356 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llms/test_openai.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3094 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llms/test_openai_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1230 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/llms/test_palm.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/logger/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/logger/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1244 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/logger/test_base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/mock_utils/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/mock_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6681 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/mock_utils/mock_predict.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2389 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/mock_utils/mock_prompts.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1133 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/mock_utils/mock_text_splitter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      739 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/mock_utils/mock_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/objects/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/objects/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1466 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/objects/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2303 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/objects/test_node_mapping.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/output_parsers/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/output_parsers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1496 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/output_parsers/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1289 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/output_parsers/test_pydantic.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1555 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/output_parsers/test_selection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/playground/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/playground/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3441 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/playground/test_base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/program/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/program/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      712 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/program/test_guidance.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1025 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/program/test_llm_program.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/prompts/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/prompts/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3442 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/prompts/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1260 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/prompts/test_guidance_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/question_gen/
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/question_gen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      867 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/question_gen/test_guidance_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)      817 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/question_gen/test_llm_generators.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.475011 llama_index-0.7.0/tests/readers/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/readers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12234 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/readers/test_file.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1845 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/readers/test_json.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1786 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/readers/test_mongo.py
+-rw-r--r--   0 runner    (1001) docker     (123)      339 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/readers/test_string_iterable.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.479011 llama_index-0.7.0/tests/selectors/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/selectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1261 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/selectors/test_llm_selectors.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.479011 llama_index-0.7.0/tests/storage/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/storage/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      930 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/storage/conftest.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.479011 llama_index-0.7.0/tests/storage/docstore/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/storage/docstore/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3636 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/storage/docstore/test_dynamodb_docstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2122 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/storage/docstore/test_mongo_docstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2956 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/storage/docstore/test_redis_docstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2133 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/storage/docstore/test_simple_docstore.py
+-rw-r--r--   0 runner    (1001) docker     (123)      956 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/storage/test_storage_context.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3016 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.479011 llama_index-0.7.0/tests/token_predictor/
+-rw-r--r--   0 runner    (1001) docker     (123)       17 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/token_predictor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1791 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/token_predictor/test_base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.479011 llama_index-0.7.0/tests/tools/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      383 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/tools/conftest.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1702 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/tools/test_base.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1426 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/tools/test_ondemand_loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)      748 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/tools/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.479011 llama_index-0.7.0/tests/tools/tool_spec/
+-rw-r--r--   0 runner    (1001) docker     (123)       19 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/tools/tool_spec/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2947 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/tools/tool_spec/test_base.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:38.479011 llama_index-0.7.0/tests/vector_stores/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/vector_stores/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5121 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/vector_stores/test_docarray.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3625 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/vector_stores/test_postgres.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6124 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/vector_stores/test_qdrant.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4932 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/vector_stores/test_tair.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1077 2023-07-04 15:28:20.000000 llama_index-0.7.0/tests/vector_stores/test_weaviate.py
```

### Comparing `llama_index-0.6.9/LICENSE` & `llama_index-0.7.0/LICENSE`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/PKG-INFO` & `llama_index-0.7.0/PKG-INFO`

 * *Files 13% similar despite different names*

```diff
@@ -1,62 +1,59 @@
 Metadata-Version: 2.1
 Name: llama_index
-Version: 0.6.9
+Version: 0.7.0
 Summary: Interface between LLMs and your data
 Home-page: https://github.com/jerryjliu/llama_index
+Author: Jerry Liu
 License: MIT
 Description-Content-Type: text/markdown
 License-File: LICENSE
 
 # 🗂️ LlamaIndex 🦙
 
-LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.
+LlamaIndex (GPT Index) is a data framework for your LLM application.
 
 PyPI: 
 - LlamaIndex: https://pypi.org/project/llama-index/.
 - GPT Index (duplicate): https://pypi.org/project/gpt-index/.
 
 Documentation: https://gpt-index.readthedocs.io/.
 
-Twitter: https://twitter.com/gpt_index.
+Twitter: https://twitter.com/llama_index.
 
 Discord: https://discord.gg/dGcwcsnxhU.
 
 ### Ecosystem
 
 - LlamaHub (community library of data loaders): https://llamahub.ai
 - LlamaLab (cutting-edge AGI projects using LlamaIndex): https://github.com/run-llama/llama-lab
 
 
 ## 🚀 Overview
 
 **NOTE**: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!
 
 ### Context
-- LLMs are a phenomenal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.
+- LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.
 - How do we best augment LLMs with our own private data?
-- One paradigm that has emerged is *in-context* learning (the other is finetuning), where we insert context into the input prompt. That way,
-we take advantage of the LLM's reasoning capabilities to generate a response.
 
-To perform LLM's data augmentation in a performant, efficient, and cheap manner, we need to solve two components:
-- Data Ingestion
-- Data Indexing
+We need a comprehensive toolkit to help perform this data augmentation for LLMs.
 
 ### Proposed Solution
 
-That's where the **LlamaIndex** comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion:
+That's where **LlamaIndex** comes in. LlamaIndex is a "data framework" to help you build LLM apps. It provides the following tools:
 
-- Offers **data connectors** to your existing data sources and data formats (API's, PDF's, docs, SQL, etc.)
-- Provides **indices** over your unstructured and structured data for use with LLM's. 
-These indices help to abstract away common boilerplate and pain points for in-context learning:
-   - Storing context in an easy-to-access format for prompt insertion.
-   - Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big.
-   - Dealing with text splitting.
-- Provides users an interface to **query** the index (feed in an input prompt) and obtain a knowledge-augmented output.
-- Offers you a comprehensive toolset trading off cost and performance.
+- Offers **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)
+- Provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.
+- Provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.
+- Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, ChatGPT, anything else).
+
+LlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in
+5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules),
+to fit their needs.
 
 
 ## 💡 Contributing
 
 Interested in contributing? See our [Contribution Guide](CONTRIBUTING.md) for more details.
 
 ## 📄 Documentation
@@ -75,17 +72,17 @@
 Examples are in the `examples` folder. Indices are in the `indices` folder (see list of indices below).
 
 To build a simple vector store index:
 ```python
 import os
 os.environ["OPENAI_API_KEY"] = 'YOUR_OPENAI_API_KEY'
 
-from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader
+from llama_index import VectorStoreIndex, SimpleDirectoryReader
 documents = SimpleDirectoryReader('data').load_data()
-index = GPTVectorStoreIndex.from_documents(documents)
+index = VectorStoreIndex.from_documents(documents)
 ```
 
 
 To query:
 ```python
 query_engine = index.as_query_engine()
 query_engine.query("<question_text>?")
```

### Comparing `llama_index-0.6.9/README.md` & `llama_index-0.7.0/llama_index.egg-info/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,53 +1,59 @@
+Metadata-Version: 2.1
+Name: llama-index
+Version: 0.7.0
+Summary: Interface between LLMs and your data
+Home-page: https://github.com/jerryjliu/llama_index
+Author: Jerry Liu
+License: MIT
+Description-Content-Type: text/markdown
+License-File: LICENSE
+
 # 🗂️ LlamaIndex 🦙
 
-LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.
+LlamaIndex (GPT Index) is a data framework for your LLM application.
 
 PyPI: 
 - LlamaIndex: https://pypi.org/project/llama-index/.
 - GPT Index (duplicate): https://pypi.org/project/gpt-index/.
 
 Documentation: https://gpt-index.readthedocs.io/.
 
-Twitter: https://twitter.com/gpt_index.
+Twitter: https://twitter.com/llama_index.
 
 Discord: https://discord.gg/dGcwcsnxhU.
 
 ### Ecosystem
 
 - LlamaHub (community library of data loaders): https://llamahub.ai
 - LlamaLab (cutting-edge AGI projects using LlamaIndex): https://github.com/run-llama/llama-lab
 
 
 ## 🚀 Overview
 
 **NOTE**: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!
 
 ### Context
-- LLMs are a phenomenal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.
+- LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.
 - How do we best augment LLMs with our own private data?
-- One paradigm that has emerged is *in-context* learning (the other is finetuning), where we insert context into the input prompt. That way,
-we take advantage of the LLM's reasoning capabilities to generate a response.
 
-To perform LLM's data augmentation in a performant, efficient, and cheap manner, we need to solve two components:
-- Data Ingestion
-- Data Indexing
+We need a comprehensive toolkit to help perform this data augmentation for LLMs.
 
 ### Proposed Solution
 
-That's where the **LlamaIndex** comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion:
+That's where **LlamaIndex** comes in. LlamaIndex is a "data framework" to help you build LLM apps. It provides the following tools:
 
-- Offers **data connectors** to your existing data sources and data formats (API's, PDF's, docs, SQL, etc.)
-- Provides **indices** over your unstructured and structured data for use with LLM's. 
-These indices help to abstract away common boilerplate and pain points for in-context learning:
-   - Storing context in an easy-to-access format for prompt insertion.
-   - Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big.
-   - Dealing with text splitting.
-- Provides users an interface to **query** the index (feed in an input prompt) and obtain a knowledge-augmented output.
-- Offers you a comprehensive toolset trading off cost and performance.
+- Offers **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)
+- Provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.
+- Provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.
+- Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, ChatGPT, anything else).
+
+LlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in
+5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules),
+to fit their needs.
 
 
 ## 💡 Contributing
 
 Interested in contributing? See our [Contribution Guide](CONTRIBUTING.md) for more details.
 
 ## 📄 Documentation
@@ -66,17 +72,17 @@
 Examples are in the `examples` folder. Indices are in the `indices` folder (see list of indices below).
 
 To build a simple vector store index:
 ```python
 import os
 os.environ["OPENAI_API_KEY"] = 'YOUR_OPENAI_API_KEY'
 
-from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader
+from llama_index import VectorStoreIndex, SimpleDirectoryReader
 documents = SimpleDirectoryReader('data').load_data()
-index = GPTVectorStoreIndex.from_documents(documents)
+index = VectorStoreIndex.from_documents(documents)
 ```
 
 
 To query:
 ```python
 query_engine = index.as_query_engine()
 query_engine.query("<question_text>?")
```

### Comparing `llama_index-0.6.9/llama_index/__init__.py` & `llama_index-0.7.0/llama_index/__init__.py`

 * *Files 18% similar despite different names*

```diff
@@ -3,58 +3,79 @@
 
 with open(Path(__file__).absolute().parents[0] / "VERSION") as _f:
     __version__ = _f.read().strip()
 
 
 import logging
 from logging import NullHandler
+from typing import Optional
 
 from llama_index.data_structs.struct_type import IndexStructType
 
 # embeddings
 from llama_index.embeddings.langchain import LangchainEmbedding
 from llama_index.embeddings.openai import OpenAIEmbedding
 
-# structured
-from llama_index.indices.common.struct_store.base import SQLDocumentContextBuilder
-from llama_index.indices.composability.graph import ComposableGraph
-from llama_index.indices.empty import GPTEmptyIndex
 
 # indices
 from llama_index.indices.keyword_table import (
+    KeywordTableIndex,
+    RAKEKeywordTableIndex,
+    SimpleKeywordTableIndex,
     GPTKeywordTableIndex,
     GPTRAKEKeywordTableIndex,
     GPTSimpleKeywordTableIndex,
 )
-from llama_index.indices.list import GPTListIndex
+from llama_index.indices.knowledge_graph import (
+    KnowledgeGraphIndex,
+    GPTKnowledgeGraphIndex,
+)
+from llama_index.indices.list import ListIndex, GPTListIndex
+from llama_index.indices.tree import TreeIndex, GPTTreeIndex
+from llama_index.indices.vector_store import VectorStoreIndex, GPTVectorStoreIndex
+from llama_index.indices.document_summary import (
+    DocumentSummaryIndex,
+    GPTDocumentSummaryIndex,
+)
+from llama_index.indices.empty import EmptyIndex, GPTEmptyIndex
+from llama_index.indices.struct_store.pandas import PandasIndex, GPTPandasIndex
+from llama_index.indices.struct_store.sql import (
+    SQLStructStoreIndex,
+    GPTSQLStructStoreIndex,
+)
+
+# structured
+from llama_index.indices.common.struct_store.base import SQLDocumentContextBuilder
+
+# for composability
+from llama_index.indices.composability.graph import ComposableGraph
 
 # loading
 from llama_index.indices.loading import (
     load_graph_from_storage,
     load_index_from_storage,
     load_indices_from_storage,
 )
 
 # prompt helper
 from llama_index.indices.prompt_helper import PromptHelper
 
 # Response Synthesizer
-from llama_index.indices.query.response_synthesis import ResponseSynthesizer
+from llama_index.response_synthesizers.factory import get_response_synthesizer
 
 # QueryBundle
 from llama_index.indices.query.schema import QueryBundle
 
-# for composability
-from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.struct_store.sql import GPTSQLStructStoreIndex
-from llama_index.indices.tree import GPTTreeIndex
-from llama_index.indices.vector_store import GPTVectorStoreIndex
+from llama_index.indices.service_context import (
+    ServiceContext,
+    set_global_service_context,
+)
 
 # langchain helper
-from llama_index.langchain_helpers.chain_wrapper import LLMPredictor
+from llama_index.llm_predictor import LLMPredictor
 from llama_index.langchain_helpers.memory_wrapper import GPTIndexMemory
 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase
 
 # prompts
 from llama_index.prompts.base import Prompt
 from llama_index.prompts.prompts import (
     KeywordExtractPrompt,
@@ -64,29 +85,30 @@
     SummaryPrompt,
     TreeInsertPrompt,
     TreeSelectMultiplePrompt,
     TreeSelectPrompt,
 )
 
 # readers
+from llama_index.schema import Document
 from llama_index.readers import (
     BeautifulSoupWebReader,
     ChromaReader,
     DeepLakeReader,
     DiscordReader,
-    Document,
     FaissReader,
     GithubRepositoryReader,
     GoogleDocsReader,
     JSONReader,
     MboxReader,
     MilvusReader,
     NotionPageReader,
     ObsidianReader,
     PineconeReader,
+    PsychicReader,
     QdrantReader,
     RssReader,
     SimpleDirectoryReader,
     SimpleMongoReader,
     SimpleWebPageReader,
     SlackReader,
     StringIterableReader,
@@ -100,34 +122,53 @@
 # response
 from llama_index.response.schema import Response
 
 # storage
 from llama_index.storage.storage_context import StorageContext
 
 # token predictor
-from llama_index.token_counter.mock_chain_wrapper import MockLLMPredictor
+from llama_index.llm_predictor.mock import MockLLMPredictor
 from llama_index.token_counter.mock_embed_model import MockEmbedding
 
+# vellum
+from llama_index.llm_predictor.vellum import VellumPredictor, VellumPromptRegistry
+
 # best practices for library logging:
 # https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library
 logging.getLogger(__name__).addHandler(NullHandler())
 
 
 __all__ = [
     "StorageContext",
     "ServiceContext",
     "ComposableGraph",
+    # indices
+    "VectorStoreIndex",
+    "ListIndex",
+    "SimpleKeywordTableIndex",
+    "KeywordTableIndex",
+    "RAKEKeywordTableIndex",
+    "TreeIndex",
+    "SQLStructStoreIndex",
+    "PandasIndex",
+    "EmptyIndex",
+    "DocumentSummaryIndex",
+    "KnowledgeGraphIndex",
+    # indices - legacy names
     "GPTKeywordTableIndex",
+    "GPTKnowledgeGraphIndex",
     "GPTSimpleKeywordTableIndex",
     "GPTRAKEKeywordTableIndex",
     "GPTListIndex",
     "GPTEmptyIndex",
     "GPTTreeIndex",
     "GPTVectorStoreIndex",
+    "GPTPandasIndex",
     "GPTSQLStructStoreIndex",
+    "GPTDocumentSummaryIndex",
     "Prompt",
     "LangchainEmbedding",
     "OpenAIEmbedding",
     "SummaryPrompt",
     "TreeInsertPrompt",
     "TreeSelectPrompt",
     "TreeSelectMultiplePrompt",
@@ -148,35 +189,42 @@
     "SlackReader",
     "StringIterableReader",
     "WeaviateReader",
     "FaissReader",
     "ChromaReader",
     "DeepLakeReader",
     "PineconeReader",
+    "PsychicReader",
     "QdrantReader",
     "MilvusReader",
     "DiscordReader",
     "SimpleWebPageReader",
     "RssReader",
     "BeautifulSoupWebReader",
     "TrafilaturaWebReader",
     "LLMPredictor",
     "MockLLMPredictor",
+    "VellumPredictor",
+    "VellumPromptRegistry",
     "MockEmbedding",
     "SQLDatabase",
     "GPTIndexMemory",
     "SQLDocumentContextBuilder",
     "SQLContextBuilder",
     "PromptHelper",
     "IndexStructType",
     "TwitterTweetReader",
     "download_loader",
     "GithubRepositoryReader",
     "load_graph_from_storage",
     "load_index_from_storage",
     "load_indices_from_storage",
     "QueryBundle",
-    "ResponseSynthesizer",
+    "get_response_synthesizer",
+    "set_global_service_context",
 ]
 
 # NOTE: keep for backwards compatibility
 SQLContextBuilder = SQLDocumentContextBuilder
+
+# global service context for ServiceContext.from_defaults()
+global_service_context: Optional[ServiceContext] = None
```

### Comparing `llama_index-0.6.9/llama_index/callbacks/aim.py` & `llama_index-0.7.0/llama_index/callbacks/aim.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 
 try:
     from aim import Run, Text
 except ModuleNotFoundError:
     Run, Text = None, None
 
 from llama_index.callbacks.base import BaseCallbackHandler
-from llama_index.callbacks.schema import CBEventType
+from llama_index.callbacks.schema import CBEventType, EventPayload
 
 logger = logging.getLogger(__name__)
 logger.setLevel(logging.WARNING)
 
 
 class AimCallback(BaseCallbackHandler):
     """
@@ -105,30 +105,30 @@
             event_id (str): event id to store.
         """
         if not self._run:
             raise ValueError("AimCallback failed to init properly.")
 
         if event_type is CBEventType.LLM and payload:
             self._run.track(
-                Text(payload["formatted_prompt"]),
+                Text(payload[EventPayload.PROMPT]),
                 name="prompt",
                 step=self._llm_response_step,
                 context={"event_id": event_id},
             )
 
             self._run.track(
-                Text(payload["response"]),
+                Text(payload[EventPayload.RESPONSE]),
                 name="response",
                 step=self._llm_response_step,
                 context={"event_id": event_id},
             )
 
             self._llm_response_step += 1
         elif event_type is CBEventType.CHUNKING and payload:
-            for chunk_id, chunk in enumerate(payload["chunks"]):
+            for chunk_id, chunk in enumerate(payload[EventPayload.CHUNKS]):
                 self._run.track(
                     Text(chunk),
                     name="chunk",
                     step=self._llm_response_step,
                     context={"chunk_id": chunk_id, "event_id": event_id},
                 )
 
@@ -165,7 +165,17 @@
                     self._run.set(key, args[key], strict=False)
             except Exception as e:
                 logger.warning(f"Aim could not log config parameters -> {e}")
 
     def __del__(self) -> None:
         if self._run and self._run.active:
             self._run.close()
+
+    def start_trace(self, trace_id: Optional[str] = None) -> None:
+        pass
+
+    def end_trace(
+        self,
+        trace_id: Optional[str] = None,
+        trace_map: Optional[Dict[str, List[str]]] = None,
+    ) -> None:
+        pass
```

### Comparing `llama_index-0.6.9/llama_index/callbacks/llama_debug.py` & `llama_index-0.7.0/llama_index/callbacks/base.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,153 +1,164 @@
+import uuid
+from abc import ABC, abstractmethod
 from collections import defaultdict
-from datetime import datetime
-from typing import Dict, List, Any, Optional
+from contextlib import contextmanager
+from typing import Any, Dict, List, Optional, Generator
 
-from llama_index.callbacks.base import BaseCallbackHandler
-from llama_index.callbacks.schema import (
-    CBEvent,
-    CBEventType,
-    EventStats,
-    TIMESTAMP_FORMAT,
-)
+from llama_index.callbacks.schema import CBEventType, LEAF_EVENTS, BASE_TRACE_ID
 
 
-class LlamaDebugHandler(BaseCallbackHandler):
-    """Callback handler that keeps track of debug info.
+class BaseCallbackHandler(ABC):
+    """Base callback handler that can be used to track event starts and ends."""
 
-    NOTE: this is a beta feature. The usage within our codebase, and the interface
-    may change.
+    def __init__(
+        self,
+        event_starts_to_ignore: List[CBEventType],
+        event_ends_to_ignore: List[CBEventType],
+    ) -> None:
+        """Initialize the base callback handler."""
+        self.event_starts_to_ignore = tuple(event_starts_to_ignore)
+        self.event_ends_to_ignore = tuple(event_ends_to_ignore)
 
-    This handler simply keeps track of event starts/ends, separated by event types.
-    You can use this callback handler to keep track of and debug events.
+    @abstractmethod
+    def on_event_start(
+        self,
+        event_type: CBEventType,
+        payload: Optional[Dict[str, Any]] = None,
+        event_id: str = "",
+        **kwargs: Any
+    ) -> str:
+        """Run when an event starts and return id of event."""
 
-    Args:
-        event_starts_to_ignore (Optional[List[CBEventType]]): list of event types to
-            ignore when tracking event starts.
-        event_ends_to_ignore (Optional[List[CBEventType]]): list of event types to
-            ignore when tracking event ends.
+    @abstractmethod
+    def on_event_end(
+        self,
+        event_type: CBEventType,
+        payload: Optional[Dict[str, Any]] = None,
+        event_id: str = "",
+        **kwargs: Any
+    ) -> None:
+        """Run when an event ends."""
 
-    """
+    @abstractmethod
+    def start_trace(self, trace_id: Optional[str] = None) -> None:
+        """Run when an overall trace is launched."""
 
-    def __init__(
+    @abstractmethod
+    def end_trace(
         self,
-        event_starts_to_ignore: Optional[List[CBEventType]] = None,
-        event_ends_to_ignore: Optional[List[CBEventType]] = None,
+        trace_id: Optional[str] = None,
+        trace_map: Optional[Dict[str, List[str]]] = None,
     ) -> None:
-        """Initialize the llama debug handler."""
-        self._events: Dict[CBEventType, List[CBEvent]] = defaultdict(list)
-        self._sequential_events: List[CBEvent] = []
-        event_starts_to_ignore = (
-            event_starts_to_ignore if event_starts_to_ignore else []
-        )
-        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []
-        super().__init__(
-            event_starts_to_ignore=event_starts_to_ignore,
-            event_ends_to_ignore=event_ends_to_ignore,
-        )
+        """Run when an overall trace is exited."""
+
+
+class CallbackManager(BaseCallbackHandler, ABC):
+    """Callback manager that handles callbacks for events within LlamaIndex.
+
+    The callback manager provides a way to call handlers on event starts/ends.
+
+    Additionally, the callback manager traces the current stack of events.
+    It does this by using a few key attributes.
+    - trace_stack - The current stack of events that have not ended yet.
+                    When an event ends, it's remove from the stack.
+    - trace_map - A mapping of event ids to their children events.
+                  On the start of events, the bottom of the trace stack
+                  is used as the current parent event for the trace map.
+    - trace_id - A simple name for the current trace, usually denoting the
+                 entrypoint (query, index_construction, insert, etc.)
+
+    Args:
+        handlers (List[BaseCallbackHandler]): list of handlers to use.
+
+    """
+
+    def __init__(self, handlers: List[BaseCallbackHandler]):
+        """Initialize the manager with a list of handlers."""
+        self.handlers = handlers
+        self._trace_map: Dict[str, List[str]] = defaultdict(list)
+        self._trace_stack: List[str] = [BASE_TRACE_ID]
+        self._trace_id: Optional[str] = None
 
     def on_event_start(
         self,
         event_type: CBEventType,
         payload: Optional[Dict[str, Any]] = None,
         event_id: str = "",
         **kwargs: Any
     ) -> str:
-        """Store event start data by event type.
+        """Run handlers when an event starts and return id of event."""
+        event_id = event_id or str(uuid.uuid4())
+        self._trace_map[self._trace_stack[-1]].append(event_id)
+        for handler in self.handlers:
+            if event_type not in handler.event_starts_to_ignore:
+                handler.on_event_start(event_type, payload, event_id=event_id, **kwargs)
+
+        if event_type not in LEAF_EVENTS:
+            self._trace_stack.append(event_id)
 
-        Args:
-            event_type (CBEventType): event type to store.
-            payload (Optional[Dict[str, Any]]): payload to store.
-            event_id (str): event id to store.
-
-        """
-        event = CBEvent(event_type, payload=payload, id_=event_id)
-        self._events[event.event_type].append(event)
-        self._sequential_events.append(event)
-        return event.id_
+        return event_id
 
     def on_event_end(
         self,
         event_type: CBEventType,
         payload: Optional[Dict[str, Any]] = None,
         event_id: str = "",
         **kwargs: Any
     ) -> None:
-        """Store event end data by event type.
+        """Run handlers when an event ends."""
+        event_id = event_id or str(uuid.uuid4())
+        for handler in self.handlers:
+            if event_type not in handler.event_ends_to_ignore:
+                handler.on_event_end(event_type, payload, event_id=event_id, **kwargs)
+
+        if event_type not in LEAF_EVENTS:
+            self._trace_stack.pop()
+
+    def add_handler(self, handler: BaseCallbackHandler) -> None:
+        """Add a handler to the callback manager."""
+        self.handlers.append(handler)
+
+    def remove_handler(self, handler: BaseCallbackHandler) -> None:
+        """Remove a handler from the callback manager."""
+        self.handlers.remove(handler)
+
+    def set_handlers(self, handlers: List[BaseCallbackHandler]) -> None:
+        """Set handlers as the only handlers on the callback manager."""
+        self.handlers = handlers
+
+    @contextmanager
+    def as_trace(self, trace_id: str) -> Generator[None, None, None]:
+        """Context manager tracer for lanching and shutdown of traces."""
+        self.start_trace(trace_id=trace_id)
+        yield
+        self.end_trace(trace_id=trace_id)
+
+    def start_trace(self, trace_id: Optional[str] = None) -> None:
+        """Run when an overall trace is launched."""
+        if not self._trace_id:
+            self._reset_trace_events()
 
-        Args:
-            event_type (CBEventType): event type to store.
-            payload (Optional[Dict[str, Any]]): payload to store.
-            event_id (str): event id to store.
-
-        """
-        event = CBEvent(event_type, payload=payload, id_=event_id)
-        self._events[event.event_type].append(event)
-        self._sequential_events.append(event)
-
-    def get_events(self, event_type: Optional[CBEventType] = None) -> List[CBEvent]:
-        """Get all events for a specific event type."""
-        if event_type is not None:
-            return self._events[event_type]
-
-        return self._sequential_events
-
-    def _get_event_pairs(self, events: List[CBEvent]) -> List[List[CBEvent]]:
-        """Helper function to pair events according to their ID."""
-        event_pairs: Dict[str, List[CBEvent]] = defaultdict(list)
-        for event in events:
-            event_pairs[event.id_].append(event)
-
-        sorted_events = sorted(
-            event_pairs.values(),
-            key=lambda x: datetime.strptime(x[0].time, TIMESTAMP_FORMAT),
-        )
-        return sorted_events
-
-    def _get_time_stats_from_event_pairs(
-        self, event_pairs: List[List[CBEvent]]
-    ) -> EventStats:
-        """Calculate time-based stats for a set of event pairs"""
-        total_secs = 0.0
-        for event_pair in event_pairs:
-            start_time = datetime.strptime(event_pair[0].time, TIMESTAMP_FORMAT)
-            end_time = datetime.strptime(event_pair[-1].time, TIMESTAMP_FORMAT)
-            total_secs += (end_time - start_time).total_seconds()
-
-        stats = EventStats(
-            total_secs=total_secs,
-            average_secs=total_secs / len(event_pairs),
-            total_count=len(event_pairs),
-        )
-        return stats
-
-    def get_event_pairs(
-        self, event_type: Optional[CBEventType] = None
-    ) -> List[List[CBEvent]]:
-        """Pair events by ID, either all events or a sepcific type."""
-        if event_type is not None:
-            return self._get_event_pairs(self._events[event_type])
-
-        return self._get_event_pairs(self._sequential_events)
-
-    def get_llm_inputs_outputs(self) -> List[List[CBEvent]]:
-        """Get the exact LLM inputs and outputs."""
-        return self._get_event_pairs(self.events[CBEventType.LLM])
-
-    def get_event_time_info(
-        self, event_type: Optional[CBEventType] = None
-    ) -> EventStats:
-        event_pairs = self.get_event_pairs(event_type)
-        return self._get_time_stats_from_event_pairs(event_pairs)
-
-    def flush_event_logs(self) -> None:
-        """Clear all events from memory."""
-        self._events = defaultdict(list)
-        self._sequential_events = []
+            for handler in self.handlers:
+                handler.start_trace(trace_id=trace_id)
 
-    @property
-    def events(self) -> Dict[CBEventType, List[CBEvent]]:
-        return self._events
+            self._trace_id = trace_id
+
+    def end_trace(
+        self,
+        trace_id: Optional[str] = None,
+        trace_map: Optional[Dict[str, List[str]]] = None,
+    ) -> None:
+        """Run when an overall trace is exited."""
+        if trace_id is not None and trace_id == self._trace_id:
+            for handler in self.handlers:
+                handler.end_trace(trace_id=trace_id, trace_map=self._trace_map)
+            self._trace_id = None
+
+    def _reset_trace_events(self) -> None:
+        """Helper function to reset the current trace."""
+        self._trace_map = defaultdict(list)
+        self._trace_stack = [BASE_TRACE_ID]
 
     @property
-    def sequential_events(self) -> List[CBEvent]:
-        return self._sequential_events
+    def trace_map(self) -> Dict[str, List[str]]:
+        return self._trace_map
```

### Comparing `llama_index-0.6.9/llama_index/composability/joint_qa_summary.py` & `llama_index-0.7.0/llama_index/composability/joint_qa_summary.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,19 @@
 """Joint QA Summary graph."""
 
 
-from typing import Sequence, Optional
+from typing import Optional, Sequence
 
+from llama_index.indices.list.base import ListIndex
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.list.base import GPTListIndex
-from llama_index.indices.vector_store import GPTVectorStoreIndex
-from llama_index.readers.schema.base import Document
+from llama_index.indices.vector_store import VectorStoreIndex
+from llama_index.query_engine.router_query_engine import RouterQueryEngine
+from llama_index.schema import Document
 from llama_index.selectors.llm_selectors import LLMSingleSelector
 from llama_index.storage.storage_context import StorageContext
-
-from llama_index.query_engine.router_query_engine import RouterQueryEngine
 from llama_index.tools.query_engine import QueryEngineTool
 
 DEFAULT_SUMMARY_TEXT = "Use this index for summarization queries"
 DEFAULT_QA_TEXT = (
     "Use this index for queries that require retrieval of specific "
     "context from documents."
 )
@@ -60,20 +59,20 @@
         # parse nodes
         nodes = self._service_context.node_parser.get_nodes_from_documents(documents)
 
         # ingest nodes
         self._storage_context.docstore.add_documents(nodes, allow_update=True)
 
         # build indices
-        vector_index = GPTVectorStoreIndex(
+        vector_index = VectorStoreIndex(
             nodes,
             service_context=self._service_context,
             storage_context=self._storage_context,
         )
-        list_index = GPTListIndex(
+        list_index = ListIndex(
             nodes,
             service_context=self._service_context,
             storage_context=self._storage_context,
         )
 
         vector_query_engine = vector_index.as_query_engine(
             service_context=self._service_context
```

### Comparing `llama_index-0.6.9/llama_index/data_structs/data_structs.py` & `llama_index-0.7.0/llama_index/data_structs/data_structs.py`

 * *Files 24% similar despite different names*

```diff
@@ -3,22 +3,26 @@
 Nodes are decoupled from the indices.
 
 """
 
 import uuid
 from abc import abstractmethod
 from dataclasses import dataclass, field
-from typing import Dict, List, Optional, Sequence, Set, Tuple
+from typing import Dict, List, Optional, Sequence, Set
 
 from dataclasses_json import DataClassJsonMixin
 
-from llama_index.data_structs.node import Node
+from llama_index.schema import BaseNode, TextNode
 from llama_index.data_structs.struct_type import IndexStructType
 
 
+# TODO: legacy backport of old Node class
+Node = TextNode
+
+
 @dataclass
 class IndexStruct(DataClassJsonMixin):
     """A base data struct for a LlamaIndex."""
 
     index_id: str = field(default_factory=lambda: str(uuid.uuid4()))
     summary: Optional[str] = None
 
@@ -49,81 +53,82 @@
         return {node_id: index for index, node_id in self.all_nodes.items()}
 
     @property
     def size(self) -> int:
         """Get the size of the graph."""
         return len(self.all_nodes)
 
-    def get_index(self, node: Node) -> int:
+    def get_index(self, node: BaseNode) -> int:
         """Get index of node."""
-        return self.node_id_to_index[node.get_doc_id()]
+        return self.node_id_to_index[node.node_id]
 
     def insert(
         self,
-        node: Node,
+        node: BaseNode,
         index: Optional[int] = None,
-        children_nodes: Optional[Sequence[Node]] = None,
+        children_nodes: Optional[Sequence[BaseNode]] = None,
     ) -> None:
         """Insert node."""
         index = index or self.size
-        node_id = node.get_doc_id()
+        node_id = node.node_id
 
         self.all_nodes[index] = node_id
 
         if children_nodes is None:
             children_nodes = []
-        children_ids = [n.get_doc_id() for n in children_nodes]
+        children_ids = [n.node_id for n in children_nodes]
         self.node_id_to_children_ids[node_id] = children_ids
 
-    def get_children(self, parent_node: Optional[Node]) -> Dict[int, str]:
+    def get_children(self, parent_node: Optional[BaseNode]) -> Dict[int, str]:
         """Get children nodes."""
         if parent_node is None:
             return self.root_nodes
         else:
-            parent_id = parent_node.get_doc_id()
+            parent_id = parent_node.node_id
             children_ids = self.node_id_to_children_ids[parent_id]
             return {
                 self.node_id_to_index[child_id]: child_id for child_id in children_ids
             }
 
     def insert_under_parent(
-        self, node: Node, parent_node: Optional[Node], new_index: Optional[int] = None
+        self,
+        node: BaseNode,
+        parent_node: Optional[BaseNode],
+        new_index: Optional[int] = None,
     ) -> None:
         """Insert under parent node."""
         new_index = new_index or self.size
         if parent_node is None:
-            self.root_nodes[new_index] = node.get_doc_id()
-            self.node_id_to_children_ids[node.get_doc_id()] = []
+            self.root_nodes[new_index] = node.node_id
+            self.node_id_to_children_ids[node.node_id] = []
         else:
-            if parent_node.doc_id not in self.node_id_to_children_ids:
-                self.node_id_to_children_ids[parent_node.get_doc_id()] = []
-            self.node_id_to_children_ids[parent_node.get_doc_id()].append(
-                node.get_doc_id()
-            )
+            if parent_node.node_id not in self.node_id_to_children_ids:
+                self.node_id_to_children_ids[parent_node.node_id] = []
+            self.node_id_to_children_ids[parent_node.node_id].append(node.node_id)
 
-        self.all_nodes[new_index] = node.get_doc_id()
+        self.all_nodes[new_index] = node.node_id
 
     @classmethod
     def get_type(cls) -> IndexStructType:
         """Get type."""
         return IndexStructType.TREE
 
 
 @dataclass
 class KeywordTable(IndexStruct):
     """A table of keywords mapping keywords to text chunks."""
 
     table: Dict[str, Set[str]] = field(default_factory=dict)
 
-    def add_node(self, keywords: List[str], node: Node) -> None:
+    def add_node(self, keywords: List[str], node: BaseNode) -> None:
         """Add text to table."""
         for keyword in keywords:
             if keyword not in self.table:
                 self.table[keyword] = set()
-            self.table[keyword].add(node.get_doc_id())
+            self.table[keyword].add(node.node_id)
 
     @property
     def node_ids(self) -> Set[str]:
         """Get all node ids."""
         return set.union(*self.table.values())
 
     @property
@@ -144,147 +149,110 @@
 
 @dataclass
 class IndexList(IndexStruct):
     """A list of documents."""
 
     nodes: List[str] = field(default_factory=list)
 
-    def add_node(self, node: Node) -> None:
+    def add_node(self, node: BaseNode) -> None:
         """Add text to table, return current position in list."""
         # don't worry about child indices for now, nodes are all in order
-        self.nodes.append(node.get_doc_id())
+        self.nodes.append(node.node_id)
 
     @classmethod
     def get_type(cls) -> IndexStructType:
         """Get type."""
         return IndexStructType.LIST
 
 
 @dataclass
 class IndexDict(IndexStruct):
     """A simple dictionary of documents."""
 
-    # mapping from vector store id to node id
+    # TODO: slightly deprecated, should likely be a list or set now
+    # mapping from vector store id to node doc_id
     nodes_dict: Dict[str, str] = field(default_factory=dict)
-    # mapping from doc_id to vector store id
+
+    # TODO: deprecated, not used
+    # mapping from node doc_id to vector store id
     doc_id_dict: Dict[str, List[str]] = field(default_factory=dict)
 
-    # TODO: temporary hack to store embeddings for simple vector index
+    # TODO: deprecated, not used
     # this should be empty for all other indices
     embeddings_dict: Dict[str, List[float]] = field(default_factory=dict)
 
     def add_node(
         self,
-        node: Node,
+        node: BaseNode,
         text_id: Optional[str] = None,
     ) -> str:
         """Add text to table, return current position in list."""
         # # don't worry about child indices for now, nodes are all in order
         # self.nodes_dict[int_id] = node
-        vector_id = text_id if text_id is not None else node.get_doc_id()
-        self.nodes_dict[vector_id] = node.get_doc_id()
-        if node.ref_doc_id is not None:
-            if node.ref_doc_id not in self.doc_id_dict:
-                self.doc_id_dict[node.ref_doc_id] = []
-            self.doc_id_dict[node.ref_doc_id].append(vector_id)
+        vector_id = text_id if text_id is not None else node.node_id
+        self.nodes_dict[vector_id] = node.node_id
 
         return vector_id
 
     def delete(self, doc_id: str) -> None:
         """Delete a Node."""
-        if doc_id not in self.doc_id_dict:
-            return
-        for vector_id in self.doc_id_dict[doc_id]:
-            del self.nodes_dict[vector_id]
-        del self.doc_id_dict[doc_id]
+        del self.nodes_dict[doc_id]
 
     @classmethod
     def get_type(cls) -> IndexStructType:
         """Get type."""
         return IndexStructType.VECTOR_STORE
 
 
 @dataclass
 class KG(IndexStruct):
     """A table of keywords mapping keywords to text chunks."""
 
     # Unidirectional
 
+    # table of keywords to node ids
     table: Dict[str, Set[str]] = field(default_factory=dict)
-    # text_chunks: Dict[str, Node] = field(default_factory=dict)
-    rel_map: Dict[str, List[Tuple[str, str]]] = field(default_factory=dict)
+
+    # TODO: legacy attribute, remove in future releases
+    rel_map: Dict[str, List[List[str]]] = field(default_factory=dict)
+
+    # TBD, should support vector store, now we just persist the embedding memory
+    # maybe chainable abstractions for *_stores could be designed
     embedding_dict: Dict[str, List[float]] = field(default_factory=dict)
 
     @property
     def node_ids(self) -> Set[str]:
         """Get all node ids."""
         return set.union(*self.table.values())
 
     def add_to_embedding_dict(self, triplet_str: str, embedding: List[float]) -> None:
         """Add embedding to dict."""
         self.embedding_dict[triplet_str] = embedding
 
-    def upsert_triplet(self, triplet: Tuple[str, str, str]) -> None:
-        """Upsert a knowledge triplet to the graph."""
-        subj, relationship, obj = triplet
-        if subj not in self.rel_map:
-            self.rel_map[subj] = []
-        self.rel_map[subj].append((obj, relationship))
-
-    def add_node(self, keywords: List[str], node: Node) -> None:
+    def add_node(self, keywords: List[str], node: BaseNode) -> None:
         """Add text to table."""
-        node_id = node.get_doc_id()
+        node_id = node.node_id
         for keyword in keywords:
             if keyword not in self.table:
                 self.table[keyword] = set()
             self.table[keyword].add(node_id)
-        # self.text_chunks[node_id] = node
-
-    def get_rel_map_texts(self, keyword: str) -> List[str]:
-        """Get the corresponding knowledge for a given keyword."""
-        # NOTE: return a single node for now
-        if keyword not in self.rel_map:
-            return []
-        texts = []
-        for obj, rel in self.rel_map[keyword]:
-            texts.append(str((keyword, rel, obj)))
-        return texts
-
-    def get_rel_map_tuples(self, keyword: str) -> List[Tuple[str, str]]:
-        """Get the corresponding knowledge for a given keyword."""
-        # NOTE: return a single node for now
-        if keyword not in self.rel_map:
-            return []
-        return self.rel_map[keyword]
 
-    def get_node_ids(self, keyword: str, depth: int = 1) -> List[str]:
-        """Get the corresponding knowledge for a given keyword."""
-        if depth > 1:
-            raise ValueError("Depth > 1 not supported yet.")
+    def search_node_by_keyword(self, keyword: str) -> List[str]:
+        """Search for nodes by keyword."""
         if keyword not in self.table:
             return []
-        keywords = [keyword]
-        # some keywords may correspond to a leaf node, may not be in rel_map
-        if keyword in self.rel_map:
-            keywords.extend([child for child, _ in self.rel_map[keyword]])
-
-        node_ids: List[str] = []
-        for keyword in keywords:
-            for node_id in self.table.get(keyword, set()):
-                node_ids.append(node_id)
-            # TODO: Traverse (with depth > 1)
-        return node_ids
+        return list(self.table[keyword])
 
     @classmethod
     def get_type(cls) -> IndexStructType:
         """Get type."""
         return IndexStructType.KG
 
 
 @dataclass
-class EmptyIndex(IndexDict):
+class EmptyIndexStruct(IndexStruct):
     """Empty index."""
 
     @classmethod
     def get_type(cls) -> IndexStructType:
         """Get type."""
         return IndexStructType.EMPTY
```

### Comparing `llama_index-0.6.9/llama_index/data_structs/document_summary.py` & `llama_index-0.7.0/llama_index/data_structs/document_summary.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 """Data struct for document summary index."""
 
-from llama_index.data_structs.node import Node
+from llama_index.schema import BaseNode
 from llama_index.data_structs.data_structs import IndexStruct
 from dataclasses import dataclass, field
 from typing import List, Dict
 from llama_index.data_structs.struct_type import IndexStructType
 
 
 @dataclass
@@ -19,29 +19,29 @@
     node_id_to_summary_id: Dict[str, str] = field(default_factory=dict)
 
     # track mapping from doc id to node summary id
     doc_id_to_summary_id: Dict[str, str] = field(default_factory=dict)
 
     def add_summary_and_nodes(
         self,
-        summary_node: Node,
-        nodes: List[Node],
+        summary_node: BaseNode,
+        nodes: List[BaseNode],
     ) -> str:
         """Add node and summary."""
-        summary_id = summary_node.get_doc_id()
+        summary_id = summary_node.node_id
         ref_doc_id = summary_node.ref_doc_id
         if ref_doc_id is None:
             raise ValueError(
                 "ref_doc_id of node cannot be None when building a document "
                 "summary index"
             )
         self.doc_id_to_summary_id[ref_doc_id] = summary_id
 
         for node in nodes:
-            node_id = node.get_doc_id()
+            node_id = node.node_id
             if summary_id not in self.summary_id_to_node_ids:
                 self.summary_id_to_node_ids[summary_id] = []
             self.summary_id_to_node_ids[summary_id].append(node_id)
 
             self.node_id_to_summary_id[node_id] = summary_id
 
         return summary_id
```

### Comparing `llama_index-0.6.9/llama_index/data_structs/registry.py` & `llama_index-0.7.0/llama_index/data_structs/registry.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """Index registry."""
 
 from typing import Dict, Type
 
 from llama_index.data_structs.data_structs import (
     KG,
-    EmptyIndex,
+    EmptyIndexStruct,
     IndexDict,
     IndexGraph,
     IndexList,
     KeywordTable,
     IndexStruct,
 )
 from llama_index.data_structs.struct_type import IndexStructType
@@ -20,10 +20,10 @@
     IndexStructType.TREE: IndexGraph,
     IndexStructType.LIST: IndexList,
     IndexStructType.KEYWORD_TABLE: KeywordTable,
     IndexStructType.VECTOR_STORE: IndexDict,
     IndexStructType.SQL: SQLStructTable,
     IndexStructType.PANDAS: PandasStructTable,
     IndexStructType.KG: KG,
-    IndexStructType.EMPTY: EmptyIndex,
+    IndexStructType.EMPTY: EmptyIndexStruct,
     IndexStructType.DOCUMENT_SUMMARY: IndexDocumentSummary,
 }
```

### Comparing `llama_index-0.6.9/llama_index/data_structs/struct_type.py` & `llama_index-0.7.0/llama_index/data_structs/struct_type.py`

 * *Files 3% similar despite different names*

```diff
@@ -79,14 +79,16 @@
     OPENSEARCH = "opensearch"
     CHATGPT_RETRIEVAL_PLUGIN = "chatgpt_retrieval_plugin"
     DEEPLAKE = "deeplake"
     # for SQL index
     SQL = "sql"
     # for KG index
     KG = "kg"
+    SIMPLE_KG = "simple_kg"
+    NEBULAGRAPH = "nebulagraph"
 
     # EMPTY
     EMPTY = "empty"
     COMPOSITE = "composite"
 
     PANDAS = "pandas"
```

### Comparing `llama_index-0.6.9/llama_index/data_structs/table.py` & `llama_index-0.7.0/llama_index/data_structs/table.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/embeddings/base.py` & `llama_index-0.7.0/llama_index/embeddings/base.py`

 * *Files 10% similar despite different names*

```diff
@@ -4,16 +4,16 @@
 from abc import abstractmethod
 from enum import Enum
 from typing import Callable, Coroutine, List, Optional, Tuple
 
 import numpy as np
 
 from llama_index.callbacks.base import CallbackManager
-from llama_index.callbacks.schema import CBEventType
-from llama_index.utils import globals_helper
+from llama_index.callbacks.schema import CBEventType, EventPayload
+from llama_index.utils import globals_helper, get_tqdm_iterable
 
 # TODO: change to numpy array
 EMB_TYPE = List
 
 DEFAULT_EMBED_BATCH_SIZE = 10
 
 
@@ -33,15 +33,16 @@
 def similarity(
     embedding1: EMB_TYPE,
     embedding2: EMB_TYPE,
     mode: SimilarityMode = SimilarityMode.DEFAULT,
 ) -> float:
     """Get embedding similarity."""
     if mode == SimilarityMode.EUCLIDEAN:
-        return float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))
+        # Using -euclidean distance as similarity to achieve same ranking order
+        return -float(np.linalg.norm(np.array(embedding1) - np.array(embedding2)))
     elif mode == SimilarityMode.DOT_PRODUCT:
         product = np.dot(embedding1, embedding2)
         return product
     else:
         product = np.dot(embedding1, embedding2)
         norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)
         return product / norm
@@ -74,15 +75,17 @@
     def get_query_embedding(self, query: str) -> List[float]:
         """Get query embedding."""
         event_id = self.callback_manager.on_event_start(CBEventType.EMBEDDING)
         query_embedding = self._get_query_embedding(query)
         query_tokens_count = len(self._tokenizer(query))
         self._total_tokens_used += query_tokens_count
         self.callback_manager.on_event_end(
-            CBEventType.EMBEDDING, payload={"num_nodes": 1}, event_id=event_id
+            CBEventType.EMBEDDING,
+            payload={EventPayload.CHUNKS: [query]},
+            event_id=event_id,
         )
         return query_embedding
 
     def get_agg_embedding_from_queries(
         self,
         queries: List[str],
         agg_fn: Optional[Callable[..., List[float]]] = None,
@@ -130,56 +133,70 @@
     def get_text_embedding(self, text: str) -> List[float]:
         """Get text embedding."""
         event_id = self.callback_manager.on_event_start(CBEventType.EMBEDDING)
         text_embedding = self._get_text_embedding(text)
         text_tokens_count = len(self._tokenizer(text))
         self._total_tokens_used += text_tokens_count
         self.callback_manager.on_event_end(
-            CBEventType.EMBEDDING, payload={"num_nodes": 1}, event_id=event_id
+            CBEventType.EMBEDDING,
+            payload={EventPayload.CHUNKS: [text]},
+            event_id=event_id,
         )
         return text_embedding
 
     def queue_text_for_embedding(self, text_id: str, text: str) -> None:
         """Queue text for embedding.
 
         Used for batching texts during embedding calls.
 
         """
         self._text_queue.append((text_id, text))
 
-    def get_queued_text_embeddings(self) -> Tuple[List[str], List[List[float]]]:
+    def get_queued_text_embeddings(
+        self, show_progress: bool = False
+    ) -> Tuple[List[str], List[List[float]]]:
         """Get queued text embeddings.
 
         Call embedding API to get embeddings for all queued texts.
 
         """
         text_queue = self._text_queue
         cur_batch: List[Tuple[str, str]] = []
         result_ids: List[str] = []
         result_embeddings: List[List[float]] = []
-        for idx, (text_id, text) in enumerate(text_queue):
+
+        queue_with_progress = enumerate(
+            get_tqdm_iterable(text_queue, show_progress, "Generating embeddings")
+        )
+
+        for idx, (text_id, text) in queue_with_progress:
             cur_batch.append((text_id, text))
             text_tokens_count = len(self._tokenizer(text))
             self._total_tokens_used += text_tokens_count
             if idx == len(text_queue) - 1 or len(cur_batch) == self._embed_batch_size:
                 # flush
+                event_id = self.callback_manager.on_event_start(CBEventType.EMBEDDING)
                 cur_batch_ids = [text_id for text_id, _ in cur_batch]
                 cur_batch_texts = [text for _, text in cur_batch]
                 embeddings = self._get_text_embeddings(cur_batch_texts)
                 result_ids.extend(cur_batch_ids)
                 result_embeddings.extend(embeddings)
-
+                self.callback_manager.on_event_end(
+                    CBEventType.EMBEDDING,
+                    payload={EventPayload.CHUNKS: cur_batch_texts},
+                    event_id=event_id,
+                )
                 cur_batch = []
 
         # reset queue
         self._text_queue = []
         return result_ids, result_embeddings
 
     async def aget_queued_text_embeddings(
-        self, text_queue: List[Tuple[str, str]]
+        self, text_queue: List[Tuple[str, str]], show_progress: bool = False
     ) -> Tuple[List[str], List[List[float]]]:
         """Asynchronously get a list of text embeddings.
 
         Call async embedding API to get embeddings for all queued texts in parallel.
         Argument `text_queue` must be passed in to avoid updating it async.
 
         """
@@ -189,26 +206,49 @@
         embeddings_coroutines: List[Coroutine] = []
         for idx, (text_id, text) in enumerate(text_queue):
             cur_batch.append((text_id, text))
             text_tokens_count = len(self._tokenizer(text))
             self._total_tokens_used += text_tokens_count
             if idx == len(text_queue) - 1 or len(cur_batch) == self._embed_batch_size:
                 # flush
+                event_id = self.callback_manager.on_event_start(CBEventType.EMBEDDING)
                 cur_batch_ids = [text_id for text_id, _ in cur_batch]
                 cur_batch_texts = [text for _, text in cur_batch]
                 embeddings_coroutines.append(
                     self._aget_text_embeddings(cur_batch_texts)
                 )
                 result_ids.extend(cur_batch_ids)
+                self.callback_manager.on_event_end(
+                    CBEventType.EMBEDDING,
+                    payload={EventPayload.CHUNKS: cur_batch_texts},
+                    event_id=event_id,
+                )
 
         # flatten the results of asyncio.gather, which is a list of embeddings lists
+        nested_embeddings = []
+        if show_progress:
+            try:
+                from tqdm.auto import tqdm
+
+                nested_embeddings = [
+                    await f
+                    for f in tqdm(
+                        asyncio.as_completed(embeddings_coroutines),
+                        total=len(embeddings_coroutines),
+                        desc="Generating embeddings",
+                    )
+                ]
+            except ImportError:
+                nested_embeddings = await asyncio.gather(*embeddings_coroutines)
+                pass
+        else:
+            nested_embeddings = await asyncio.gather(*embeddings_coroutines)
+
         result_embeddings = [
-            embedding
-            for embeddings in await asyncio.gather(*embeddings_coroutines)
-            for embedding in embeddings
+            embedding for embeddings in nested_embeddings for embedding in embeddings
         ]
 
         return result_ids, result_embeddings
 
     def similarity(
         self,
         embedding1: EMB_TYPE,
```

### Comparing `llama_index-0.6.9/llama_index/embeddings/google.py` & `llama_index-0.7.0/llama_index/embeddings/google.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/embeddings/langchain.py` & `llama_index-0.7.0/llama_index/embeddings/langchain.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Langchain Embedding Wrapper Module."""
 
 
 from typing import Any, List
 
-from langchain.embeddings.base import Embeddings as LCEmbeddings
+from llama_index.bridge.langchain import Embeddings as LCEmbeddings
 
 from llama_index.embeddings.base import BaseEmbedding
 
 
 class LangchainEmbedding(BaseEmbedding):
     """External embeddings (taken from Langchain).
```

### Comparing `llama_index-0.6.9/llama_index/embeddings/openai.py` & `llama_index-0.7.0/llama_index/embeddings/openai.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 """OpenAI embeddings file."""
 
 from enum import Enum
-from typing import Any, Dict, List, Optional, Tuple, Callable
+from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import openai
 from tenacity import retry, stop_after_attempt, wait_random_exponential
 
-from llama_index.embeddings.base import BaseEmbedding, DEFAULT_EMBED_BATCH_SIZE
 from llama_index.callbacks.base import CallbackManager
+from llama_index.embeddings.base import DEFAULT_EMBED_BATCH_SIZE, BaseEmbedding
 
 
 class OpenAIEmbeddingMode(str, Enum):
     """OpenAI embedding mode."""
 
     SIMILARITY_MODE = "similarity"
     TEXT_SEARCH_MODE = "text_search"
```

### Comparing `llama_index-0.6.9/llama_index/embeddings/utils.py` & `llama_index-0.7.0/llama_index/embeddings/utils.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/evaluation/base.py` & `llama_index-0.7.0/llama_index/evaluation/base.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,20 +1,39 @@
 """Evaluating the responses from an index."""
 from __future__ import annotations
 
+from abc import ABC, abstractmethod
+from dataclasses import dataclass
 from typing import List, Optional
 
-from llama_index import (
-    Document,
-    GPTListIndex,
-    QuestionAnswerPrompt,
-    RefinePrompt,
-    Response,
-    ServiceContext,
-)
+from llama_index.indices.base import ServiceContext
+from llama_index.indices.list.base import ListIndex
+from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt
+from llama_index.schema import Document
+from llama_index.response.schema import Response
+
+
+@dataclass
+class Evaluation:
+    query: str  # The query
+    response: Response  # The response
+    passing: bool = False  # True if the response is correct, False otherwise
+    feedback: str = ""  # Feedback for the response
+
+
+class BaseEvaluator(ABC):
+    def __init__(self, service_context: Optional[ServiceContext] = None) -> None:
+        """Base class for evaluating responses"""
+        self.service_context = service_context or ServiceContext.from_defaults()
+
+    @abstractmethod
+    def evaluate_response(self, query: str, response: Response) -> Evaluation:
+        """Evaluate the response for a query and return an Evaluation."""
+        raise NotImplementedError
+
 
 DEFAULT_EVAL_PROMPT = (
     "Please tell if a given piece of information "
     "is supported by the context.\n"
     "You need to answer with either YES or NO.\n"
     "Answer YES if any of the context supports the information, even "
     "if most of the context is unrelated. "
@@ -110,15 +129,15 @@
         Returns:
             List of Documents of source nodes information as context information.
         """
 
         context = []
 
         for context_info in response.source_nodes:
-            context.append(Document(context_info.source_text))
+            context.append(Document(text=context_info.node.get_content()))
 
         return context
 
     def evaluate(self, response: Response) -> str:
         """Evaluate the response from an index.
 
         Args:
@@ -129,17 +148,15 @@
                     or If Query, answer and context information are matching.
             No -> If answer, context information are not matching \
                     or If Query, answer and context information are not matching.
         """
         answer = str(response)
 
         context = self.get_context(response)
-        index = GPTListIndex.from_documents(
-            context, service_context=self.service_context
-        )
+        index = ListIndex.from_documents(context, service_context=self.service_context)
         response_txt = ""
 
         EVAL_PROMPT_TMPL = QuestionAnswerPrompt(DEFAULT_EVAL_PROMPT)
         REFINE_PROMPT_TMPL = RefinePrompt(DEFAULT_REFINE_PROMPT)
 
         query_engine = index.as_query_engine(
             text_qa_template=EVAL_PROMPT_TMPL,
@@ -173,15 +190,15 @@
         answer = str(response)
 
         context_list = self.get_context(response)
 
         response_texts = []
 
         for context in context_list:
-            index = GPTListIndex.from_documents(
+            index = ListIndex.from_documents(
                 [context], service_context=self.service_context
             )
             response_txt = ""
 
             EVAL_PROMPT_TMPL = QuestionAnswerPrompt(DEFAULT_EVAL_PROMPT)
             REFINE_PROMPT_TMPL = RefinePrompt(DEFAULT_REFINE_PROMPT)
 
@@ -200,15 +217,15 @@
                     raise ValueError("The response is invalid")
 
             response_texts.append(response_txt)
 
         return response_texts
 
 
-class QueryResponseEvaluator:
+class QueryResponseEvaluator(BaseEvaluator):
     """Evaluate based on query and response from indices.
 
     NOTE: this is a beta feature, subject to change!
 
     Args:
         service_context (Optional[ServiceContext]): ServiceContext object
 
@@ -216,15 +233,15 @@
 
     def __init__(
         self,
         service_context: Optional[ServiceContext] = None,
         raise_error: bool = False,
     ) -> None:
         """Init params."""
-        self.service_context = service_context or ServiceContext.from_defaults()
+        super().__init__(service_context)
         self.raise_error = raise_error
 
     def get_context(self, response: Response) -> List[Document]:
         """Get context information from given Response object using source nodes.
 
         Args:
             response (Response): Response object from an index based on the query.
@@ -232,15 +249,15 @@
         Returns:
             List of Documents of source nodes information as context information.
         """
 
         context = []
 
         for context_info in response.source_nodes:
-            context.append(Document(context_info.source_text))
+            context.append(Document(text=context_info.node.get_content()))
 
         return context
 
     def evaluate(self, query: str, response: Response) -> str:
         """Evaluate the response from an index.
 
         Args:
@@ -248,21 +265,29 @@
             response: Response object from an index based on the query.
         Returns:
             Yes -> If answer, context information are matching \
                     or If Query, answer and context information are matching.
             No -> If answer, context information are not matching \
                     or If Query, answer and context information are not matching.
         """
+        return self.evaluate_response(query, response).feedback
+
+    def evaluate_response(self, query: str, response: Response) -> Evaluation:
+        """Evaluate the response from an index.
+
+        Args:
+            query: Query for which response is generated from index.
+            response: Response object from an index based on the query.
+        Returns:
+            Evaluation object with passing boolean and feedback "YES" or "NO".
+        """
         answer = str(response)
 
         context = self.get_context(response)
-        index = GPTListIndex.from_documents(
-            context, service_context=self.service_context
-        )
-        response_txt = ""
+        index = ListIndex.from_documents(context, service_context=self.service_context)
 
         QUERY_RESPONSE_EVAL_PROMPT_TMPL = QuestionAnswerPrompt(
             QUERY_RESPONSE_EVAL_PROMPT
         )
         QUERY_RESPONSE_REFINE_PROMPT_TMPL = RefinePrompt(QUERY_RESPONSE_REFINE_PROMPT)
 
         query_response = f"Question: {query}\nResponse: {answer}"
@@ -272,21 +297,19 @@
             refine_template=QUERY_RESPONSE_REFINE_PROMPT_TMPL,
         )
         response_obj = query_engine.query(query_response)
 
         raw_response_txt = str(response_obj)
 
         if "yes" in raw_response_txt.lower():
-            response_txt = "YES"
+            return Evaluation(query, response, True, "YES")
         else:
-            response_txt = "NO"
             if self.raise_error:
                 raise ValueError("The response is invalid")
-
-        return response_txt
+            return Evaluation(query, response, False, "NO")
 
     def evaluate_source_nodes(self, query: str, response: Response) -> List[str]:
         """Function to evaluate if each source node contains the answer \
             to a given query by comparing the query, response, \
                 and context information.
 
         Args:
@@ -305,15 +328,15 @@
         answer = str(response)
 
         context_list = self.get_context(response)
 
         response_texts = []
 
         for context in context_list:
-            index = GPTListIndex.from_documents(
+            index = ListIndex.from_documents(
                 [context], service_context=self.service_context
             )
             response_txt = ""
 
             QUERY_RESPONSE_EVAL_PROMPT_TMPL = QuestionAnswerPrompt(
                 QUERY_RESPONSE_EVAL_PROMPT
             )
```

### Comparing `llama_index-0.6.9/llama_index/evaluation/dataset_generation.py` & `llama_index-0.7.0/llama_index/evaluation/dataset_generation.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,41 +1,36 @@
 """Dataset generation from documents"""
 from __future__ import annotations
 
-from typing import List, Optional
 import re
+from typing import List, Optional
+
 
 from llama_index import (
     Document,
-    GPTListIndex,
+    ListIndex,
     QuestionAnswerPrompt,
     ServiceContext,
-    LLMPredictor,
 )
+from llama_index.llms.openai import OpenAI
+from llama_index.schema import BaseNode, NodeWithScore, MetadataMode
 from llama_index.indices.postprocessor.node import KeywordNodePostprocessor
-from llama_index.data_structs.node import Node, NodeWithScore
-
-from langchain.chat_models import ChatOpenAI
 
 DEFAULT_QUESTION_GENERATION_PROMPT = """Context information is below.\n"
 "\n---------------------\n{context_str}\n---------------------\n"
 "Given the context information and not prior knowledge.\n"
 "generate only questions based on the below query.\n"
 "{query_str}\n"
 """
 
 
 def _get_default_service_context() -> ServiceContext:
     """Get default service context."""
-    llm_predictor = LLMPredictor(
-        llm=ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
-    )
-    service_context = ServiceContext.from_defaults(
-        llm_predictor=llm_predictor, chunk_size_limit=3000
-    )
+    llm = OpenAI(temperature=0, model="gpt-3.5-turbo")
+    service_context = ServiceContext.from_defaults(llm=llm, chunk_size_limit=3000)
     return service_context
 
 
 class DatasetGenerator:
     """Generate dataset (question/ question-answer pairs) \
     based on the given documents.
 
@@ -47,15 +42,15 @@
         num_questions_per_chunk: number of question to be \
         generated per chunk. Each document is chunked of size 512 words.
         text_question_template: Question generation template.
     """
 
     def __init__(
         self,
-        nodes: List[Node],
+        nodes: List[BaseNode],
         service_context: Optional[ServiceContext] = None,
         num_questions_per_chunk: int = 10,
         text_question_template: Optional[QuestionAnswerPrompt] = None,
         question_gen_query: Optional[str] = None,
         required_keywords: Optional[List[str]] = None,
         exclude_keywords: Optional[List[str]] = None,
     ) -> None:
@@ -96,32 +91,43 @@
         required_keywords = required_keywords or []
         exclude_keywords = exclude_keywords or []
         node_postprocessor = KeywordNodePostprocessor(
             service_context=service_context,
             required_keywords=required_keywords,
             exclude_keywords=exclude_keywords,
         )
-        node_with_scores = [NodeWithScore(node) for node in nodes]
+        node_with_scores = [NodeWithScore(node=node) for node in nodes]
         node_with_scores = node_postprocessor.postprocess_nodes(node_with_scores)
         nodes = [node_with_score.node for node_with_score in node_with_scores]
 
         return cls(
             nodes=nodes,
             service_context=service_context,
             num_questions_per_chunk=num_questions_per_chunk,
             text_question_template=text_question_template,
             question_gen_query=question_gen_query,
         )
 
-    def _node_question_generator(self, nodes: List[Node]) -> List[str]:
+    def _node_question_generator(
+        self, nodes: List[BaseNode], num: Optional[int] = None
+    ) -> List[str]:
         """Node question generator."""
-        questions = []
+        questions: List[str] = []
 
         for node in nodes:
-            index = GPTListIndex.from_documents([Document(node.get_text())])
+            if num is not None and len(questions) >= num:
+                break
+            index = ListIndex.from_documents(
+                [
+                    Document(
+                        text=node.get_content(metadata_mode=MetadataMode.NONE),
+                        metadata=node.metadata,
+                    )
+                ]
+            )
 
             query_engine = index.as_query_engine(
                 service_context=self.service_context,
                 text_qa_template=self.text_question_template,
                 use_async=True,
             )
             response = query_engine.query(
@@ -132,12 +138,14 @@
             cleaned_questions = [
                 re.sub(r"^\d+[\).\s]", "", question).strip() for question in result
             ]
             questions.extend(cleaned_questions)
 
         questions = [question for question in questions if question != ""]
 
+        if num is not None:
+            questions = questions[:num]
         return questions
 
-    def generate_questions_from_nodes(self) -> List[str]:
+    def generate_questions_from_nodes(self, num: Optional[int] = None) -> List[str]:
         """Generates questions for each document."""
-        return self._node_question_generator(self.nodes)
+        return self._node_question_generator(self.nodes, num)
```

### Comparing `llama_index-0.6.9/llama_index/img_utils.py` & `llama_index-0.7.0/llama_index/img_utils.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/indices/base_retriever.py` & `llama_index-0.7.0/llama_index/indices/base_retriever.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from abc import ABC, abstractmethod
 from typing import List
-from llama_index.data_structs.node import NodeWithScore
 
+from llama_index.schema import NodeWithScore
 from llama_index.indices.query.schema import QueryBundle, QueryType
 
 
 class BaseRetriever(ABC):
     """Base retriever."""
 
     def retrieve(self, str_or_query_bundle: QueryType) -> List[NodeWithScore]:
```

### Comparing `llama_index-0.6.9/llama_index/indices/common/struct_store/base.py` & `llama_index-0.7.0/llama_index/indices/common/struct_store/base.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,47 +1,47 @@
 """Common classes for structured operations."""
 
 import logging
 from abc import abstractmethod
 from typing import Any, Callable, Dict, List, Optional, Sequence, cast
 
-from llama_index.callbacks.schema import CBEventType
-from llama_index.data_structs.node import Node
+from llama_index.callbacks.schema import CBEventType, EventPayload
 from llama_index.data_structs.table import StructDatapoint
-from llama_index.indices.response import get_response_builder
 from llama_index.indices.service_context import ServiceContext
-from llama_index.langchain_helpers.chain_wrapper import LLMPredictor
 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase
 from llama_index.langchain_helpers.text_splitter import TextSplitter
+from llama_index.llm_predictor.base import BaseLLMPredictor
 from llama_index.prompts.default_prompt_selectors import (
     DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL,
 )
 from llama_index.prompts.default_prompts import (
     DEFAULT_TABLE_CONTEXT_PROMPT,
     DEFAULT_TABLE_CONTEXT_QUERY,
 )
+from llama_index.prompts.prompt_type import PromptType
 from llama_index.prompts.prompts import (
     QuestionAnswerPrompt,
     RefinePrompt,
     RefineTableContextPrompt,
     SchemaExtractPrompt,
     TableContextPrompt,
 )
-from llama_index.schema import BaseDocument
+from llama_index.response_synthesizers import get_response_synthesizer
+from llama_index.schema import BaseNode, MetadataMode
 from llama_index.utils import truncate_text
 
 logger = logging.getLogger(__name__)
 
 
 class SQLDocumentContextBuilder:
     """Builder that builds context for a given set of SQL tables.
 
     Args:
         sql_database (Optional[SQLDatabase]): SQL database to use,
-        llm_predictor (Optional[LLMPredictor]): LLM Predictor to use.
+        llm_predictor (Optional[BaseLLMPredictor]): LLM Predictor to use.
         prompt_helper (Optional[PromptHelper]): Prompt Helper to use.
         text_splitter (Optional[TextSplitter]): Text Splitter to use.
         table_context_prompt (Optional[TableContextPrompt]): A
             Table Context Prompt (see :ref:`Prompt-Templates`).
         refine_table_context_prompt (Optional[RefineTableContextPrompt]):
             A Refine Table Context Prompt (see :ref:`Prompt-Templates`).
         table_context_task (Optional[str]): The query to perform
@@ -71,58 +71,64 @@
         self._refine_table_context_prompt = (
             refine_table_context_prompt or DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL
         )
         self._table_context_task = table_context_task or DEFAULT_TABLE_CONTEXT_QUERY
 
     def build_all_context_from_documents(
         self,
-        documents_dict: Dict[str, List[BaseDocument]],
+        documents_dict: Dict[str, List[BaseNode]],
     ) -> Dict[str, str]:
         """Build context for all tables in the database."""
         context_dict = {}
-        for table_name in self._sql_database.get_table_names():
+        for table_name in self._sql_database.get_usable_table_names():
             context_dict[table_name] = self.build_table_context_from_documents(
                 documents_dict[table_name], table_name
             )
         return context_dict
 
     def build_table_context_from_documents(
         self,
-        documents: Sequence[BaseDocument],
+        documents: Sequence[BaseNode],
         table_name: str,
     ) -> str:
         """Build context from documents for a single table."""
         schema = self._sql_database.get_single_table_info(table_name)
         prompt_with_schema = QuestionAnswerPrompt.from_prompt(
-            self._table_context_prompt.partial_format(schema=schema)
+            self._table_context_prompt.partial_format(schema=schema),
+            prompt_type=PromptType.QUESTION_ANSWER,
         )
         refine_prompt_with_schema = RefinePrompt.from_prompt(
-            self._refine_table_context_prompt.partial_format(schema=schema)
+            self._refine_table_context_prompt.partial_format(schema=schema),
+            prompt_type=PromptType.REFINE,
         )
         text_splitter = (
             self._text_splitter
             or self._service_context.prompt_helper.get_text_splitter_given_prompt(
-                prompt_with_schema, 1
+                prompt_with_schema
             )
         )
         # we use the ResponseBuilder to iteratively go through all texts
-        response_builder = get_response_builder(
-            self._service_context,
-            prompt_with_schema,
-            refine_prompt_with_schema,
+        response_builder = get_response_synthesizer(
+            service_context=self._service_context,
+            text_qa_template=prompt_with_schema,
+            refine_template=refine_prompt_with_schema,
         )
         event_id = self._service_context.callback_manager.on_event_start(
-            CBEventType.CHUNKING, payload={"documents": documents}
+            CBEventType.CHUNKING, payload={EventPayload.DOCUMENTS: documents}
         )
         text_chunks = []
         for doc in documents:
-            chunks = text_splitter.split_text(doc.get_text())
+            chunks = text_splitter.split_text(
+                doc.get_content(metadata_mode=MetadataMode.LLM)
+            )
             text_chunks.extend(chunks)
         self._service_context.callback_manager.on_event_end(
-            CBEventType.CHUNKING, payload={"chunks": text_chunks}, event_id=event_id
+            CBEventType.CHUNKING,
+            payload={EventPayload.CHUNKS: text_chunks},
+            event_id=event_id,
         )
 
         # feed in the "query_str" or the task
         table_context = response_builder.get_response(
             text_chunks=text_chunks, query_str=self._table_context_task
         )
         return cast(str, table_context)
@@ -132,15 +138,15 @@
 
 
 class BaseStructDatapointExtractor:
     """Extracts datapoints from a structured document."""
 
     def __init__(
         self,
-        llm_predictor: LLMPredictor,
+        llm_predictor: BaseLLMPredictor,
         schema_extract_prompt: SchemaExtractPrompt,
         output_parser: OUTPUT_PARSER_TYPE,
     ) -> None:
         """Initialize params."""
         self._llm_predictor = llm_predictor
         self._schema_extract_prompt = schema_extract_prompt
         self._output_parser = output_parser
@@ -181,24 +187,26 @@
     def _get_col_types_map(self) -> Dict[str, type]:
         """Get col types map for schema."""
 
     @abstractmethod
     def _get_schema_text(self) -> str:
         """Get schema text for extracting relevant info from unstructured text."""
 
-    def insert_datapoint_from_nodes(self, nodes: Sequence[Node]) -> None:
+    def insert_datapoint_from_nodes(self, nodes: Sequence[BaseNode]) -> None:
         """Extract datapoint from a document and insert it."""
-        text_chunks = [node.get_text() for node in nodes]
+        text_chunks = [
+            node.get_content(metadata_mode=MetadataMode.LLM) for node in nodes
+        ]
         fields = {}
         for i, text_chunk in enumerate(text_chunks):
             fmt_text_chunk = truncate_text(text_chunk, 50)
             logger.info(f"> Adding chunk {i}: {fmt_text_chunk}")
             # if embedding specified in document, pass it to the Node
             schema_text = self._get_schema_text()
-            response_str, _ = self._llm_predictor.predict(
+            response_str = self._llm_predictor.predict(
                 self._schema_extract_prompt,
                 text=text_chunk,
                 schema=schema_text,
             )
             cur_fields = self._output_parser(response_str)
             if cur_fields is None:
                 continue
```

### Comparing `llama_index-0.6.9/llama_index/indices/common/struct_store/schema.py` & `llama_index-0.7.0/llama_index/indices/common/struct_store/schema.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/indices/common/struct_store/sql.py` & `llama_index-0.7.0/llama_index/indices/common/struct_store/sql.py`

 * *Files 2% similar despite different names*

```diff
@@ -5,25 +5,25 @@
 from sqlalchemy import Table
 
 from llama_index.data_structs.table import StructDatapoint
 from llama_index.indices.common.struct_store.base import (
     OUTPUT_PARSER_TYPE,
     BaseStructDatapointExtractor,
 )
-from llama_index.langchain_helpers.chain_wrapper import LLMPredictor
 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase
+from llama_index.llm_predictor.base import BaseLLMPredictor
 from llama_index.prompts.prompts import SchemaExtractPrompt
 
 
 class SQLStructDatapointExtractor(BaseStructDatapointExtractor):
     """Extracts datapoints from a structured document for a SQL db."""
 
     def __init__(
         self,
-        llm_predictor: LLMPredictor,
+        llm_predictor: BaseLLMPredictor,
         schema_extract_prompt: SchemaExtractPrompt,
         output_parser: OUTPUT_PARSER_TYPE,
         sql_database: SQLDatabase,
         table_name: Optional[str] = None,
         table: Optional[Table] = None,
         ref_doc_id_column: Optional[str] = None,
     ) -> None:
```

### Comparing `llama_index-0.6.9/llama_index/indices/common_tree/base.py` & `llama_index-0.7.0/llama_index/indices/common_tree/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,22 +2,24 @@
 
 
 import asyncio
 import logging
 from typing import Dict, List, Optional, Sequence, Tuple
 
 from llama_index.async_utils import run_async_tasks
-from llama_index.callbacks.schema import CBEventType
+from llama_index.callbacks.schema import CBEventType, EventPayload
 from llama_index.data_structs.data_structs import IndexGraph
-from llama_index.data_structs.node import Node
+from llama_index.schema import BaseNode, TextNode
 from llama_index.storage.docstore import BaseDocumentStore
 from llama_index.storage.docstore.registry import get_default_docstore
 from llama_index.indices.service_context import ServiceContext
 from llama_index.indices.utils import get_sorted_node_list, truncate_text
 from llama_index.prompts.prompts import SummaryPrompt
+from llama_index.utils import get_tqdm_iterable
+from llama_index.schema import MetadataMode
 
 logger = logging.getLogger(__name__)
 
 
 class GPTTreeIndexBuilder:
     """GPT tree index builder.
 
@@ -28,33 +30,35 @@
 
     def __init__(
         self,
         num_children: int,
         summary_prompt: SummaryPrompt,
         service_context: ServiceContext,
         docstore: Optional[BaseDocumentStore] = None,
+        show_progress: bool = False,
         use_async: bool = False,
     ) -> None:
         """Initialize with params."""
         if num_children < 2:
             raise ValueError("Invalid number of children.")
         self.num_children = num_children
         self.summary_prompt = summary_prompt
         self._service_context = service_context
         self._use_async = use_async
+        self._show_progress = show_progress
         self._docstore = docstore or get_default_docstore()
 
     @property
     def docstore(self) -> BaseDocumentStore:
         """Return docstore."""
         return self._docstore
 
     def build_from_nodes(
         self,
-        nodes: Sequence[Node],
+        nodes: Sequence[BaseNode],
         build_tree: bool = True,
     ) -> IndexGraph:
         """Build from text.
 
         Returns:
             IndexGraph: graph object consisting of all_nodes, root_nodes
 
@@ -68,40 +72,45 @@
                 index_graph, index_graph.all_nodes, index_graph.all_nodes, level=0
             )
         else:
             return index_graph
 
     def _prepare_node_and_text_chunks(
         self, cur_node_ids: Dict[int, str]
-    ) -> Tuple[List[int], List[List[Node]], List[str]]:
+    ) -> Tuple[List[int], List[List[BaseNode]], List[str]]:
         """Prepare node and text chunks."""
         cur_nodes = {
             index: self._docstore.get_node(node_id)
             for index, node_id in cur_node_ids.items()
         }
         cur_node_list = get_sorted_node_list(cur_nodes)
         logger.info(
             f"> Building index from nodes: {len(cur_nodes) // self.num_children} chunks"
         )
         indices, cur_nodes_chunks, text_chunks = [], [], []
         for i in range(0, len(cur_node_list), self.num_children):
             cur_nodes_chunk = cur_node_list[i : i + self.num_children]
-            text_chunk = self._service_context.prompt_helper.get_text_from_nodes(
-                cur_nodes_chunk, prompt=self.summary_prompt
+            truncated_chunks = self._service_context.prompt_helper.truncate(
+                prompt=self.summary_prompt,
+                text_chunks=[
+                    node.get_content(metadata_mode=MetadataMode.LLM)
+                    for node in cur_nodes_chunk
+                ],
             )
+            text_chunk = "\n".join(truncated_chunks)
             indices.append(i)
             cur_nodes_chunks.append(cur_nodes_chunk)
             text_chunks.append(text_chunk)
         return indices, cur_nodes_chunks, text_chunks
 
     def _construct_parent_nodes(
         self,
         index_graph: IndexGraph,
         indices: List[int],
-        cur_nodes_chunks: List[List[Node]],
+        cur_nodes_chunks: List[List[BaseNode]],
         summaries: List[str],
     ) -> Dict[int, str]:
         """Construct parent nodes.
 
         Save nodes to docstore.
 
         """
@@ -109,20 +118,18 @@
         for i, cur_nodes_chunk, new_summary in zip(
             indices, cur_nodes_chunks, summaries
         ):
             logger.debug(
                 f"> {i}/{len(cur_nodes_chunk)}, "
                 f"summary: {truncate_text(new_summary, 50)}"
             )
-            new_node = Node(
-                text=new_summary,
-            )
+            new_node = TextNode(text=new_summary)
             index_graph.insert(new_node, children_nodes=cur_nodes_chunk)
             index = index_graph.get_index(new_node)
-            new_node_dict[index] = new_node.get_doc_id()
+            new_node_dict[index] = new_node.node_id
             self._docstore.add_documents([new_node], allow_update=False)
         return new_node_dict
 
     def build_index_from_nodes(
         self,
         index_graph: IndexGraph,
         cur_node_ids: Dict[int, str],
@@ -134,32 +141,41 @@
             index_graph.root_nodes = cur_node_ids
             return index_graph
 
         indices, cur_nodes_chunks, text_chunks = self._prepare_node_and_text_chunks(
             cur_node_ids
         )
         event_id = self._service_context.callback_manager.on_event_start(
-            CBEventType.TREE, payload={"chunks": text_chunks}
+            CBEventType.TREE, payload={EventPayload.CHUNKS: text_chunks}
         )
 
         if self._use_async:
             tasks = [
                 self._service_context.llm_predictor.apredict(
                     self.summary_prompt, context_str=text_chunk
                 )
                 for text_chunk in text_chunks
             ]
-            outputs: List[Tuple[str, str]] = run_async_tasks(tasks)
+            outputs: List[Tuple[str, str]] = run_async_tasks(
+                tasks,
+                show_progress=self._show_progress,
+                progress_bar_desc="Generating summaries",
+            )
             summaries = [output[0] for output in outputs]
         else:
+            text_chunks_progress = get_tqdm_iterable(
+                text_chunks,
+                show_progress=self._show_progress,
+                desc="Generating summaries",
+            )
             summaries = [
                 self._service_context.llm_predictor.predict(
                     self.summary_prompt, context_str=text_chunk
-                )[0]
-                for text_chunk in text_chunks
+                )
+                for text_chunk in text_chunks_progress
             ]
         self._service_context.llama_logger.add_log(
             {"summaries": summaries, "level": level}
         )
         self._service_context.callback_manager.on_event_end(
             CBEventType.TREE,
             payload={"summaries": summaries, "level": level},
@@ -195,19 +211,22 @@
         indices, cur_nodes_chunks, text_chunks = self._prepare_node_and_text_chunks(
             cur_node_ids
         )
         event_id = self._service_context.callback_manager.on_event_start(
             CBEventType.TREE, payload={"chunks": text_chunks}
         )
 
+        text_chunks_progress = get_tqdm_iterable(
+            text_chunks, show_progress=self._show_progress, desc="Generating summaries"
+        )
         tasks = [
             self._service_context.llm_predictor.apredict(
                 self.summary_prompt, context_str=text_chunk
             )
-            for text_chunk in text_chunks
+            for text_chunk in text_chunks_progress
         ]
         outputs: List[Tuple[str, str]] = await asyncio.gather(*tasks)
         summaries = [output[0] for output in outputs]
         self._service_context.llama_logger.add_log(
             {"summaries": summaries, "level": level}
         )
         self._service_context.callback_manager.on_event_end(
```

### Comparing `llama_index-0.6.9/llama_index/indices/composability/graph.py` & `llama_index-0.7.0/llama_index/indices/composability/graph.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,106 +1,122 @@
 """Composability graphs."""
 
 from typing import Any, Dict, List, Optional, Sequence, Type, cast
 
 from llama_index.data_structs.data_structs import IndexStruct
-from llama_index.data_structs.node import IndexNode, DocumentRelationship
-from llama_index.indices.base import BaseGPTIndex
+from llama_index.indices.base import BaseIndex
 from llama_index.indices.query.base import BaseQueryEngine
 from llama_index.indices.service_context import ServiceContext
+from llama_index.schema import IndexNode, NodeRelationship, RelatedNodeInfo, ObjectType
+from llama_index.storage.storage_context import StorageContext
 
 
 class ComposableGraph:
     """Composable graph."""
 
     def __init__(
         self,
-        all_indices: Dict[str, BaseGPTIndex],
+        all_indices: Dict[str, BaseIndex],
         root_id: str,
+        storage_context: Optional[StorageContext] = None,
     ) -> None:
         """Init params."""
         self._all_indices = all_indices
         self._root_id = root_id
+        self.storage_context = storage_context
 
     @property
     def root_id(self) -> str:
         return self._root_id
 
     @property
-    def all_indices(self) -> Dict[str, BaseGPTIndex]:
+    def all_indices(self) -> Dict[str, BaseIndex]:
         return self._all_indices
 
     @property
-    def root_index(self) -> BaseGPTIndex:
+    def root_index(self) -> BaseIndex:
         return self._all_indices[self._root_id]
 
     @property
     def index_struct(self) -> IndexStruct:
         return self._all_indices[self._root_id].index_struct
 
     @property
     def service_context(self) -> ServiceContext:
         return self._all_indices[self._root_id].service_context
 
     @classmethod
     def from_indices(
         cls,
-        root_index_cls: Type[BaseGPTIndex],
-        children_indices: Sequence[BaseGPTIndex],
+        root_index_cls: Type[BaseIndex],
+        children_indices: Sequence[BaseIndex],
         index_summaries: Optional[Sequence[str]] = None,
+        service_context: Optional[ServiceContext] = None,
+        storage_context: Optional[StorageContext] = None,
         **kwargs: Any,
     ) -> "ComposableGraph":  # type: ignore
         """Create composable graph using this index class as the root."""
-        if index_summaries is None:
-            for index in children_indices:
-                if index.index_struct.summary is None:
-                    raise ValueError(
-                        "Summary must be set for children indices. If the index does "
-                        "a summary (through index.index_struct.summary), then it must "
-                        "be specified with then `index_summaries` "
-                        "argument in this function."
-                        "We will support automatically setting the summary in the "
-                        "future."
-                    )
-            index_summaries = [index.index_struct.summary for index in children_indices]
-        else:
-            # set summaries for each index
-            for index, summary in zip(children_indices, index_summaries):
-                index.index_struct.summary = summary
+        service_context = service_context or ServiceContext.from_defaults()
+        with service_context.callback_manager.as_trace("graph_construction"):
+            if index_summaries is None:
+                for index in children_indices:
+                    if index.index_struct.summary is None:
+                        raise ValueError(
+                            "Summary must be set for children indices. "
+                            "If the index does a summary "
+                            "(through index.index_struct.summary), then "
+                            "it must be specified with then `index_summaries` "
+                            "argument in this function. We will support "
+                            "automatically setting the summary in the future."
+                        )
+                index_summaries = [
+                    index.index_struct.summary for index in children_indices
+                ]
+            else:
+                # set summaries for each index
+                for index, summary in zip(children_indices, index_summaries):
+                    index.index_struct.summary = summary
 
-        if len(children_indices) != len(index_summaries):
-            raise ValueError("indices and index_summaries must have same length!")
+            if len(children_indices) != len(index_summaries):
+                raise ValueError("indices and index_summaries must have same length!")
 
-        # construct index nodes
-        index_nodes = []
-        for index, summary in zip(children_indices, index_summaries):
-            assert isinstance(index.index_struct, IndexStruct)
-            index_node = IndexNode(
-                text=summary,
-                index_id=index.index_id,
-                relationships={DocumentRelationship.SOURCE: index.index_id},
+            # construct index nodes
+            index_nodes = []
+            for index, summary in zip(children_indices, index_summaries):
+                assert isinstance(index.index_struct, IndexStruct)
+                index_node = IndexNode(
+                    text=summary,
+                    index_id=index.index_id,
+                    relationships={
+                        NodeRelationship.SOURCE: RelatedNodeInfo(
+                            node_id=index.index_id, node_type=ObjectType.INDEX
+                        )
+                    },
+                )
+                index_nodes.append(index_node)
+
+            # construct root index
+            root_index = root_index_cls(
+                nodes=index_nodes,
+                service_context=service_context,
+                storage_context=storage_context,
+                **kwargs,
+            )
+            # type: ignore
+            all_indices: List[BaseIndex] = cast(List[BaseIndex], children_indices) + [
+                root_index
+            ]
+
+            return cls(
+                all_indices={index.index_id: index for index in all_indices},
+                root_id=root_index.index_id,
+                storage_context=storage_context,
             )
-            index_nodes.append(index_node)
-
-        # construct root index
-        root_index = root_index_cls(
-            nodes=index_nodes,
-            **kwargs,
-        )
-        # type: ignore
-        all_indices: List[BaseGPTIndex] = cast(List[BaseGPTIndex], children_indices) + [
-            root_index
-        ]
-
-        return cls(
-            all_indices={index.index_id: index for index in all_indices},
-            root_id=root_index.index_id,
-        )
 
-    def get_index(self, index_struct_id: Optional[str] = None) -> BaseGPTIndex:
+    def get_index(self, index_struct_id: Optional[str] = None) -> BaseIndex:
         """Get index from index struct id."""
         if index_struct_id is None:
             index_struct_id = self._root_id
         return self._all_indices[index_struct_id]
 
     def as_query_engine(self, **kwargs: Any) -> BaseQueryEngine:
         # NOTE: lazy import
```

### Comparing `llama_index-0.6.9/llama_index/indices/document_summary/base.py` & `llama_index-0.7.0/llama_index/indices/document_summary/base.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,27 +1,34 @@
 """Document summary index.
 
 A data structure where LlamaIndex stores the summary per document, and maps
 the summary to the underlying Nodes.
 This summary can be used for retrieval.
 
 """
-
+import logging
 from collections import defaultdict
-from llama_index.indices.query.schema import QueryBundle
-from llama_index.indices.base import BaseGPTIndex
+from enum import Enum
+from typing import Any, Dict, Optional, Sequence, Union, cast
+from llama_index.utils import get_tqdm_iterable
+
 from llama_index.data_structs.document_summary import IndexDocumentSummary
-from llama_index.data_structs.node import Node, DocumentRelationship, NodeWithScore
-from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.query.response_synthesis import ResponseSynthesizer
-from typing import Optional, Sequence, Any, Union, cast
-import logging
+from llama_index.indices.base import BaseIndex
 from llama_index.indices.base_retriever import BaseRetriever
+from llama_index.indices.service_context import ServiceContext
 from llama_index.response.schema import Response
-from enum import Enum
+from llama_index.response_synthesizers import BaseSynthesizer, get_response_synthesizer
+from llama_index.schema import (
+    BaseNode,
+    NodeWithScore,
+    NodeRelationship,
+    RelatedNodeInfo,
+    TextNode,
+)
+from llama_index.storage.docstore.types import RefDocInfo
 
 logger = logging.getLogger(__name__)
 
 
 DEFAULT_SUMMARY_QUERY = (
     "Give a concise summary of this document. Also describe some of the questions "
     "that this document can answer. "
@@ -32,44 +39,46 @@
     DEFAULT = "default"
     EMBEDDING = "embedding"
 
 
 DSRM = DocumentSummaryRetrieverMode
 
 
-class GPTDocumentSummaryIndex(BaseGPTIndex[IndexDocumentSummary]):
-    """GPT Document Summary Index.
+class DocumentSummaryIndex(BaseIndex[IndexDocumentSummary]):
+    """Document Summary Index.
 
     Args:
         summary_template (Optional[SummaryPrompt]): A Summary Prompt
             (see :ref:`Prompt-Templates`).
+        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.
 
     """
 
     index_struct_cls = IndexDocumentSummary
 
     def __init__(
         self,
-        nodes: Optional[Sequence[Node]] = None,
+        nodes: Optional[Sequence[BaseNode]] = None,
         index_struct: Optional[IndexDocumentSummary] = None,
         service_context: Optional[ServiceContext] = None,
-        response_synthesizer: Optional[ResponseSynthesizer] = None,
+        response_synthesizer: Optional[BaseSynthesizer] = None,
         summary_query: str = DEFAULT_SUMMARY_QUERY,
+        show_progress: bool = False,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
-        print(response_synthesizer)
-        self._response_synthesizer = (
-            response_synthesizer or ResponseSynthesizer.from_args()
+        self._response_synthesizer = response_synthesizer or get_response_synthesizer(
+            service_context=service_context,
         )
         self._summary_query = summary_query or "summarize:"
         super().__init__(
             nodes=nodes,
             index_struct=index_struct,
             service_context=service_context,
+            show_progress=show_progress,
             **kwargs,
         )
 
     def as_retriever(
         self,
         retriever_mode: Union[str, DSRM] = DSRM.DEFAULT,
         **kwargs: Any,
@@ -101,67 +110,97 @@
         Args:
             doc_id (str): A document id.
 
         """
         if doc_id not in self._index_struct.doc_id_to_summary_id:
             raise ValueError(f"doc_id {doc_id} not in index")
         summary_id = self._index_struct.doc_id_to_summary_id[doc_id]
-        return self.docstore.get_node(summary_id).get_text()
+        return self.docstore.get_node(summary_id).get_content()
 
     def _add_nodes_to_index(
-        self, index_struct: IndexDocumentSummary, nodes: Sequence[Node]
+        self,
+        index_struct: IndexDocumentSummary,
+        nodes: Sequence[BaseNode],
+        show_progress: bool = False,
     ) -> None:
         """Add nodes to index."""
         doc_id_to_nodes = defaultdict(list)
         for node in nodes:
             if node.ref_doc_id is None:
                 raise ValueError(
                     "ref_doc_id of node cannot be None when building a document "
                     "summary index"
                 )
             doc_id_to_nodes[node.ref_doc_id].append(node)
 
         summary_node_dict = {}
-        for doc_id, nodes in doc_id_to_nodes.items():
+        items = doc_id_to_nodes.items()
+        iterable_with_progress = get_tqdm_iterable(
+            items, show_progress, "Summarizing documents"
+        )
+
+        for doc_id, nodes in iterable_with_progress:
             print(f"current doc id: {doc_id}")
-            nodes_with_scores = [NodeWithScore(n) for n in nodes]
+            nodes_with_scores = [NodeWithScore(node=n) for n in nodes]
             # get the summary for each doc_id
             summary_response = self._response_synthesizer.synthesize(
-                query_bundle=QueryBundle(self._summary_query),
+                query=self._summary_query,
                 nodes=nodes_with_scores,
             )
             summary_response = cast(Response, summary_response)
-            summary_node_dict[doc_id] = Node(
-                summary_response.response,
-                relationships={DocumentRelationship.SOURCE: doc_id},
+            summary_node_dict[doc_id] = TextNode(
+                text=summary_response.response,
+                relationships={
+                    NodeRelationship.SOURCE: RelatedNodeInfo(node_id=doc_id)
+                },
             )
             self.docstore.add_documents([summary_node_dict[doc_id]])
             logger.info(
                 f"> Generated summary for doc {doc_id}: " f"{summary_response.response}"
             )
 
         for doc_id, nodes in doc_id_to_nodes.items():
             index_struct.add_summary_and_nodes(summary_node_dict[doc_id], nodes)
 
-    def _build_index_from_nodes(self, nodes: Sequence[Node]) -> IndexDocumentSummary:
+    def _build_index_from_nodes(
+        self, nodes: Sequence[BaseNode]
+    ) -> IndexDocumentSummary:
         """Build index from nodes."""
         # first get doc_id to nodes_dict, generate a summary for each doc_id,
         # then build the index struct
         index_struct = IndexDocumentSummary()
-        self._add_nodes_to_index(index_struct, nodes)
+        self._add_nodes_to_index(index_struct, nodes, self._show_progress)
         return index_struct
 
-    def _insert(self, nodes: Sequence[Node], **insert_kwargs: Any) -> None:
+    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
         """Insert a document."""
         self._add_nodes_to_index(self._index_struct, nodes)
 
-    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document."""
-        if doc_id not in self._index_struct.doc_id_to_summary_id:
-            raise ValueError(f"doc_id {doc_id} not in index")
-        summary_id = self._index_struct.doc_id_to_summary_id[doc_id]
+    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:
+        """Delete a node."""
+        if node_id not in self._index_struct.doc_id_to_summary_id:
+            raise ValueError(f"node_id {node_id} not in index")
+        summary_id = self._index_struct.doc_id_to_summary_id[node_id]
 
         # delete summary node from docstore
         self.docstore.delete_document(summary_id)
+
         # delete from index struct
-        self._index_struct.delete(doc_id)
-        # TODO: figure out whether to delete source nodes
+        self._index_struct.delete(node_id)
+
+    @property
+    def ref_doc_info(self) -> Dict[str, RefDocInfo]:
+        """Retrieve a dict mapping of ingested documents and their nodes+metadata."""
+        ref_doc_ids = list(self._index_struct.doc_id_to_summary_id.keys())
+
+        all_ref_doc_info = {}
+        for ref_doc_id in ref_doc_ids:
+            ref_doc_info = self.docstore.get_ref_doc_info(ref_doc_id)
+            if not ref_doc_info:
+                continue
+
+            all_ref_doc_info[ref_doc_id] = ref_doc_info
+        return all_ref_doc_info
+
+
+# legacy
+GPTDocumentSummaryIndex = DocumentSummaryIndex
```

### Comparing `llama_index-0.6.9/llama_index/indices/document_summary/retrievers.py` & `llama_index-0.7.0/llama_index/indices/document_summary/retrievers.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,49 +1,48 @@
 """Document summary retrievers.
 
 This module contains retrievers for document summary indices.
 
 """
 
-from llama_index.callbacks.schema import CBEventType
-from llama_index.indices.document_summary.base import GPTDocumentSummaryIndex
-from llama_index.indices.query.schema import QueryBundle
+import logging
+from typing import Any, Callable, Dict, List, Optional, Tuple
+
+from llama_index.callbacks.schema import CBEventType, EventPayload
 from llama_index.indices.base_retriever import BaseRetriever
-from typing import Any, List, Optional, Callable, Tuple, Dict
-from llama_index.data_structs.node import Node, NodeWithScore
-from llama_index.prompts.choice_select import ChoiceSelectPrompt
+from llama_index.indices.document_summary.base import DocumentSummaryIndex
+from llama_index.indices.query.embedding_utils import get_top_k_embeddings
+from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.query.embedding_utils import (
-    get_top_k_embeddings,
-)
-from llama_index.prompts.choice_select import (
-    DEFAULT_CHOICE_SELECT_PROMPT,
-)
-import logging
 from llama_index.indices.utils import (
     default_format_node_batch_fn,
     default_parse_choice_select_answer_fn,
 )
+from llama_index.prompts.choice_select import (
+    DEFAULT_CHOICE_SELECT_PROMPT,
+    ChoiceSelectPrompt,
+)
+from llama_index.schema import NodeWithScore, BaseNode, MetadataMode
 
 logger = logging.getLogger(__name__)
 
 
 class DocumentSummaryIndexRetriever(BaseRetriever):
     """Document Summary Index Retriever.
 
     By default, select relevant summaries from index using LLM calls.
 
     Args:
-        index (GPTDocumentSummaryIndex): The index to retrieve from.
+        index (DocumentSummaryIndex): The index to retrieve from.
 
     """
 
     def __init__(
         self,
-        index: GPTDocumentSummaryIndex,
+        index: DocumentSummaryIndex,
         choice_select_prompt: Optional[ChoiceSelectPrompt] = None,
         choice_batch_size: int = 10,
         format_node_batch_fn: Optional[Callable] = None,
         parse_choice_select_answer_fn: Optional[Callable] = None,
         service_context: Optional[ServiceContext] = None,
         **kwargs: Any,
     ) -> None:
@@ -69,15 +68,15 @@
         results = []
         for idx in range(0, len(summary_ids), self._choice_batch_size):
             summary_ids_batch = summary_ids[idx : idx + self._choice_batch_size]
             summary_nodes = self._index.docstore.get_nodes(summary_ids_batch)
             query_str = query_bundle.query_str
             fmt_batch_str = self._format_node_batch_fn(summary_nodes)
             # call each batch independently
-            raw_response, _ = self._service_context.llm_predictor.predict(
+            raw_response = self._service_context.llm_predictor.predict(
                 self._choice_select_prompt,
                 context_str=fmt_batch_str,
                 query_str=query_str,
             )
             raw_choices, relevances = self._parse_choice_select_answer_fn(
                 raw_response, len(summary_nodes)
             )
@@ -85,33 +84,33 @@
 
             choice_summary_ids = [summary_ids_batch[ci] for ci in choice_idxs]
 
             for idx, summary_id in enumerate(choice_summary_ids):
                 node_ids = self._index.index_struct.summary_id_to_node_ids[summary_id]
                 nodes = self._index.docstore.get_nodes(node_ids)
                 relevance = relevances[idx] if relevances is not None else None
-                results.extend([NodeWithScore(n, score=relevance) for n in nodes])
+                results.extend([NodeWithScore(node=n, score=relevance) for n in nodes])
 
         return results
 
 
 class DocumentSummaryIndexEmbeddingRetriever(BaseRetriever):
     """Document Summary Index Embedding Retriever.
 
     Generates embeddings on the fly, attaches to each summary node.
 
     NOTE: implementation is similar to ListIndexEmbeddingRetriever.
 
     Args:
-        index (GPTDocumentSummaryIndex): The index to retrieve from.
+        index (DocumentSummaryIndex): The index to retrieve from.
 
     """
 
     def __init__(
-        self, index: GPTDocumentSummaryIndex, similarity_top_k: int = 1, **kwargs: Any
+        self, index: DocumentSummaryIndex, similarity_top_k: int = 1, **kwargs: Any
     ) -> None:
         """Init params."""
         self._index = index
         self._similarity_top_k = similarity_top_k
 
     def _retrieve(
         self,
@@ -132,49 +131,56 @@
         )
 
         top_k_summary_ids = [summary_ids[i] for i in top_idxs]
         results = []
         for summary_id in top_k_summary_ids:
             node_ids = self._index.index_struct.summary_id_to_node_ids[summary_id]
             nodes = self._index.docstore.get_nodes(node_ids)
-            results.extend([NodeWithScore(n) for n in nodes])
+            results.extend([NodeWithScore(node=n) for n in nodes])
         return results
 
     def _get_embeddings(
-        self, query_bundle: QueryBundle, nodes: List[Node]
+        self, query_bundle: QueryBundle, nodes: List[BaseNode]
     ) -> Tuple[List[float], List[List[float]]]:
         """Get top nodes by similarity to the query."""
         embed_model = self._index.service_context.embed_model
         if query_bundle.embedding is None:
             event_id = self._index._service_context.callback_manager.on_event_start(
                 CBEventType.EMBEDDING
             )
             query_bundle.embedding = embed_model.get_agg_embedding_from_queries(
                 query_bundle.embedding_strs
             )
             self._index._service_context.callback_manager.on_event_end(
-                CBEventType.EMBEDDING, payload={"num_nodes": 1}, event_id=event_id
+                CBEventType.EMBEDDING,
+                payload={EventPayload.CHUNKS: query_bundle.embedding_strs},
+                event_id=event_id,
             )
 
         event_id = self._index._service_context.callback_manager.on_event_start(
             CBEventType.EMBEDDING
         )
         id_to_embed_map: Dict[str, List[float]] = {}
         for node in nodes:
             if node.embedding is None:
-                embed_model.queue_text_for_embedding(node.get_doc_id(), node.get_text())
+                embed_model.queue_text_for_embedding(
+                    node.node_id,
+                    node.get_content(metadata_mode=MetadataMode.EMBED),
+                )
             else:
-                id_to_embed_map[node.get_doc_id()] = node.embedding
+                id_to_embed_map[node.node_id] = node.embedding
 
         (
             result_ids,
             result_embeddings,
         ) = embed_model.get_queued_text_embeddings()
         self._index._service_context.callback_manager.on_event_end(
             CBEventType.EMBEDDING,
-            payload={"num_nodes": len(result_ids)},
+            payload={
+                EventPayload.CHUNKS: [x for x in nodes if node.embedding is not None]
+            },
             event_id=event_id,
         )
         for new_id, text_embedding in zip(result_ids, result_embeddings):
             id_to_embed_map[new_id] = text_embedding
-        node_embeddings = [id_to_embed_map[n.get_doc_id()] for n in nodes]
+        node_embeddings = [id_to_embed_map[n.node_id] for n in nodes]
         return query_bundle.embedding, node_embeddings
```

### Comparing `llama_index-0.6.9/llama_index/indices/empty/retrievers.py` & `llama_index-0.7.0/llama_index/indices/empty/retrievers.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,33 +1,32 @@
-"""Default query for GPTEmptyIndex."""
+"""Default query for EmptyIndex."""
 from typing import Any, List, Optional
-from llama_index.indices.base_retriever import BaseRetriever
 
-from llama_index.data_structs.node import NodeWithScore
+from llama_index.indices.base_retriever import BaseRetriever
+from llama_index.indices.empty.base import EmptyIndex
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.prompts.default_prompts import DEFAULT_SIMPLE_INPUT_PROMPT
 from llama_index.prompts.prompts import SimpleInputPrompt
-
-from llama_index.indices.empty.base import GPTEmptyIndex
+from llama_index.schema import NodeWithScore
 
 
 class EmptyIndexRetriever(BaseRetriever):
-    """GPTEmptyIndex query.
+    """EmptyIndex query.
 
     Passes the raw LLM call to the underlying LLM model.
 
     Args:
         input_prompt (Optional[SimpleInputPrompt]): A Simple Input Prompt
             (see :ref:`Prompt-Templates`).
 
     """
 
     def __init__(
         self,
-        index: GPTEmptyIndex,
+        index: EmptyIndex,
         input_prompt: Optional[SimpleInputPrompt] = None,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
         self._index = index
         self._input_prompt = input_prompt or DEFAULT_SIMPLE_INPUT_PROMPT
```

### Comparing `llama_index-0.6.9/llama_index/indices/keyword_table/base.py` & `llama_index-0.7.0/llama_index/indices/keyword_table/base.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,40 +6,42 @@
 from the query text. Then, it tries to match those keywords to
 existing keywords in the table.
 
 """
 
 from abc import abstractmethod
 from enum import Enum
-from typing import Any, Optional, Sequence, Set, Union
+from typing import Any, Dict, Optional, Sequence, Set, Union
+from llama_index.utils import get_tqdm_iterable
 
 from llama_index.async_utils import run_async_tasks
 from llama_index.data_structs.data_structs import KeywordTable
-from llama_index.data_structs.node import Node
-from llama_index.indices.base import BaseGPTIndex
+from llama_index.indices.base import BaseIndex
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.keyword_table.utils import extract_keywords_given_response
 from llama_index.indices.service_context import ServiceContext
 from llama_index.prompts.default_prompts import (
     DEFAULT_KEYWORD_EXTRACT_TEMPLATE,
     DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,
 )
 from llama_index.prompts.prompts import KeywordExtractPrompt
+from llama_index.schema import BaseNode, MetadataMode
+from llama_index.storage.docstore.types import RefDocInfo
 
 DQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE
 
 
 class KeywordTableRetrieverMode(str, Enum):
     DEFAULT = "default"
     SIMPLE = "simple"
     RAKE = "rake"
 
 
-class BaseGPTKeywordTableIndex(BaseGPTIndex[KeywordTable]):
-    """GPT Keyword Table Index.
+class BaseKeywordTableIndex(BaseIndex[KeywordTable]):
+    """Base Keyword Table Index.
 
     This index extracts keywords from the text, and maps each
     keyword to the node(s) that it corresponds to. In this sense it mimicks a
     "hash table". During index construction, the keyword table is constructed
     by extracting keywords from each node and creating an internal mapping.
 
     During query time, the keywords are extracted from the query text, and these
@@ -47,27 +49,29 @@
     are then used to answer the query.
 
     Args:
         keyword_extract_template (Optional[KeywordExtractPrompt]): A Keyword
             Extraction Prompt
             (see :ref:`Prompt-Templates`).
         use_async (bool): Whether to use asynchronous calls. Defaults to False.
+        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.
 
     """
 
     index_struct_cls = KeywordTable
 
     def __init__(
         self,
-        nodes: Optional[Sequence[Node]] = None,
+        nodes: Optional[Sequence[BaseNode]] = None,
         index_struct: Optional[KeywordTable] = None,
         service_context: Optional[ServiceContext] = None,
         keyword_extract_template: Optional[KeywordExtractPrompt] = None,
         max_keywords_per_chunk: int = 10,
         use_async: bool = False,
+        show_progress: bool = False,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
         # need to set parameters before building index in base class.
         self.max_keywords_per_chunk = max_keywords_per_chunk
         self.keyword_extract_template = (
             keyword_extract_template or DEFAULT_KEYWORD_EXTRACT_TEMPLATE
@@ -77,14 +81,15 @@
             max_keywords=self.max_keywords_per_chunk
         )
         self._use_async = use_async
         super().__init__(
             nodes=nodes,
             index_struct=index_struct,
             service_context=service_context,
+            show_progress=show_progress,
             **kwargs,
         )
 
     def as_retriever(
         self,
         retriever_mode: Union[
             str, KeywordTableRetrieverMode
@@ -113,91 +118,123 @@
 
     async def _async_extract_keywords(self, text: str) -> Set[str]:
         """Extract keywords from text."""
         # by default just call sync version
         return self._extract_keywords(text)
 
     def _add_nodes_to_index(
-        self, index_struct: KeywordTable, nodes: Sequence[Node]
+        self,
+        index_struct: KeywordTable,
+        nodes: Sequence[BaseNode],
+        show_progress: bool = False,
     ) -> None:
         """Add document to index."""
-        for n in nodes:
-            keywords = self._extract_keywords(n.get_text())
+        nodes_with_progress = get_tqdm_iterable(
+            nodes, show_progress, "Extracting keywords from nodes"
+        )
+        for n in nodes_with_progress:
+            keywords = self._extract_keywords(
+                n.get_content(metadata_mode=MetadataMode.LLM)
+            )
             index_struct.add_node(list(keywords), n)
 
     async def _async_add_nodes_to_index(
-        self, index_struct: KeywordTable, nodes: Sequence[Node]
+        self,
+        index_struct: KeywordTable,
+        nodes: Sequence[BaseNode],
+        show_progress: bool = False,
     ) -> None:
         """Add document to index."""
-        for n in nodes:
-            keywords = await self._async_extract_keywords(n.get_text())
+        nodes_with_progress = get_tqdm_iterable(
+            nodes, show_progress, "Extracting keywords from nodes"
+        )
+        for n in nodes_with_progress:
+            keywords = await self._async_extract_keywords(
+                n.get_content(metadata_mode=MetadataMode.LLM)
+            )
             index_struct.add_node(list(keywords), n)
 
-    def _build_index_from_nodes(self, nodes: Sequence[Node]) -> KeywordTable:
+    def _build_index_from_nodes(self, nodes: Sequence[BaseNode]) -> KeywordTable:
         """Build the index from nodes."""
         # do simple concatenation
         index_struct = KeywordTable(table={})
         if self._use_async:
-            tasks = [self._async_add_nodes_to_index(index_struct, nodes)]
+            tasks = [
+                self._async_add_nodes_to_index(index_struct, nodes, self._show_progress)
+            ]
             run_async_tasks(tasks)
         else:
-            self._add_nodes_to_index(index_struct, nodes)
+            self._add_nodes_to_index(index_struct, nodes, self._show_progress)
 
         return index_struct
 
-    def _insert(self, nodes: Sequence[Node], **insert_kwargs: Any) -> None:
+    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
         """Insert nodes."""
         for n in nodes:
-            keywords = self._extract_keywords(n.get_text())
+            keywords = self._extract_keywords(
+                n.get_content(metadata_mode=MetadataMode.LLM)
+            )
             self._index_struct.add_node(list(keywords), n)
 
-    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document."""
-        # get set of ids that correspond to node
-        node_idxs_to_delete = set()
-        node_id_list = list(self._index_struct.node_ids)
-        nodes = self._docstore.get_nodes(node_id_list)
-        for node_idx, node in zip(node_id_list, nodes):
-            if node.ref_doc_id != doc_id:
-                continue
-            node_idxs_to_delete.add(node_idx)
-        for node_idx in node_idxs_to_delete:
-            self._docstore.delete_document(node_idx)
-
-        # delete node_idxs from keyword to node idxs mapping
+    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:
+        """Delete a node."""
+        # delete node from the keyword table
         keywords_to_delete = set()
-        for keyword, node_idxs in self._index_struct.table.items():
-            if node_idxs_to_delete.intersection(node_idxs):
-                self._index_struct.table[keyword] = node_idxs.difference(
-                    node_idxs_to_delete
-                )
-                if not self._index_struct.table[keyword]:
+        for keyword, existing_node_ids in self._index_struct.table.items():
+            if node_id in existing_node_ids:
+                existing_node_ids.remove(node_id)
+                if len(existing_node_ids) == 0:
                     keywords_to_delete.add(keyword)
 
+        # delete keywords that have zero nodes
         for keyword in keywords_to_delete:
             del self._index_struct.table[keyword]
 
+    @property
+    def ref_doc_info(self) -> Dict[str, RefDocInfo]:
+        """Retrieve a dict mapping of ingested documents and their nodes+metadata."""
+        node_doc_ids_sets = list(self._index_struct.table.values())
+        node_doc_ids = list(set().union(*node_doc_ids_sets))
+        nodes = self.docstore.get_nodes(node_doc_ids)
+
+        all_ref_doc_info = {}
+        for node in nodes:
+            ref_node = node.source_node
+            if not ref_node:
+                continue
+
+            ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)
+            if not ref_doc_info:
+                continue
+
+            all_ref_doc_info[ref_node.node_id] = ref_doc_info
+        return all_ref_doc_info
 
-class GPTKeywordTableIndex(BaseGPTKeywordTableIndex):
-    """GPT Keyword Table Index.
+
+class KeywordTableIndex(BaseKeywordTableIndex):
+    """Keyword Table Index.
 
     This index uses a GPT model to extract keywords from the text.
 
     """
 
     def _extract_keywords(self, text: str) -> Set[str]:
         """Extract keywords from text."""
-        response, formatted_prompt = self._service_context.llm_predictor.predict(
+        response = self._service_context.llm_predictor.predict(
             self.keyword_extract_template,
             text=text,
         )
         keywords = extract_keywords_given_response(response, start_token="KEYWORDS:")
         return keywords
 
     async def _async_extract_keywords(self, text: str) -> Set[str]:
         """Extract keywords from text."""
-        response, formatted_prompt = await self._service_context.llm_predictor.apredict(
+        response = await self._service_context.llm_predictor.apredict(
             self.keyword_extract_template,
             text=text,
         )
         keywords = extract_keywords_given_response(response, start_token="KEYWORDS:")
         return keywords
+
+
+# legacy
+GPTKeywordTableIndex = KeywordTableIndex
```

### Comparing `llama_index-0.6.9/llama_index/indices/keyword_table/rake_base.py` & `llama_index-0.7.0/llama_index/indices/keyword_table/rake_base.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,22 +1,26 @@
 """RAKE keyword-table based index.
 
-Similar to GPTKeywordTableIndex, but uses RAKE instead of GPT.
+Similar to KeywordTableIndex, but uses RAKE instead of GPT.
 
 """
 
 from typing import Set
 
-from llama_index.indices.keyword_table.base import BaseGPTKeywordTableIndex
+from llama_index.indices.keyword_table.base import BaseKeywordTableIndex
 from llama_index.indices.keyword_table.utils import rake_extract_keywords
 
 
-class GPTRAKEKeywordTableIndex(BaseGPTKeywordTableIndex):
-    """GPT RAKE Keyword Table Index.
+class RAKEKeywordTableIndex(BaseKeywordTableIndex):
+    """RAKE Keyword Table Index.
 
     This index uses a RAKE keyword extractor to extract keywords from the text.
 
     """
 
     def _extract_keywords(self, text: str) -> Set[str]:
         """Extract keywords from text."""
         return rake_extract_keywords(text, max_keywords=self.max_keywords_per_chunk)
+
+
+# legacy
+GPTRAKEKeywordTableIndex = RAKEKeywordTableIndex
```

### Comparing `llama_index-0.6.9/llama_index/indices/keyword_table/retrievers.py` & `llama_index-0.7.0/llama_index/indices/keyword_table/retrievers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,38 +1,36 @@
-"""Query for GPTKeywordTableIndex."""
+"""Query for KeywordTableIndex."""
 import logging
 from abc import abstractmethod
 from collections import defaultdict
 from typing import Any, Dict, List, Optional
 
-from llama_index.data_structs.node import NodeWithScore
 from llama_index.indices.base_retriever import BaseRetriever
-from llama_index.indices.keyword_table.base import (
-    BaseGPTKeywordTableIndex,
-)
+from llama_index.indices.keyword_table.base import BaseKeywordTableIndex
 from llama_index.indices.keyword_table.utils import (
     extract_keywords_given_response,
     rake_extract_keywords,
     simple_extract_keywords,
 )
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.prompts.default_prompts import (
     DEFAULT_KEYWORD_EXTRACT_TEMPLATE,
     DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,
 )
 from llama_index.prompts.prompts import KeywordExtractPrompt, QueryKeywordExtractPrompt
+from llama_index.schema import NodeWithScore
 from llama_index.utils import truncate_text
 
 DQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE
 
 logger = logging.getLogger(__name__)
 
 
 class BaseKeywordTableRetriever(BaseRetriever):
-    """Base GPT Keyword Table Index Query.
+    """Base Keyword Table Retriever.
 
     Arguments are shared among subclasses.
 
     Args:
         keyword_extract_template (Optional[KeywordExtractPrompt]): A Keyword
             Extraction Prompt
             (see :ref:`Prompt-Templates`).
@@ -46,15 +44,15 @@
         max_keywords_per_query (int): Maximum number of keywords to extract from query.
         num_chunks_per_query (int): Maximum number of text chunks to query.
 
     """
 
     def __init__(
         self,
-        index: BaseGPTKeywordTableIndex,
+        index: BaseKeywordTableIndex,
         keyword_extract_template: Optional[KeywordExtractPrompt] = None,
         query_keyword_extract_template: Optional[QueryKeywordExtractPrompt] = None,
         max_keywords_per_query: int = 10,
         num_chunks_per_query: int = 10,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
@@ -98,17 +96,17 @@
         sorted_chunk_indices = sorted_chunk_indices[: self.num_chunks_per_query]
         sorted_nodes = self._docstore.get_nodes(sorted_chunk_indices)
 
         if logging.getLogger(__name__).getEffectiveLevel() == logging.DEBUG:
             for chunk_idx, node in zip(sorted_chunk_indices, sorted_nodes):
                 logger.debug(
                     f"> Querying with idx: {chunk_idx}: "
-                    f"{truncate_text(node.get_text(), 50)}"
+                    f"{truncate_text(node.get_content(), 50)}"
                 )
-        sorted_nodes_with_scores = [NodeWithScore(node) for node in sorted_nodes]
+        sorted_nodes_with_scores = [NodeWithScore(node=node) for node in sorted_nodes]
 
         return sorted_nodes_with_scores
 
 
 class KeywordTableGPTRetriever(BaseKeywordTableRetriever):
     """Keyword Table Index GPT Retriever.
 
@@ -116,15 +114,15 @@
 
     See BaseGPTKeywordTableQuery for arguments.
 
     """
 
     def _get_keywords(self, query_str: str) -> List[str]:
         """Extract keywords."""
-        response, formatted_prompt = self._service_context.llm_predictor.predict(
+        response = self._service_context.llm_predictor.predict(
             self.query_keyword_extract_template,
             max_keywords=self.max_keywords_per_query,
             question=query_str,
         )
         keywords = extract_keywords_given_response(response, start_token="KEYWORDS:")
         return list(keywords)
```

### Comparing `llama_index-0.6.9/llama_index/indices/keyword_table/simple_base.py` & `llama_index-0.7.0/llama_index/indices/keyword_table/simple_base.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,26 +1,30 @@
 """Simple keyword-table based index.
 
-Similar to GPTKeywordTableIndex, but uses a simpler keyword extraction
+Similar to KeywordTableIndex, but uses a simpler keyword extraction
 technique that doesn't involve GPT - just uses regex.
 
 """
 
 from typing import Set
 
-from llama_index.indices.keyword_table.base import BaseGPTKeywordTableIndex
+from llama_index.indices.keyword_table.base import BaseKeywordTableIndex
 from llama_index.indices.keyword_table.utils import simple_extract_keywords
 from llama_index.prompts.default_prompts import DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE
 
 DQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE
 
 
-class GPTSimpleKeywordTableIndex(BaseGPTKeywordTableIndex):
-    """GPT Simple Keyword Table Index.
+class SimpleKeywordTableIndex(BaseKeywordTableIndex):
+    """Simple Keyword Table Index.
 
     This index uses a simple regex extractor to extract keywords from the text.
 
     """
 
     def _extract_keywords(self, text: str) -> Set[str]:
         """Extract keywords from text."""
         return simple_extract_keywords(text, self.max_keywords_per_chunk)
+
+
+# legacy
+GPTSimpleKeywordTableIndex = SimpleKeywordTableIndex
```

### Comparing `llama_index-0.6.9/llama_index/indices/keyword_table/utils.py` & `llama_index-0.7.0/llama_index/indices/keyword_table/utils.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/indices/knowledge_graph/base.py` & `llama_index-0.7.0/llama_index/indices/knowledge_graph/base.py`

 * *Files 22% similar despite different names*

```diff
@@ -5,52 +5,67 @@
 keywords as keys per item. It similarly extracts keywords
 from the query text. Then, it tries to match those keywords to
 existing keywords in the table.
 
 """
 
 import logging
-from typing import Any, List, Optional, Sequence, Tuple
+from typing import Any, Dict, List, Optional, Sequence, Tuple
+from llama_index.utils import get_tqdm_iterable
 
+from llama_index.constants import GRAPH_STORE_KEY
 from llama_index.data_structs.data_structs import KG
-from llama_index.data_structs.node import Node
-from llama_index.indices.base import BaseGPTIndex
+from llama_index.graph_stores.simple import SimpleGraphStore
+from llama_index.graph_stores.types import GraphStore
+from llama_index.indices.base import BaseIndex
 from llama_index.indices.base_retriever import BaseRetriever
+from llama_index.indices.service_context import ServiceContext
 from llama_index.prompts.default_prompts import (
     DEFAULT_KG_TRIPLET_EXTRACT_PROMPT,
     DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE,
 )
 from llama_index.prompts.prompts import KnowledgeGraphPrompt
+from llama_index.schema import BaseNode, MetadataMode
+from llama_index.storage.docstore.types import RefDocInfo
+from llama_index.storage.storage_context import StorageContext
+
+# import registery functions
+
 
 DQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE
 
 logger = logging.getLogger(__name__)
 
 
-class GPTKnowledgeGraphIndex(BaseGPTIndex[KG]):
-    """GPT Knowledge Graph Index.
+class KnowledgeGraphIndex(BaseIndex[KG]):
+    """Knowledge Graph Index.
 
     Build a KG by extracting triplets, and leveraging the KG during query-time.
 
     Args:
         kg_triple_extract_template (KnowledgeGraphPrompt): The prompt to use for
             extracting triplets.
         max_triplets_per_chunk (int): The maximum number of triplets to extract.
+        graph_store (Optional[GraphStore]): The graph store to use.
+        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.
 
     """
 
     index_struct_cls = KG
 
     def __init__(
         self,
-        nodes: Optional[Sequence[Node]] = None,
+        nodes: Optional[Sequence[BaseNode]] = None,
         index_struct: Optional[KG] = None,
+        service_context: Optional[ServiceContext] = None,
+        storage_context: Optional[StorageContext] = None,
         kg_triple_extract_template: Optional[KnowledgeGraphPrompt] = None,
         max_triplets_per_chunk: int = 10,
         include_embeddings: bool = False,
+        show_progress: bool = False,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
         # need to set parameters before building index in base class.
         self.include_embeddings = include_embeddings
         self.max_triplets_per_chunk = max_triplets_per_chunk
         self.kg_triple_extract_template = (
@@ -62,162 +77,223 @@
                 max_knowledge_triplets=self.max_triplets_per_chunk
             )
         )
 
         super().__init__(
             nodes=nodes,
             index_struct=index_struct,
+            service_context=service_context,
+            storage_context=storage_context,
+            show_progress=show_progress,
             **kwargs,
         )
 
+        # TODO: legacy conversion - remove in next release
+        if (
+            len(self.index_struct.table) > 0
+            and isinstance(self.graph_store, SimpleGraphStore)
+            and len(self.graph_store._data.graph_dict) == 0
+        ):
+            logger.warning("Upgrading previously saved KG index to new storage format.")
+            self.graph_store._data.graph_dict = self.index_struct.rel_map
+
+    @property
+    def graph_store(self) -> GraphStore:
+        return self._graph_store
+
     def as_retriever(self, **kwargs: Any) -> BaseRetriever:
-        from llama_index.indices.knowledge_graph.retrievers import (
+        from llama_index.indices.knowledge_graph.retriever import (
             KGRetrieverMode,
             KGTableRetriever,
         )
 
         if len(self.index_struct.embedding_dict) > 0 and "retriever_mode" not in kwargs:
             kwargs["retriever_mode"] = KGRetrieverMode.HYBRID
 
         return KGTableRetriever(self, **kwargs)
 
     def _extract_triplets(self, text: str) -> List[Tuple[str, str, str]]:
         """Extract keywords from text."""
-        response, _ = self._service_context.llm_predictor.predict(
+        response = self._service_context.llm_predictor.predict(
             self.kg_triple_extract_template,
             text=text,
         )
         return self._parse_triplet_response(response)
 
     @staticmethod
     def _parse_triplet_response(response: str) -> List[Tuple[str, str, str]]:
         knowledge_strs = response.strip().split("\n")
         results = []
         for text in knowledge_strs:
+            if text == "" or text[0] != "(":
+                # skip empty lines and non-triplets
+                continue
             tokens = text[1:-1].split(",")
             if len(tokens) != 3:
                 continue
             subj, pred, obj = tokens
             results.append((subj.strip(), pred.strip(), obj.strip()))
         return results
 
-    def _build_index_from_nodes(self, nodes: Sequence[Node]) -> KG:
+    def _build_index_from_nodes(self, nodes: Sequence[BaseNode]) -> KG:
         """Build the index from nodes."""
         # do simple concatenation
-        index_struct = KG(table={})
-        for n in nodes:
-            triplets = self._extract_triplets(n.get_text())
+        index_struct = self.index_struct_cls()
+        nodes_with_progress = get_tqdm_iterable(
+            nodes, self._show_progress, "Processing nodes"
+        )
+        for n in nodes_with_progress:
+            triplets = self._extract_triplets(
+                n.get_content(metadata_mode=MetadataMode.LLM)
+            )
             logger.debug(f"> Extracted triplets: {triplets}")
             for triplet in triplets:
                 subj, _, obj = triplet
-                index_struct.upsert_triplet(triplet)
+                self.upsert_triplet(triplet)
                 index_struct.add_node([subj, obj], n)
 
             if self.include_embeddings:
                 for triplet in triplets:
                     self._service_context.embed_model.queue_text_for_embedding(
                         str(triplet), str(triplet)
                     )
 
                 embed_outputs = (
-                    self._service_context.embed_model.get_queued_text_embeddings()
+                    self._service_context.embed_model.get_queued_text_embeddings(
+                        self._show_progress
+                    )
                 )
                 for rel_text, rel_embed in zip(*embed_outputs):
                     index_struct.add_to_embedding_dict(rel_text, rel_embed)
 
         return index_struct
 
-    def _insert(self, nodes: Sequence[Node], **insert_kwargs: Any) -> None:
+    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
         """Insert a document."""
         for n in nodes:
-            triplets = self._extract_triplets(n.get_text())
+            triplets = self._extract_triplets(
+                n.get_content(metadata_mode=MetadataMode.LLM)
+            )
             logger.debug(f"Extracted triplets: {triplets}")
             for triplet in triplets:
                 subj, _, obj = triplet
                 triplet_str = str(triplet)
-                self._index_struct.upsert_triplet(triplet)
+                self.upsert_triplet(triplet)
                 self._index_struct.add_node([subj, obj], n)
                 if (
                     self.include_embeddings
                     and triplet_str not in self._index_struct.embedding_dict
                 ):
                     rel_embedding = (
                         self._service_context.embed_model.get_text_embedding(
                             triplet_str
                         )
                     )
-                    self.index_struct.add_to_embedding_dict(triplet_str, rel_embedding)
+                    self._index_struct.add_to_embedding_dict(triplet_str, rel_embedding)
 
     def upsert_triplet(self, triplet: Tuple[str, str, str]) -> None:
         """Insert triplets.
 
         Used for manual insertion of KG triplets (in the form
         of (subject, relationship, object)).
 
         Args
             triplet (str): Knowledge triplet
 
         """
-        self._index_struct.upsert_triplet(triplet)
+        self._graph_store.upsert_triplet(*triplet)
 
-    def add_node(self, keywords: List[str], node: Node) -> None:
+    def add_node(self, keywords: List[str], node: BaseNode) -> None:
         """Add node.
 
         Used for manual insertion of nodes (keyed by keywords).
 
         Args:
             keywords (List[str]): Keywords to index the node.
             node (Node): Node to be indexed.
 
         """
         self._index_struct.add_node(keywords, node)
         self._docstore.add_documents([node], allow_update=True)
 
     def upsert_triplet_and_node(
-        self, triplet: Tuple[str, str, str], node: Node
+        self, triplet: Tuple[str, str, str], node: BaseNode
     ) -> None:
         """Upsert KG triplet and node.
 
         Calls both upsert_triplet and add_node.
         Behavior is idempotent; if Node already exists,
         only triplet will be added.
 
         Args:
             keywords (List[str]): Keywords to index the node.
             node (Node): Node to be indexed.
 
         """
         subj, _, obj = triplet
-        self._index_struct.add_node([subj, obj], node)
-        self._index_struct.upsert_triplet(triplet)
-        self._docstore.add_documents([node], allow_update=True)
+        self.upsert_triplet(triplet)
+        self.add_node([subj, obj], node)
 
-    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document."""
+    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:
+        """Delete a node."""
         raise NotImplementedError("Delete is not supported for KG index yet.")
 
-    def get_networkx_graph(self) -> Any:
+    @property
+    def ref_doc_info(self) -> Dict[str, RefDocInfo]:
+        """Retrieve a dict mapping of ingested documents and their nodes+metadata."""
+        node_doc_ids_sets = list(self._index_struct.table.values())
+        node_doc_ids = list(set().union(*node_doc_ids_sets))
+        nodes = self.docstore.get_nodes(node_doc_ids)
+
+        all_ref_doc_info = {}
+        for node in nodes:
+            ref_node = node.source_node
+            if not ref_node:
+                continue
+
+            ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)
+            if not ref_doc_info:
+                continue
+
+            all_ref_doc_info[ref_node.node_id] = ref_doc_info
+        return all_ref_doc_info
+
+    def get_networkx_graph(self, limit: int = 100) -> Any:
         """Get networkx representation of the graph structure.
 
+        Args:
+            limit (int): Number of starting nodes to be included in the graph.
+
         NOTE: This function requires networkx to be installed.
         NOTE: This is a beta feature.
 
         """
         try:
             import networkx as nx
         except ImportError:
             raise ImportError(
                 "Please install networkx to visualize the graph: `pip install networkx`"
             )
 
         g = nx.Graph()
-        # add nodes
+        # add nodes with limited number of starting nodes
         for node_name in self.index_struct.table.keys():
+            if limit <= 0:
+                break
             g.add_node(node_name)
+            limit -= 1
 
         # add edges
-        rel_map = self.index_struct.rel_map
+        rel_map = self._graph_store.get_rel_map(list(g.nodes().keys()), 1)
         for keyword in rel_map.keys():
-            for obj, rel in rel_map[keyword]:
-                g.add_edge(keyword, obj, title=rel)
+            for rel, obj in rel_map[keyword]:
+                g.add_edge(keyword, obj, label=rel, title=rel)
 
         return g
+
+    @property
+    def query_context(self) -> Dict[str, Any]:
+        return {GRAPH_STORE_KEY: self._graph_store}
+
+
+# legacy
+GPTKnowledgeGraphIndex = KnowledgeGraphIndex
```

### Comparing `llama_index-0.6.9/llama_index/indices/knowledge_graph/retrievers.py` & `llama_index-0.7.0/llama_index/indices/knowledge_graph/retriever.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,28 +1,27 @@
-"""Query for GPTKGTableIndex."""
+"""KGTable Retriever."""
 import logging
 from collections import defaultdict
 from enum import Enum
 from typing import Any, Dict, List, Optional
 
-from llama_index.data_structs.node import Node, NodeWithScore
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.keyword_table.utils import extract_keywords_given_response
-from llama_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex
-from llama_index.indices.query.embedding_utils import (
-    get_top_k_embeddings,
-)
+from llama_index.indices.knowledge_graph.base import KnowledgeGraphIndex
+from llama_index.indices.query.embedding_utils import get_top_k_embeddings
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.prompts.default_prompts import DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE
 from llama_index.prompts.prompts import QueryKeywordExtractPrompt
+from llama_index.schema import BaseNode, NodeWithScore, TextNode, MetadataMode
 from llama_index.utils import truncate_text
 
-
 DQKET = DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE
 DEFAULT_NODE_SCORE = 1000.0
+GLOBAL_EXPLORE_NODE_LIMIT = 3
+REL_TEXT_LIMIT = 30
 
 logger = logging.getLogger(__name__)
 
 
 class KGRetrieverMode(str, Enum):
     """Query mode enum for Knowledge Graphs.
 
@@ -38,15 +37,15 @@
 
     KEYWORD = "keyword"
     EMBEDDING = "embedding"
     HYBRID = "hybrid"
 
 
 class KGTableRetriever(BaseRetriever):
-    """Base GPT KG Table Index Query.
+    """KG Table Retriever.
 
     Arguments are shared among subclasses.
 
     Args:
         query_keyword_extract_template (Optional[QueryKGExtractPrompt]): A Query
             KG Extraction
             Prompt (see :ref:`Prompt-Templates`).
@@ -59,45 +58,52 @@
         include_text (bool): Use the document text source from each relevant triplet
             during queries.
         retriever_mode (KGRetrieverMode): Specifies whether to use keyowrds,
             embeddings, or both to find relevant triplets. Should be one of "keyword",
             "embedding", or "hybrid".
         similarity_top_k (int): The number of top embeddings to use
             (if embeddings are used).
+        graph_store_query_depth (int): The depth of the graph store query.
     """
 
     def __init__(
         self,
-        index: GPTKnowledgeGraphIndex,
+        index: KnowledgeGraphIndex,
         query_keyword_extract_template: Optional[QueryKeywordExtractPrompt] = None,
         max_keywords_per_query: int = 10,
         num_chunks_per_query: int = 10,
         include_text: bool = True,
         retriever_mode: Optional[KGRetrieverMode] = KGRetrieverMode.KEYWORD,
         similarity_top_k: int = 2,
+        graph_store_query_depth: int = 2,
+        use_global_node_triplets: bool = False,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
 
-        assert isinstance(index, GPTKnowledgeGraphIndex)
+        assert isinstance(index, KnowledgeGraphIndex)
         self._index = index
         self._service_context = self._index.service_context
         self._index_struct = self._index.index_struct
         self._docstore = self._index.docstore
 
         self.max_keywords_per_query = max_keywords_per_query
         self.num_chunks_per_query = num_chunks_per_query
         self.query_keyword_extract_template = query_keyword_extract_template or DQKET
         self.similarity_top_k = similarity_top_k
         self._include_text = include_text
         self._retriever_mode = KGRetrieverMode(retriever_mode)
 
+        self._graph_store = index.graph_store
+        self.graph_store_query_depth = graph_store_query_depth
+        self.use_global_node_triplets = use_global_node_triplets
+
     def _get_keywords(self, query_str: str) -> List[str]:
         """Extract keywords."""
-        response, _ = self._service_context.llm_predictor.predict(
+        response = self._service_context.llm_predictor.predict(
             self.query_keyword_extract_template,
             max_keywords=self.max_keywords_per_query,
             question=query_str,
         )
         keywords = extract_keywords_given_response(
             response, start_token="KEYWORDS:", lowercase=False
         )
@@ -114,29 +120,64 @@
 
     def _retrieve(
         self,
         query_bundle: QueryBundle,
     ) -> List[NodeWithScore]:
         """Get nodes for response."""
         logger.info(f"> Starting query: {query_bundle.query_str}")
+        node_visited = set()
         keywords = self._get_keywords(query_bundle.query_str)
         logger.info(f"> Query keywords: {keywords}")
         rel_texts = []
         cur_rel_map = {}
         chunk_indices_count: Dict[str, int] = defaultdict(int)
-
         if self._retriever_mode != KGRetrieverMode.EMBEDDING:
             for keyword in keywords:
-                cur_rel_texts = self._index_struct.get_rel_map_texts(keyword)
-                rel_texts.extend(cur_rel_texts)
-                cur_rel_map[keyword] = self._index_struct.get_rel_map_tuples(keyword)
-                if self._include_text:
-                    for node_id in self._index_struct.get_node_ids(keyword):
+                subjs = set((keyword,))
+                node_ids = self._index_struct.search_node_by_keyword(keyword)
+                for node_id in node_ids[:GLOBAL_EXPLORE_NODE_LIMIT]:
+                    if node_id in node_visited:
+                        continue
+
+                    if self._include_text:
                         chunk_indices_count[node_id] += 1
 
+                    node_visited.add(node_id)
+                    if self.use_global_node_triplets:
+                        # Get nodes from keyword search, and add them to the subjs
+                        # set. This helps introduce more global knowledge into the
+                        # query. While it's more expensive, thus to be turned off
+                        # by default, it can be useful for some applications.
+
+                        # TODO: we should a keyword-node_id map in IndexStruct, so that
+                        # node-keywords extraction with LLM will be called only once
+                        # during indexing.
+                        extended_subjs = self._get_keywords(
+                            self._docstore.get_node(node_id).get_content(
+                                metadata_mode=MetadataMode.LLM
+                            )
+                        )
+                        subjs.update(extended_subjs)
+
+                rel_map = self._graph_store.get_rel_map(
+                    list(subjs), self.graph_store_query_depth
+                )
+                logger.debug(f"rel_map: {rel_map}")
+
+                if not rel_map:
+                    continue
+                rel_texts.extend(
+                    [
+                        f"{sub} {rel_obj}"
+                        for sub, rel_objs in rel_map.items()
+                        for rel_obj in rel_objs
+                    ]
+                )
+                cur_rel_map.update(rel_map)
+
         if (
             self._retriever_mode != KGRetrieverMode.KEYWORD
             and len(self._index_struct.embedding_dict) > 0
         ):
             query_embedding = self._service_context.embed_model.get_text_embedding(
                 query_bundle.query_str
             )
@@ -155,77 +196,97 @@
                 f"Found the following rel_texts+query similarites: {str(similarities)}"
             )
             logger.debug(f"Found the following top_k rel_texts: {str(rel_texts)}")
             rel_texts.extend(top_rel_texts)
             if self._include_text:
                 keywords = self._extract_rel_text_keywords(top_rel_texts)
                 nested_node_ids = [
-                    self._index_struct.get_node_ids(keyword) for keyword in keywords
+                    self._index_struct.search_node_by_keyword(keyword)
+                    for keyword in keywords
                 ]
-                # flatten list
                 node_ids = [_id for ids in nested_node_ids for _id in ids]
                 for node_id in node_ids:
                     chunk_indices_count[node_id] += 1
         elif len(self._index_struct.embedding_dict) == 0:
             logger.error(
                 "Index was not constructed with embeddings, skipping embedding usage..."
             )
 
         # remove any duplicates from keyword + embedding queries
         if self._retriever_mode == KGRetrieverMode.HYBRID:
             rel_texts = list(set(rel_texts))
 
+            # remove shorter rel_texts that are substrings of longer rel_texts
+            rel_texts.sort(key=len, reverse=True)
+            for i in range(len(rel_texts)):
+                for j in range(i + 1, len(rel_texts)):
+                    if rel_texts[j] in rel_texts[i]:
+                        rel_texts[j] = ""
+            rel_texts = [rel_text for rel_text in rel_texts if rel_text != ""]
+
+            # truncate rel_texts to REL_TEXT_LIMIT
+            rel_texts = rel_texts[:REL_TEXT_LIMIT]
+
         sorted_chunk_indices = sorted(
             list(chunk_indices_count.keys()),
             key=lambda x: chunk_indices_count[x],
             reverse=True,
         )
         sorted_chunk_indices = sorted_chunk_indices[: self.num_chunks_per_query]
         sorted_nodes = self._docstore.get_nodes(sorted_chunk_indices)
 
         # TMP/TODO: also filter rel_texts as nodes until we figure out better
         # abstraction
         # TODO(suo): figure out what this does
         # rel_text_nodes = [Node(text=rel_text) for rel_text in rel_texts]
         # for node_processor in self._node_postprocessors:
         #     rel_text_nodes = node_processor.postprocess_nodes(rel_text_nodes)
-        # rel_texts = [node.get_text() for node in rel_text_nodes]
+        # rel_texts = [node.get_content() for node in rel_text_nodes]
 
         sorted_nodes_with_scores = []
         for chunk_idx, node in zip(sorted_chunk_indices, sorted_nodes):
             # nodes are found with keyword mapping, give high conf to avoid cutoff
-            sorted_nodes_with_scores.append(NodeWithScore(node, DEFAULT_NODE_SCORE))
+            sorted_nodes_with_scores.append(
+                NodeWithScore(node=node, score=DEFAULT_NODE_SCORE)
+            )
             logger.info(
                 f"> Querying with idx: {chunk_idx}: "
-                f"{truncate_text(node.get_text(), 80)}"
+                f"{truncate_text(node.get_content(), 80)}"
             )
 
         # add relationships as Node
         # TODO: make initial text customizable
         rel_initial_text = (
-            "The following are knowledge triplets "
-            "in the form of (subset, predicate, object):"
+            f"The following are knowledge triplets in max depth"
+            f" {self.graph_store_query_depth} "
+            f"in the form of "
+            f"`subject [predicate, object, predicate_next_hop, object_next_hop ...]`"
         )
         rel_info = [rel_initial_text] + rel_texts
         rel_node_info = {
             "kg_rel_texts": rel_texts,
             "kg_rel_map": cur_rel_map,
         }
-        rel_text_node = Node(text="\n".join(rel_info), node_info=rel_node_info)
+        rel_text_node = TextNode(
+            text="\n".join(rel_info),
+            metadata=rel_node_info,
+            excluded_embed_metadata_keys=["kg_rel_map", "kg_rel_texts"],
+            excluded_llm_metadata_keys=["kg_rel_map", "kg_rel_texts"],
+        )
         # this node is constructed from rel_texts, give high confidence to avoid cutoff
         sorted_nodes_with_scores.append(
-            NodeWithScore(rel_text_node, DEFAULT_NODE_SCORE)
+            NodeWithScore(node=rel_text_node, score=DEFAULT_NODE_SCORE)
         )
         rel_info_text = "\n".join(rel_info)
         logger.info(f"> Extracted relationships: {rel_info_text}")
 
         return sorted_nodes_with_scores
 
-    def _get_extra_info_for_response(
-        self, nodes: List[Node]
+    def _get_metadata_for_response(
+        self, nodes: List[BaseNode]
     ) -> Optional[Dict[str, Any]]:
-        """Get extra info for response."""
+        """Get metadata for response."""
         for node in nodes:
-            if node.node_info is None or "kg_rel_map" not in node.node_info:
+            if node.metadata is None or "kg_rel_map" not in node.metadata:
                 continue
-            return node.node_info
+            return node.metadata
         raise ValueError("kg_rel_map must be found in at least one Node.")
```

### Comparing `llama_index-0.6.9/llama_index/indices/list/base.py` & `llama_index-0.7.0/llama_index/indices/list/base.py`

 * *Files 25% similar despite different names*

```diff
@@ -2,103 +2,135 @@
 
 A simple data structure where LlamaIndex iterates through document chunks
 in sequence in order to answer a given query.
 
 """
 
 from enum import Enum
-from typing import Any, Optional, Sequence, Union
+from typing import Any, Dict, Optional, Sequence, Union
+from llama_index.utils import get_tqdm_iterable
 
 from llama_index.data_structs.data_structs import IndexList
-from llama_index.data_structs.node import Node
-from llama_index.indices.base import BaseGPTIndex
+from llama_index.indices.base import BaseIndex
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.service_context import ServiceContext
+from llama_index.schema import BaseNode
+from llama_index.storage.docstore.types import RefDocInfo
 
 
 class ListRetrieverMode(str, Enum):
     DEFAULT = "default"
     EMBEDDING = "embedding"
     LLM = "llm"
 
 
-class GPTListIndex(BaseGPTIndex[IndexList]):
-    """GPT List Index.
+class ListIndex(BaseIndex[IndexList]):
+    """List Index.
 
     The list index is a simple data structure where nodes are stored in
     a sequence. During index construction, the document texts are
     chunked up, converted to nodes, and stored in a list.
 
     During query time, the list index iterates through the nodes
     with some optional filter parameters, and synthesizes an
     answer from all the nodes.
 
     Args:
         text_qa_template (Optional[QuestionAnswerPrompt]): A Question-Answer Prompt
             (see :ref:`Prompt-Templates`).
             NOTE: this is a deprecated field.
+        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.
 
     """
 
     index_struct_cls = IndexList
 
     def __init__(
         self,
-        nodes: Optional[Sequence[Node]] = None,
+        nodes: Optional[Sequence[BaseNode]] = None,
         index_struct: Optional[IndexList] = None,
         service_context: Optional[ServiceContext] = None,
+        show_progress: bool = False,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
         super().__init__(
             nodes=nodes,
             index_struct=index_struct,
             service_context=service_context,
+            show_progress=show_progress,
             **kwargs,
         )
 
     def as_retriever(
         self,
         retriever_mode: Union[str, ListRetrieverMode] = ListRetrieverMode.DEFAULT,
         **kwargs: Any,
     ) -> BaseRetriever:
         from llama_index.indices.list.retrievers import (
             ListIndexEmbeddingRetriever,
-            ListIndexRetriever,
             ListIndexLLMRetriever,
+            ListIndexRetriever,
         )
 
         if retriever_mode == ListRetrieverMode.DEFAULT:
             return ListIndexRetriever(self, **kwargs)
         elif retriever_mode == ListRetrieverMode.EMBEDDING:
             return ListIndexEmbeddingRetriever(self, **kwargs)
         elif retriever_mode == ListRetrieverMode.LLM:
             return ListIndexLLMRetriever(self, **kwargs)
         else:
             raise ValueError(f"Unknown retriever mode: {retriever_mode}")
 
-    def _build_index_from_nodes(self, nodes: Sequence[Node]) -> IndexList:
+    def _build_index_from_nodes(
+        self, nodes: Sequence[BaseNode], show_progress: bool = False
+    ) -> IndexList:
         """Build the index from documents.
 
         Args:
             documents (List[BaseDocument]): A list of documents.
 
         Returns:
             IndexList: The created list index.
         """
         index_struct = IndexList()
-        for n in nodes:
+        nodes_with_progress = get_tqdm_iterable(
+            nodes, show_progress, "Processing nodes"
+        )
+        for n in nodes_with_progress:
             index_struct.add_node(n)
         return index_struct
 
-    def _insert(self, nodes: Sequence[Node], **insert_kwargs: Any) -> None:
+    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
         """Insert a document."""
         for n in nodes:
-            # print("inserting node to index struct: ", n.get_doc_id())
             self._index_struct.add_node(n)
 
-    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document."""
+    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:
+        """Delete a node."""
         cur_node_ids = self._index_struct.nodes
         cur_nodes = self._docstore.get_nodes(cur_node_ids)
-        nodes_to_keep = [n for n in cur_nodes if n.ref_doc_id != doc_id]
-        self._index_struct.nodes = [n.get_doc_id() for n in nodes_to_keep]
+        nodes_to_keep = [n for n in cur_nodes if n.node_id != node_id]
+        self._index_struct.nodes = [n.node_id for n in nodes_to_keep]
+
+    @property
+    def ref_doc_info(self) -> Dict[str, RefDocInfo]:
+        """Retrieve a dict mapping of ingested documents and their nodes+metadata."""
+        node_doc_ids = self._index_struct.nodes
+        nodes = self.docstore.get_nodes(node_doc_ids)
+
+        all_ref_doc_info = {}
+        for node in nodes:
+            ref_node = node.source_node
+            if not ref_node:
+                continue
+
+            ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)
+            if not ref_doc_info:
+                continue
+
+            all_ref_doc_info[ref_node.node_id] = ref_doc_info
+        return all_ref_doc_info
+
+
+# Legacy
+GPTListIndex = ListIndex
```

### Comparing `llama_index-0.6.9/llama_index/indices/list/retrievers.py` & `llama_index-0.7.0/llama_index/indices/list/retrievers.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,69 +1,67 @@
-"""Default query for GPTListIndex."""
+"""Retrievers for ListIndex."""
 import logging
-from typing import Any, List, Optional, Tuple, Callable
+from typing import Any, Callable, List, Optional, Tuple
 
-from llama_index.data_structs.node import Node, NodeWithScore
 from llama_index.indices.base_retriever import BaseRetriever
-from llama_index.indices.query.embedding_utils import (
-    get_top_k_embeddings,
-)
+from llama_index.indices.list.base import ListIndex
+from llama_index.indices.query.embedding_utils import get_top_k_embeddings
 from llama_index.indices.query.schema import QueryBundle
-from llama_index.indices.list.base import GPTListIndex
-from llama_index.prompts.choice_select import (
-    DEFAULT_CHOICE_SELECT_PROMPT,
-    ChoiceSelectPrompt,
-)
+from llama_index.indices.service_context import ServiceContext
 from llama_index.indices.utils import (
     default_format_node_batch_fn,
     default_parse_choice_select_answer_fn,
 )
-from llama_index.indices.service_context import ServiceContext
+from llama_index.prompts.choice_select import (
+    DEFAULT_CHOICE_SELECT_PROMPT,
+    ChoiceSelectPrompt,
+)
+from llama_index.schema import BaseNode, NodeWithScore, MetadataMode
 
 logger = logging.getLogger(__name__)
 
 
 class ListIndexRetriever(BaseRetriever):
     """Simple retriever for ListIndex that returns all nodes.
 
     Args:
-        index (GPTListIndex): The index to retrieve from.
+        index (ListIndex): The index to retrieve from.
 
     """
 
-    def __init__(self, index: GPTListIndex, **kwargs: Any) -> None:
+    def __init__(self, index: ListIndex, **kwargs: Any) -> None:
         self._index = index
 
     def _retrieve(
         self,
         query_bundle: QueryBundle,
     ) -> List[NodeWithScore]:
         """Retrieve nodes."""
         del query_bundle
 
         node_ids = self._index.index_struct.nodes
         nodes = self._index.docstore.get_nodes(node_ids)
-        return [NodeWithScore(node) for node in nodes]
+        return [NodeWithScore(node=node) for node in nodes]
 
 
 class ListIndexEmbeddingRetriever(BaseRetriever):
     """Embedding based retriever for ListIndex.
 
     Generates embeddings in a lazy fashion for all
     nodes that are traversed.
 
     Args:
-        index (GPTListIndex): The index to retrieve from.
+        index (ListIndex): The index to retrieve from.
         similarity_top_k (Optional[int]): The number of top nodes to return.
 
     """
 
     def __init__(
         self,
-        index: GPTListIndex,
+        index: ListIndex,
         similarity_top_k: Optional[int] = 1,
         **kwargs: Any,
     ) -> None:
         self._index = index
         self._similarity_top_k = similarity_top_k
 
     def _retrieve(
@@ -83,23 +81,23 @@
             embedding_ids=list(range(len(nodes))),
         )
 
         top_k_nodes = [nodes[i] for i in top_idxs]
 
         node_with_scores = []
         for node, similarity in zip(top_k_nodes, top_similarities):
-            node_with_scores.append(NodeWithScore(node, score=similarity))
+            node_with_scores.append(NodeWithScore(node=node, score=similarity))
 
         logger.debug(f"> Top {len(top_idxs)} nodes:\n")
         nl = "\n"
-        logger.debug(f"{ nl.join([n.get_text() for n in top_k_nodes]) }")
+        logger.debug(f"{ nl.join([n.get_content() for n in top_k_nodes]) }")
         return node_with_scores
 
     def _get_embeddings(
-        self, query_bundle: QueryBundle, nodes: List[Node]
+        self, query_bundle: QueryBundle, nodes: List[BaseNode]
     ) -> Tuple[List[float], List[List[float]]]:
         """Get top nodes by similarity to the query."""
         if query_bundle.embedding is None:
             query_bundle.embedding = (
                 self._index._service_context.embed_model.get_agg_embedding_from_queries(
                     query_bundle.embedding_strs
                 )
@@ -108,41 +106,41 @@
         node_embeddings: List[List[float]] = []
         nodes_embedded = 0
         for node in nodes:
             if node.embedding is None:
                 nodes_embedded += 1
                 node.embedding = (
                     self._index.service_context.embed_model.get_text_embedding(
-                        node.get_text()
+                        node.get_content(metadata_mode=MetadataMode.EMBED)
                     )
                 )
 
             node_embeddings.append(node.embedding)
         return query_bundle.embedding, node_embeddings
 
 
 class ListIndexLLMRetriever(BaseRetriever):
     """LLM retriever for ListIndex.
 
     Args:
-        index (GPTListIndex): The index to retrieve from.
+        index (ListIndex): The index to retrieve from.
         choice_select_prompt (Optional[ChoiceSelectPrompt]): A Choice-Select Prompt
            (see :ref:`Prompt-Templates`).)
         choice_batch_size (int): The number of nodes to query at a time.
         format_node_batch_fn (Optional[Callable]): A function that formats a
             batch of nodes.
         parse_choice_select_answer_fn (Optional[Callable]): A function that parses the
             choice select answer.
         service_context (Optional[ServiceContext]): A service context.
 
     """
 
     def __init__(
         self,
-        index: GPTListIndex,
+        index: ListIndex,
         choice_select_prompt: Optional[ChoiceSelectPrompt] = None,
         choice_batch_size: int = 10,
         format_node_batch_fn: Optional[Callable] = None,
         parse_choice_select_answer_fn: Optional[Callable] = None,
         service_context: Optional[ServiceContext] = None,
         **kwargs: Any,
     ) -> None:
@@ -166,15 +164,15 @@
         for idx in range(0, len(node_ids), self._choice_batch_size):
             node_ids_batch = node_ids[idx : idx + self._choice_batch_size]
             nodes_batch = self._index.docstore.get_nodes(node_ids_batch)
 
             query_str = query_bundle.query_str
             fmt_batch_str = self._format_node_batch_fn(nodes_batch)
             # call each batch independently
-            raw_response, _ = self._service_context.llm_predictor.predict(
+            raw_response = self._service_context.llm_predictor.predict(
                 self._choice_select_prompt,
                 context_str=fmt_batch_str,
                 query_str=query_str,
             )
 
             raw_choices, relevances = self._parse_choice_select_answer_fn(
                 raw_response, len(nodes_batch)
@@ -182,12 +180,12 @@
             choice_idxs = [int(choice) - 1 for choice in raw_choices]
             choice_node_ids = [node_ids_batch[idx] for idx in choice_idxs]
 
             choice_nodes = self._index.docstore.get_nodes(choice_node_ids)
             relevances = relevances or [1.0 for _ in choice_nodes]
             results.extend(
                 [
-                    NodeWithScore(node, score=relevance)
+                    NodeWithScore(node=node, score=relevance)
                     for node, relevance in zip(choice_nodes, relevances)
                 ]
             )
         return results
```

### Comparing `llama_index-0.6.9/llama_index/indices/loading.py` & `llama_index-0.7.0/llama_index/indices/loading.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 from typing import Any, List, Optional, Sequence
-from llama_index.indices.base import BaseGPTIndex
+from llama_index.indices.base import BaseIndex
 from llama_index.indices.composability.graph import ComposableGraph
 from llama_index.indices.registry import INDEX_STRUCT_TYPE_TO_INDEX_CLASS
 from llama_index.storage.storage_context import StorageContext
 
 import logging
 
 logger = logging.getLogger(__name__)
 
 
 def load_index_from_storage(
     storage_context: StorageContext,
     index_id: Optional[str] = None,
     **kwargs: Any,
-) -> BaseGPTIndex:
+) -> BaseIndex:
     """Load index from storage context.
 
     Args:
         storage_context (StorageContext): storage context containing
             docstore, index store and vector store.
         index_id (Optional[str]): ID of the index to load.
             Defaults to None, which assumes there's only a single index
@@ -45,15 +45,15 @@
     return indices[0]
 
 
 def load_indices_from_storage(
     storage_context: StorageContext,
     index_ids: Optional[Sequence[str]] = None,
     **kwargs: Any,
-) -> List[BaseGPTIndex]:
+) -> List[BaseIndex]:
     """Load multiple indices from storage context
 
     Args:
         storage_context (StorageContext): storage context containing
             docstore, index store and vector store.
         index_id (Optional[Sequence[str]]): IDs of the indices to load.
             Defaults to None, which loads all indices in the index store.
```

### Comparing `llama_index-0.6.9/llama_index/indices/postprocessor/__init__.py` & `llama_index-0.7.0/llama_index/indices/postprocessor/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,23 +13,25 @@
     TimeWeightedPostprocessor,
 )
 from llama_index.indices.postprocessor.pii import (
     PIINodePostprocessor,
     NERPIINodePostprocessor,
 )
 from llama_index.indices.postprocessor.llm_rerank import LLMRerank
-
 from llama_index.indices.postprocessor.cohere_rerank import CohereRerank
+from llama_index.indices.postprocessor.optimizer import SentenceEmbeddingOptimizer
+
 
 __all__ = [
     "SimilarityPostprocessor",
     "KeywordNodePostprocessor",
     "PrevNextNodePostprocessor",
     "AutoPrevNextNodePostprocessor",
     "FixedRecencyPostprocessor",
     "EmbeddingRecencyPostprocessor",
     "TimeWeightedPostprocessor",
     "PIINodePostprocessor",
     "NERPIINodePostprocessor",
     "CohereRerank",
     "LLMRerank",
+    "SentenceEmbeddingOptimizer",
 ]
```

### Comparing `llama_index-0.6.9/llama_index/indices/postprocessor/cohere_rerank.py` & `llama_index-0.7.0/llama_index/indices/postprocessor/cohere_rerank.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import os
 from typing import List, Optional
-from llama_index.data_structs.node import NodeWithScore
 from llama_index.indices.postprocessor.types import BaseNodePostprocessor
 from llama_index.indices.query.schema import QueryBundle
+from llama_index.schema import NodeWithScore
 
 
 class CohereRerank(BaseNodePostprocessor):
     def __init__(
         self,
         top_n: int = 2,
         model: str = "rerank-english-v2.0",
@@ -34,22 +34,22 @@
         self,
         nodes: List[NodeWithScore],
         query_bundle: Optional[QueryBundle] = None,
     ) -> List[NodeWithScore]:
         if query_bundle is None:
             raise ValueError("Missing query bundle in extra info.")
 
-        texts = [node.node.get_text() for node in nodes]
+        texts = [node.node.get_content() for node in nodes]
         results = self._client.rerank(
             model=self._model,
             top_n=self._top_n,
             query=query_bundle.query_str,
             documents=texts,
         )
 
         new_nodes = []
         for result in results:
             new_node_with_score = NodeWithScore(
-                nodes[result.index].node, result.relevance_score
+                node=nodes[result.index].node, score=result.relevance_score
             )
             new_nodes.append(new_node_with_score)
         return new_nodes
```

### Comparing `llama_index-0.6.9/llama_index/indices/postprocessor/llm_rerank.py` & `llama_index-0.7.0/llama_index/indices/postprocessor/llm_rerank.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 """LLM reranker."""
-from typing import List, Optional, Callable
-from llama_index.data_structs.node import NodeWithScore
+from typing import Callable, List, Optional
+
 from llama_index.indices.postprocessor.types import BaseNodePostprocessor
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
-from llama_index.prompts.default_choice_select import DEFAULT_CHOICE_SELECT_PROMPT
 from llama_index.indices.utils import (
     default_format_node_batch_fn,
     default_parse_choice_select_answer_fn,
 )
-
+from llama_index.prompts.choice_select import DEFAULT_CHOICE_SELECT_PROMPT
 from llama_index.prompts.prompts import QuestionAnswerPrompt
+from llama_index.schema import NodeWithScore
 
 
 class LLMRerank(BaseNodePostprocessor):
     """LLM-based reranker."""
 
     def __init__(
         self,
@@ -50,29 +50,29 @@
             nodes_batch = [
                 node.node for node in nodes[idx : idx + self._choice_batch_size]
             ]
 
             query_str = query_bundle.query_str
             fmt_batch_str = self._format_node_batch_fn(nodes_batch)
             # call each batch independently
-            raw_response, _ = self._service_context.llm_predictor.predict(
+            raw_response = self._service_context.llm_predictor.predict(
                 self._choice_select_prompt,
                 context_str=fmt_batch_str,
                 query_str=query_str,
             )
 
             raw_choices, relevances = self._parse_choice_select_answer_fn(
                 raw_response, len(nodes_batch)
             )
             choice_idxs = [int(choice) - 1 for choice in raw_choices]
             choice_nodes = [nodes_batch[idx] for idx in choice_idxs]
             relevances = relevances or [1.0 for _ in choice_nodes]
             initial_results.extend(
                 [
-                    NodeWithScore(node, score=relevance)
+                    NodeWithScore(node=node, score=relevance)
                     for node, relevance in zip(choice_nodes, relevances)
                 ]
             )
 
         results = sorted(initial_results, key=lambda x: x.score or 0.0, reverse=True)[
             : self._top_n
         ]
```

### Comparing `llama_index-0.6.9/llama_index/indices/postprocessor/node.py` & `llama_index-0.7.0/llama_index/indices/postprocessor/node.py`

 * *Files 13% similar despite different names*

```diff
@@ -3,21 +3,20 @@
 import logging
 import re
 from abc import abstractmethod
 from typing import Dict, List, Optional, cast
 
 from pydantic import BaseModel, Field, validator
 
-from llama_index.data_structs.node import DocumentRelationship, NodeWithScore
 from llama_index.indices.postprocessor.types import BaseNodePostprocessor
 from llama_index.indices.query.schema import QueryBundle
-from llama_index.indices.response import get_response_builder
-from llama_index.indices.response.type import ResponseMode
 from llama_index.indices.service_context import ServiceContext
 from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt
+from llama_index.response_synthesizers import ResponseMode, get_response_synthesizer
+from llama_index.schema import NodeRelationship, NodeWithScore
 from llama_index.storage.docstore import BaseDocumentStore
 
 logger = logging.getLogger(__name__)
 
 
 class BasePydanticNodePostprocessor(BaseModel, BaseNodePostprocessor):
     """Node postprocessor."""
@@ -49,22 +48,22 @@
         new_nodes = []
         for node_with_score in nodes:
             node = node_with_score.node
             should_use_node = True
             if self.required_keywords is not None:
                 for keyword in self.required_keywords:
                     pattern = r"\b" + re.escape(keyword) + r"\b"
-                    keyword_presence = re.search(pattern, node.get_text())
+                    keyword_presence = re.search(pattern, node.get_content())
                     if not keyword_presence:
                         should_use_node = False
 
             if self.exclude_keywords is not None:
                 for keyword in self.exclude_keywords:
                     pattern = r"\b" + re.escape(keyword) + r"\b"
-                    keyword_presence = re.search(keyword, node.get_text())
+                    keyword_presence = re.search(keyword, node.get_content())
                     if keyword_presence:
                         should_use_node = False
 
             if should_use_node:
                 new_nodes.append(node_with_score)
 
         return new_nodes
@@ -100,46 +99,50 @@
 
 
 def get_forward_nodes(
     node_with_score: NodeWithScore, num_nodes: int, docstore: BaseDocumentStore
 ) -> Dict[str, NodeWithScore]:
     """Get forward nodes."""
     node = node_with_score.node
-    nodes: Dict[str, NodeWithScore] = {node.get_doc_id(): node_with_score}
+    nodes: Dict[str, NodeWithScore] = {node.node_id: node_with_score}
     cur_count = 0
     # get forward nodes in an iterative manner
     while cur_count < num_nodes:
-        if DocumentRelationship.NEXT not in node.relationships:
+        if NodeRelationship.NEXT not in node.relationships:
             break
-        next_node_id = node.relationships[DocumentRelationship.NEXT]
-        next_node = docstore.get_node(next_node_id)
-        if next_node is None:
+
+        next_node_info = node.next_node
+        if next_node_info is None:
             break
-        nodes[next_node.get_doc_id()] = NodeWithScore(next_node)
+
+        next_node_id = next_node_info.node_id
+        next_node = docstore.get_node(next_node_id)
+        nodes[next_node.node_id] = NodeWithScore(node=next_node)
         node = next_node
         cur_count += 1
     return nodes
 
 
 def get_backward_nodes(
     node_with_score: NodeWithScore, num_nodes: int, docstore: BaseDocumentStore
 ) -> Dict[str, NodeWithScore]:
     """Get backward nodes."""
     node = node_with_score.node
     # get backward nodes in an iterative manner
-    nodes: Dict[str, NodeWithScore] = {node.get_doc_id(): node_with_score}
+    nodes: Dict[str, NodeWithScore] = {node.node_id: node_with_score}
     cur_count = 0
     while cur_count < num_nodes:
-        if DocumentRelationship.PREVIOUS not in node.relationships:
+        prev_node_info = node.prev_node
+        if prev_node_info is None:
             break
-        prev_node_id = node.relationships[DocumentRelationship.PREVIOUS]
+        prev_node_id = prev_node_info.node_id
         prev_node = docstore.get_node(prev_node_id)
         if prev_node is None:
             break
-        nodes[prev_node.get_doc_id()] = NodeWithScore(prev_node)
+        nodes[prev_node.node_id] = NodeWithScore(node=prev_node)
         node = prev_node
         cur_count += 1
     return nodes
 
 
 class PrevNextNodePostprocessor(BasePydanticNodePostprocessor):
     """Previous/Next Node post-processor.
@@ -172,31 +175,53 @@
         self,
         nodes: List[NodeWithScore],
         query_bundle: Optional[QueryBundle] = None,
     ) -> List[NodeWithScore]:
         """Postprocess nodes."""
         all_nodes: Dict[str, NodeWithScore] = {}
         for node in nodes:
-            all_nodes[node.node.get_doc_id()] = node
+            all_nodes[node.node.node_id] = node
             if self.mode == "next":
                 all_nodes.update(get_forward_nodes(node, self.num_nodes, self.docstore))
             elif self.mode == "previous":
                 all_nodes.update(
                     get_backward_nodes(node, self.num_nodes, self.docstore)
                 )
             elif self.mode == "both":
                 all_nodes.update(get_forward_nodes(node, self.num_nodes, self.docstore))
                 all_nodes.update(
                     get_backward_nodes(node, self.num_nodes, self.docstore)
                 )
             else:
                 raise ValueError(f"Invalid mode: {self.mode}")
 
-        sorted_nodes = sorted(all_nodes.values(), key=lambda x: x.node.get_doc_id())
-        return list(sorted_nodes)
+        all_nodes_values: List[NodeWithScore] = list(all_nodes.values())
+        sorted_nodes: List[NodeWithScore] = list()
+        for node in all_nodes_values:
+            # variable to check if cand node is inserted
+            node_inserted = False
+            for i, cand in enumerate(sorted_nodes):
+                node_id = node.node.node_id
+                # prepend to current candidate
+                prev_node_info = cand.node.prev_node
+                next_node_info = cand.node.next_node
+                if prev_node_info is not None and node_id == prev_node_info.node_id:
+                    node_inserted = True
+                    sorted_nodes.insert(i, node)
+                    break
+                # append to current candidate
+                elif next_node_info is not None and node_id == next_node_info.node_id:
+                    node_inserted = True
+                    sorted_nodes.insert(i + 1, node)
+                    break
+
+            if not node_inserted:
+                sorted_nodes.append(node)
+
+        return sorted_nodes
 
 
 DEFAULT_INFER_PREV_NEXT_TMPL = (
     "The current context information is provided. \n"
     "A question is also provided. \n"
     "You are a retrieval agent deciding whether to search the "
     "document store for additional prior context or future context. \n"
@@ -288,25 +313,25 @@
         infer_prev_next_prompt = QuestionAnswerPrompt(
             self.infer_prev_next_tmpl,
         )
         refine_infer_prev_next_prompt = RefinePrompt(self.refine_prev_next_tmpl)
 
         all_nodes: Dict[str, NodeWithScore] = {}
         for node in nodes:
-            all_nodes[node.node.get_doc_id()] = node
+            all_nodes[node.node.node_id] = node
             # use response builder instead of llm_predictor directly
             # to be more robust to handling long context
-            response_builder = get_response_builder(
+            response_builder = get_response_synthesizer(
                 service_context=self.service_context,
                 text_qa_template=infer_prev_next_prompt,
                 refine_template=refine_infer_prev_next_prompt,
-                mode=ResponseMode.TREE_SUMMARIZE,
+                response_mode=ResponseMode.TREE_SUMMARIZE,
             )
             raw_pred = response_builder.get_response(
-                text_chunks=[node.node.get_text()],
+                text_chunks=[node.node.get_content()],
                 query_str=query_bundle.query_str,
             )
             raw_pred = cast(str, raw_pred)
             mode = self._parse_prediction(raw_pred)
 
             logger.debug(f"> Postprocessor Predicted mode: {mode}")
             if self.verbose:
@@ -319,9 +344,9 @@
                     get_backward_nodes(node, self.num_nodes, self.docstore)
                 )
             elif mode == "none":
                 pass
             else:
                 raise ValueError(f"Invalid mode: {mode}")
 
-        sorted_nodes = sorted(all_nodes.values(), key=lambda x: x.node.get_doc_id())
+        sorted_nodes = sorted(all_nodes.values(), key=lambda x: x.node.node_id)
         return list(sorted_nodes)
```

### Comparing `llama_index-0.6.9/llama_index/indices/postprocessor/node_recency.py` & `llama_index-0.7.0/llama_index/indices/postprocessor/node_recency.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 """Node recency post-processor."""
-
-from llama_index.indices.postprocessor.node import BasePydanticNodePostprocessor
-from llama_index.indices.query.schema import QueryBundle
-from llama_index.indices.service_context import ServiceContext
-from llama_index.data_structs.node import NodeWithScore
 from pydantic import Field
 from typing import Optional, List, Set
 import pandas as pd
 import numpy as np
 from datetime import datetime
 
+from llama_index.indices.postprocessor.node import BasePydanticNodePostprocessor
+from llama_index.indices.query.schema import QueryBundle
+from llama_index.indices.service_context import ServiceContext
+from llama_index.schema import NodeWithScore, MetadataMode
+
 
 # NOTE: currently not being used
 # DEFAULT_INFER_RECENCY_TMPL = (
 #     "A question is provided.\n"
 #     "The goal is to determine whether the question requires finding the most recent "
 #     "context.\n"
 #     "Please respond with YES or NO.\n"
@@ -52,42 +52,39 @@
 
     """
 
     service_context: ServiceContext
     top_k: int = 1
     # infer_recency_tmpl: str = Field(default=DEFAULT_INFER_RECENCY_TMPL)
     date_key: str = "date"
-    # if false, then search node info
-    in_extra_info: bool = True
 
     def postprocess_nodes(
         self,
         nodes: List[NodeWithScore],
         query_bundle: Optional[QueryBundle] = None,
     ) -> List[NodeWithScore]:
         """Postprocess nodes."""
 
         if query_bundle is None:
             raise ValueError("Missing query bundle in extra info.")
 
-        # query_bundle = cast(QueryBundle, extra_info["query_bundle"])
+        # query_bundle = cast(QueryBundle, metadata["query_bundle"])
         # infer_recency_prompt = SimpleInputPrompt(self.infer_recency_tmpl)
-        # raw_pred, _ = self.service_context.llm_predictor.predict(
+        # raw_pred = self.service_context.llm_predictor.predict(
         #     prompt=infer_recency_prompt,
         #     query_str=query_bundle.query_str,
         # )
         # pred = parse_recency_pred(raw_pred)
         # # if no need to use recency post-processor, return nodes as is
         # if not pred:
         #     return nodes
 
         # sort nodes by date
-        info_dict_attr = "extra_info" if self.in_extra_info else "node_info"
         node_dates = pd.to_datetime(
-            [getattr(node.node, info_dict_attr)[self.date_key] for node in nodes]
+            [node.node.metadata[self.date_key] for node in nodes]
         )
         sorted_node_idxs = np.flip(node_dates.argsort())
         sorted_nodes = [nodes[idx] for idx in sorted_node_idxs]
 
         return sorted_nodes[: self.top_k]
 
 
@@ -116,83 +113,79 @@
       Because this means the subsequent node may have overlapping content
       with the current node but is also out of date
     """
 
     service_context: ServiceContext
     # infer_recency_tmpl: str = Field(default=DEFAULT_INFER_RECENCY_TMPL)
     date_key: str = "date"
-    # if false, then search node info
-    in_extra_info: bool = True
     similarity_cutoff: float = Field(default=0.7)
     query_embedding_tmpl: str = Field(default=DEFAULT_QUERY_EMBEDDING_TMPL)
 
     def postprocess_nodes(
         self,
         nodes: List[NodeWithScore],
         query_bundle: Optional[QueryBundle] = None,
     ) -> List[NodeWithScore]:
         """Postprocess nodes."""
 
         if query_bundle is None:
             raise ValueError("Missing query bundle in extra info.")
 
-        # query_bundle = cast(QueryBundle, extra_info["query_bundle"])
+        # query_bundle = cast(QueryBundle, metadata["query_bundle"])
         # infer_recency_prompt = SimpleInputPrompt(self.infer_recency_tmpl)
-        # raw_pred, _ = self.service_context.llm_predictor.predict(
+        # raw_pred = self.service_context.llm_predictor.predict(
         #     prompt=infer_recency_prompt,
         #     query_str=query_bundle.query_str,
         # )
         # pred = parse_recency_pred(raw_pred)
         # # if no need to use recency post-processor, return nodes as is
         # if not pred:
         #     return nodes
 
         # sort nodes by date
-        info_dict_attr = "extra_info" if self.in_extra_info else "node_info"
         node_dates = pd.to_datetime(
-            [getattr(node.node, info_dict_attr)[self.date_key] for node in nodes]
+            [node.node.metadata[self.date_key] for node in nodes]
         )
         sorted_node_idxs = np.flip(node_dates.argsort())
         sorted_nodes: List[NodeWithScore] = [nodes[idx] for idx in sorted_node_idxs]
 
         # get embeddings for each node
         embed_model = self.service_context.embed_model
         for node in sorted_nodes:
             embed_model.queue_text_for_embedding(
-                node.node.get_doc_id(), node.node.get_text()
+                node.node.node_id,
+                node.node.get_content(metadata_mode=MetadataMode.EMBED),
             )
 
         _, text_embeddings = embed_model.get_queued_text_embeddings()
         node_ids_to_skip: Set[str] = set()
         for idx, node in enumerate(sorted_nodes):
-            if node.node.get_doc_id() in node_ids_to_skip:
+            if node.node.node_id in node_ids_to_skip:
                 continue
             # get query embedding for the "query" node
             # NOTE: not the same as the text embedding because
             # we want to optimize for retrieval results
 
             query_text = self.query_embedding_tmpl.format(
-                context_str=node.node.get_text(),
+                context_str=node.node.get_content(metadata_mode=MetadataMode.EMBED),
             )
             query_embedding = embed_model.get_query_embedding(query_text)
 
             for idx2 in range(idx + 1, len(sorted_nodes)):
-                if sorted_nodes[idx2].node.get_doc_id() in node_ids_to_skip:
+                if sorted_nodes[idx2].node.node_id in node_ids_to_skip:
                     continue
                 node2 = sorted_nodes[idx2]
                 if (
                     np.dot(query_embedding, text_embeddings[idx2])
                     > self.similarity_cutoff
                 ):
-                    node_ids_to_skip.add(node2.node.get_doc_id())
+                    node_ids_to_skip.add(node2.node.node_id)
 
         return [
-            node
-            for node in sorted_nodes
-            if node.node.get_doc_id() not in node_ids_to_skip
+            node for node in sorted_nodes if node.node.node_id not in node_ids_to_skip
         ]
 
 
 class TimeWeightedPostprocessor(BasePydanticNodePostprocessor):
     """Time-weighted post-processor.
 
     Reranks a set of nodes based on their recency.
@@ -217,33 +210,35 @@
 
         similarities = []
         for node_with_score in nodes:
             # embedding similarity score
             score = node_with_score.score or 1.0
             node = node_with_score.node
             # time score
-            if node.node_info is None:
-                raise ValueError("node_info is None")
+            if node.metadata is None:
+                raise ValueError("metadata is None")
 
-            last_accessed = node.node_info.get(self.last_accessed_key, None)
+            last_accessed = node.metadata.get(self.last_accessed_key, None)
             if last_accessed is None:
                 last_accessed = now
 
             hours_passed = (now - last_accessed) / 3600
             time_similarity = (1 - self.time_decay) ** hours_passed
 
             similarity = score + time_similarity
 
             similarities.append(similarity)
 
         sorted_tups = sorted(zip(similarities, nodes), key=lambda x: x[0], reverse=True)
 
         top_k = min(self.top_k, len(sorted_tups))
         result_tups = sorted_tups[:top_k]
-        result_nodes = [NodeWithScore(n.node, score) for score, n in result_tups]
+        result_nodes = [
+            NodeWithScore(node=n.node, score=score) for score, n in result_tups
+        ]
 
         # set __last_accessed__ to now
         if self.time_access_refresh:
             for node_with_score in result_nodes:
-                node_with_score.node.get_node_info()[self.last_accessed_key] = now
+                node_with_score.node.metadata[self.last_accessed_key] = now
 
         return result_nodes
```

### Comparing `llama_index-0.6.9/llama_index/indices/postprocessor/pii.py` & `llama_index-0.7.0/llama_index/indices/postprocessor/pii.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 """PII postprocessor."""
+import json
+from copy import deepcopy
+from typing import List, Optional, Dict, Tuple, Callable
 
 from llama_index.indices.postprocessor.node import BasePydanticNodePostprocessor
-from llama_index.data_structs.node import NodeWithScore
-from typing import List, Optional, Dict, Tuple, Callable
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
 from llama_index.prompts.prompts import QuestionAnswerPrompt
-from copy import deepcopy
-import json
+from llama_index.schema import NodeWithScore, MetadataMode
 
 
 DEFAULT_PII_TMPL = (
     "The current context information is provided. \n"
     "A task is also provided to mask the PII within the context. \n"
     "Return the text, with all PII masked out, and a mapping of the original PII "
     "to the masked PII. \n"
@@ -59,15 +59,15 @@
         pii_prompt = QuestionAnswerPrompt(self.pii_str_tmpl)
         # TODO: allow customization
         task_str = (
             "Mask out the PII, replace each PII with a tag, and return the text. "
             "Return the mapping in JSON."
         )
 
-        response, _ = self.service_context.llm_predictor.predict(
+        response = self.service_context.llm_predictor.predict(
             pii_prompt, context_str=text, query_str=task_str
         )
         splits = response.split("Output Mapping:")
         text_output = splits[0].strip()
         json_str_output = splits[1].strip()
         json_dict = json.loads(json_str_output)
         return text_output, json_dict
@@ -78,20 +78,23 @@
         query_bundle: Optional[QueryBundle] = None,
     ) -> List[NodeWithScore]:
         """Postprocess nodes."""
         # swap out text from nodes, with the original node mappings
         new_nodes = []
         for node_with_score in nodes:
             node = node_with_score.node
-            new_text, mapping_info = self.mask_pii(node.get_text())
+            new_text, mapping_info = self.mask_pii(
+                node.get_content(metadata_mode=MetadataMode.LLM)
+            )
             new_node = deepcopy(node)
-            new_node.node_info = new_node.node_info or {}
-            new_node.node_info[self.pii_node_info_key] = mapping_info
-            new_node.text = new_text
-            new_nodes.append(NodeWithScore(new_node, node_with_score.score))
+            new_node.excluded_embed_metadata_keys.append(self.pii_node_info_key)
+            new_node.excluded_llm_metadata_keys.append(self.pii_node_info_key)
+            new_node.metadata[self.pii_node_info_key] = mapping_info
+            new_node.set_content(new_text)
+            new_nodes.append(NodeWithScore(node=new_node, score=node_with_score.score))
 
         return new_nodes
 
 
 class NERPIINodePostprocessor(BasePydanticNodePostprocessor):
     """NER PII Node processor.
 
@@ -122,15 +125,18 @@
 
         ner = pipeline("ner", grouped_entities=True)
 
         # swap out text from nodes, with the original node mappings
         new_nodes = []
         for node_with_score in nodes:
             node = node_with_score.node
-            new_text, mapping_info = self.mask_pii(ner, node.get_text())
+            new_text, mapping_info = self.mask_pii(
+                ner, node.get_content(metadata_mode=MetadataMode.LLM)
+            )
             new_node = deepcopy(node)
-            new_node.node_info = new_node.node_info or {}
-            new_node.node_info[self.pii_node_info_key] = mapping_info
-            new_node.text = new_text
-            new_nodes.append(NodeWithScore(new_node, node_with_score.score))
+            new_node.excluded_embed_metadata_keys.append(self.pii_node_info_key)
+            new_node.excluded_llm_metadata_keys.append(self.pii_node_info_key)
+            new_node.metadata[self.pii_node_info_key] = mapping_info
+            new_node.set_content(new_text)
+            new_nodes.append(NodeWithScore(node=new_node, score=node_with_score.score))
 
         return new_nodes
```

### Comparing `llama_index-0.6.9/llama_index/indices/prompt_helper.py` & `llama_index-0.7.0/llama_index/indices/prompt_helper.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,234 +1,182 @@
-"""General prompt helper that can help deal with token limitations.
+"""General prompt helper that can help deal with LLM context window token limitations.
 
-The helper can split text. It can also concatenate text from Node
-structs but keeping token limitations in mind.
+At its core, it calculates available context size by starting with the context window 
+size of an LLM and reserve token space for the prompt template, and the output.
 
+It provides utility for "repacking" text chunks (retrieved from index) to maximally 
+make use of the available context window (and thereby reducing the number of LLM calls 
+needed), or truncating them so that they fit in a single LLM call.
 """
 
 from typing import Callable, List, Optional, Sequence
+from llama_index.constants import DEFAULT_CONTEXT_WINDOW, DEFAULT_NUM_OUTPUTS
 
-from llama_index.constants import MAX_CHUNK_OVERLAP
-from llama_index.data_structs.node import Node
-from llama_index.langchain_helpers.chain_wrapper import LLMPredictor
 from llama_index.langchain_helpers.text_splitter import TokenTextSplitter
+from llama_index.llm_predictor.base import LLMMetadata
 from llama_index.prompts.base import Prompt
+from llama_index.prompts.utils import get_empty_prompt_txt
 from llama_index.utils import globals_helper
+import logging
+
+DEFAULT_PADDING = 5
+DEFAULT_CHUNK_OVERLAP_RATIO = 0.1
+
+logger = logging.getLogger(__name__)
 
 
 class PromptHelper:
     """Prompt helper.
 
-    This utility helps us fill in the prompt, split the text,
-    and fill in context information according to necessary token limitations.
+    General prompt helper that can help deal with LLM context window token limitations.
+
+    At its core, it calculates available context size by starting with the context
+    window size of an LLM and reserve token space for the prompt template, and the
+    output.
+
+    It provides utility for "repacking" text chunks (retrieved from index) to maximally
+    make use of the available context window (and thereby reducing the number of LLM
+    calls needed), or truncating them so that they fit in a single LLM call.
 
     Args:
-        max_input_size (int): Maximum input size for the LLM.
-        num_output (int): Number of outputs for the LLM.
-        max_chunk_overlap (int): Maximum chunk overlap for the LLM.
-        embedding_limit (Optional[int]): Maximum number of embeddings to use.
-        chunk_size_limit (Optional[int]): Maximum chunk size to use.
+        context_window (int):                   Context window for the LLM.
+        num_output (int):                       Number of outputs for the LLM.
+        chunk_overlap_ratio (float):            Chunk overlap as a ratio of chunk size
+        chunk_size_limit (Optional[int]):         Maximum chunk size to use.
         tokenizer (Optional[Callable[[str], List]]): Tokenizer to use.
+        separator (str):                        Separator for text splitter
 
     """
 
     def __init__(
         self,
-        max_input_size: int,
-        num_output: int,
-        max_chunk_overlap: int,
-        embedding_limit: Optional[int] = None,
+        context_window: int = DEFAULT_CONTEXT_WINDOW,
+        num_output: int = DEFAULT_NUM_OUTPUTS,
+        chunk_overlap_ratio: float = DEFAULT_CHUNK_OVERLAP_RATIO,
         chunk_size_limit: Optional[int] = None,
         tokenizer: Optional[Callable[[str], List]] = None,
         separator: str = " ",
     ) -> None:
         """Init params."""
-        self.max_input_size = max_input_size
+        self.context_window = context_window
         self.num_output = num_output
-        self.max_chunk_overlap = max_chunk_overlap
-        self.embedding_limit = embedding_limit
+
+        self.chunk_overlap_ratio = chunk_overlap_ratio
+        if self.chunk_overlap_ratio > 1.0 or self.chunk_overlap_ratio < 0.0:
+            raise ValueError("chunk_overlap_ratio must be a float between 0. and 1.")
         self.chunk_size_limit = chunk_size_limit
+
         # TODO: make configurable
         self._tokenizer = tokenizer or globals_helper.tokenizer
         self._separator = separator
-        self.use_chunk_size_limit = chunk_size_limit is not None
 
     @classmethod
-    def from_llm_predictor(
-        self,
-        llm_predictor: LLMPredictor,
-        max_chunk_overlap: Optional[int] = None,
-        embedding_limit: Optional[int] = None,
+    def from_llm_metadata(
+        cls,
+        llm_metadata: LLMMetadata,
+        chunk_overlap_ratio: float = DEFAULT_CHUNK_OVERLAP_RATIO,
         chunk_size_limit: Optional[int] = None,
         tokenizer: Optional[Callable[[str], List]] = None,
+        separator: str = " ",
     ) -> "PromptHelper":
         """Create from llm predictor.
 
-        This will autofill values like max_input_size and num_output.
+        This will autofill values like context_window and num_output.
 
         """
-        llm_metadata = llm_predictor.get_llm_metadata()
-        max_chunk_overlap = max_chunk_overlap or min(
-            MAX_CHUNK_OVERLAP,
-            llm_metadata.max_input_size // 10,
-        )
-        if chunk_size_limit is not None:
-            max_chunk_overlap = min(max_chunk_overlap, chunk_size_limit // 10)
-
-        return self(
-            llm_metadata.max_input_size,
-            llm_metadata.num_output,
-            max_chunk_overlap,
-            embedding_limit=embedding_limit,
+        context_window = llm_metadata.context_window
+        if llm_metadata.num_output == -1:
+            num_output = DEFAULT_NUM_OUTPUTS
+        else:
+            num_output = llm_metadata.num_output
+
+        return cls(
+            context_window=context_window,
+            num_output=num_output,
+            chunk_overlap_ratio=chunk_overlap_ratio,
             chunk_size_limit=chunk_size_limit,
             tokenizer=tokenizer,
+            separator=separator,
         )
 
-    def get_chunk_size_given_prompt(
-        self, prompt_text: str, num_chunks: int, padding: Optional[int] = 1
-    ) -> int:
-        """Get chunk size making sure we can also fit the prompt in.
-
-        Chunk size is computed based on a function of the total input size,
-        the prompt length, the number of outputs, and the number of chunks.
-
-        If padding is specified, then we subtract that from the chunk size.
-        By default we assume there is a padding of 1 (for the newline between chunks).
-
-        Limit by embedding_limit and chunk_size_limit if specified.
+    def _get_available_context_size(self, prompt: Prompt) -> int:
+        """Get available context size.
 
+        This is calculated as:
+            available context window = total context window
+                - input (partially filled prompt)
+                - output (room reserved for response)
         """
-        prompt_tokens = self._tokenizer(prompt_text)
+        empty_prompt_txt = get_empty_prompt_txt(prompt)
+        prompt_tokens = self._tokenizer(empty_prompt_txt)
         num_prompt_tokens = len(prompt_tokens)
 
-        # NOTE: if embedding limit is specified, then chunk_size must not be larger than
-        # embedding_limit
-        result = (
-            self.max_input_size - num_prompt_tokens - self.num_output
-        ) // num_chunks
-        if padding is not None:
-            result -= padding
-
-        if self.embedding_limit is not None:
-            result = min(result, self.embedding_limit)
-        if self.chunk_size_limit is not None and self.use_chunk_size_limit:
-            result = min(result, self.chunk_size_limit)
+        return self.context_window - num_prompt_tokens - self.num_output
 
-        return result
+    def _get_available_chunk_size(
+        self, prompt: Prompt, num_chunks: int = 1, padding: int = 5
+    ) -> int:
+        """Get available chunk size.
 
-    def _get_empty_prompt_txt(self, prompt: Prompt) -> str:
-        """Get empty prompt text.
+        This is calculated as:
+            available context window = total context window
+                - input (partially filled prompt)
+                - output (room reserved for response)
 
-        Substitute empty strings in parts of the prompt that have
-        not yet been filled out. Skip variables that have already
-        been partially formatted. This is used to compute the initial tokens.
+            available chunk size  = available context window  // number_chunks
+                - padding
 
+        Note:
+        - By default, we use padding of 5 (to save space for formatting needs).
+        - The available chunk size is further clamped to chunk_size_limit if specified
         """
-        fmt_dict = {
-            v: "" for v in prompt.input_variables if v not in prompt.partial_dict
-        }
-        # TODO: change later from llm=None
-        empty_prompt_txt = prompt.format(llm=None, **fmt_dict)
-        return empty_prompt_txt
-
-    def get_biggest_prompt(self, prompts: List[Prompt]) -> Prompt:
-        """Get biggest prompt.
-
-        Oftentimes we need to fetch the biggest prompt, in order to
-        be the most conservative about chunking text. This
-        is a helper utility for that.
+        available_context_size = self._get_available_context_size(prompt)
 
-        """
-        empty_prompt_txts = [self._get_empty_prompt_txt(prompt) for prompt in prompts]
-        empty_prompt_txt_lens = [len(txt) for txt in empty_prompt_txts]
-        biggest_prompt = prompts[
-            empty_prompt_txt_lens.index(max(empty_prompt_txt_lens))
-        ]
-        return biggest_prompt
+        result = available_context_size // num_chunks
+        result -= padding
 
-    def get_text_splitter_given_prompt(
-        self, prompt: Prompt, num_chunks: int, padding: Optional[int] = 1
-    ) -> TokenTextSplitter:
-        """Get text splitter given initial prompt.
+        if self.chunk_size_limit is not None:
+            result = min(result, self.chunk_size_limit)
 
-        Allows us to get the text splitter which will split up text according
-        to the desired chunk size.
+        return result
 
+    def get_text_splitter_given_prompt(
+        self, prompt: Prompt, num_chunks: int = 1, padding: int = DEFAULT_PADDING
+    ) -> TokenTextSplitter:
+        """Get text splitter configured to maximally pack available context window,
+        taking into account of given prompt, and desired number of chunks.
         """
-        # generate empty_prompt_txt to compute initial tokens
-        empty_prompt_txt = self._get_empty_prompt_txt(prompt)
-        chunk_size = self.get_chunk_size_given_prompt(
-            empty_prompt_txt, num_chunks, padding=padding
-        )
+        chunk_size = self._get_available_chunk_size(prompt, num_chunks, padding=padding)
+        if chunk_size == 0:
+            raise ValueError("Got 0 as available chunk size.")
+        chunk_overlap = int(self.chunk_overlap_ratio * chunk_size)
         text_splitter = TokenTextSplitter(
             separator=self._separator,
             chunk_size=chunk_size,
-            chunk_overlap=self.max_chunk_overlap // num_chunks,
+            chunk_overlap=chunk_overlap,
             tokenizer=self._tokenizer,
         )
         return text_splitter
 
-    def get_text_from_nodes(
-        self, node_list: List[Node], prompt: Optional[Prompt] = None
-    ) -> str:
-        """Get text from nodes. Used by tree-structured indices."""
-        num_nodes = len(node_list)
-        text_splitter = None
-        if prompt is not None:
-            # add padding given the newline character
-            text_splitter = self.get_text_splitter_given_prompt(
-                prompt,
-                num_nodes,
-                padding=1,
-            )
-        results = []
-        for node in node_list:
-            text = (
-                text_splitter.truncate_text(node.get_text())
-                if text_splitter is not None
-                else node.get_text()
-            )
-            results.append(text)
-
-        return "\n".join(results)
-
-    def get_numbered_text_from_nodes(
-        self, node_list: List[Node], prompt: Optional[Prompt] = None
-    ) -> str:
-        """Get text from nodes in the format of a numbered list.
-
-        Used by tree-structured indices.
-
-        """
-        num_nodes = len(node_list)
-        text_splitter = None
-        if prompt is not None:
-            # add padding given the number, and the newlines
-            text_splitter = self.get_text_splitter_given_prompt(
-                prompt,
-                num_nodes,
-                padding=5,
-            )
-        results = []
-        number = 1
-        for node in node_list:
-            node_text = " ".join(node.get_text().splitlines())
-            if text_splitter is not None:
-                node_text = text_splitter.truncate_text(node_text)
-            text = f"({number}) {node_text}"
-            results.append(text)
-            number += 1
-        return "\n\n".join(results)
+    def truncate(
+        self, prompt: Prompt, text_chunks: Sequence[str], padding: int = DEFAULT_PADDING
+    ) -> List[str]:
+        """Truncate text chunks to fit available context window."""
+        text_splitter = self.get_text_splitter_given_prompt(
+            prompt,
+            num_chunks=len(text_chunks),
+            padding=padding,
+        )
+        return [text_splitter.truncate_text(chunk) for chunk in text_chunks]
 
-    def compact_text_chunks(
-        self, prompt: Prompt, text_chunks: Sequence[str]
+    def repack(
+        self, prompt: Prompt, text_chunks: Sequence[str], padding: int = DEFAULT_PADDING
     ) -> List[str]:
-        """Compact text chunks.
+        """Repack text chunks to fit available context window.
 
         This will combine text chunks into consolidated chunks
-        that more fully "pack" the prompt template given the max_input_size.
+        that more fully "pack" the prompt template given the context_window.
 
         """
+        text_splitter = self.get_text_splitter_given_prompt(prompt, padding=padding)
         combined_str = "\n\n".join([c.strip() for c in text_chunks if c.strip()])
-        # resplit based on self.max_chunk_overlap
-        text_splitter = self.get_text_splitter_given_prompt(prompt, 1, padding=1)
         return text_splitter.split_text(combined_str)
```

### Comparing `llama_index-0.6.9/llama_index/indices/query/base.py` & `llama_index-0.7.0/llama_index/indices/query/base.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,30 +1,38 @@
 """Base query engine."""
 
 import logging
 from abc import ABC, abstractmethod
 from typing import List, Optional, Sequence
 
-from llama_index.data_structs.node import NodeWithScore
+from llama_index.callbacks.base import CallbackManager
 from llama_index.indices.query.schema import QueryBundle, QueryType
 from llama_index.response.schema import RESPONSE_TYPE
+from llama_index.schema import NodeWithScore
 
 logger = logging.getLogger(__name__)
 
 
 class BaseQueryEngine(ABC):
+    def __init__(self, callback_manager: Optional[CallbackManager]) -> None:
+        self.callback_manager = callback_manager or CallbackManager([])
+
     def query(self, str_or_query_bundle: QueryType) -> RESPONSE_TYPE:
-        if isinstance(str_or_query_bundle, str):
-            str_or_query_bundle = QueryBundle(str_or_query_bundle)
-        return self._query(str_or_query_bundle)
+        with self.callback_manager.as_trace("query"):
+            if isinstance(str_or_query_bundle, str):
+                str_or_query_bundle = QueryBundle(str_or_query_bundle)
+            response = self._query(str_or_query_bundle)
+            return response
 
     async def aquery(self, str_or_query_bundle: QueryType) -> RESPONSE_TYPE:
-        if isinstance(str_or_query_bundle, str):
-            str_or_query_bundle = QueryBundle(str_or_query_bundle)
-        return await self._aquery(str_or_query_bundle)
+        with self.callback_manager.as_trace("query"):
+            if isinstance(str_or_query_bundle, str):
+                str_or_query_bundle = QueryBundle(str_or_query_bundle)
+            response = await self._aquery(str_or_query_bundle)
+            return response
 
     def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
         raise NotImplementedError(
             "This query engine does not support retrieve, use query directly"
         )
 
     def synthesize(
```

### Comparing `llama_index-0.6.9/llama_index/indices/query/query_transform/base.py` & `llama_index-0.7.0/llama_index/indices/query/query_transform/base.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,26 @@
 """Query transform."""
 
 import dataclasses
 from abc import abstractmethod
 from typing import Dict, Optional, cast
 
-from langchain.input import print_text
+from llama_index.bridge.langchain import print_text
 
 from llama_index.indices.query.query_transform.prompts import (
     DEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT,
     DEFAULT_IMAGE_OUTPUT_PROMPT,
     DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT,
     DecomposeQueryTransformPrompt,
     ImageOutputQueryTransformPrompt,
     StepDecomposeQueryTransformPrompt,
 )
 from llama_index.indices.query.schema import QueryBundle, QueryType
-from llama_index.langchain_helpers.chain_wrapper import LLMPredictor
+from llama_index.llm_predictor import LLMPredictor
+from llama_index.llm_predictor.base import BaseLLMPredictor
 from llama_index.prompts.base import Prompt
 from llama_index.prompts.default_prompts import DEFAULT_HYDE_PROMPT
 from llama_index.response.schema import Response
 
 
 class BaseQueryTransform:
     """Base class for query transform.
@@ -28,51 +29,51 @@
     to improve index querying.
 
     The query transformation is performed before the query is sent to the index.
 
     """
 
     @abstractmethod
-    def _run(self, query_bundle: QueryBundle, extra_info: Dict) -> QueryBundle:
+    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:
         """Run query transform."""
 
     def run(
         self,
         query_bundle_or_str: QueryType,
-        extra_info: Optional[Dict] = None,
+        metadata: Optional[Dict] = None,
     ) -> QueryBundle:
         """Run query transform."""
-        extra_info = extra_info or {}
+        metadata = metadata or {}
         if isinstance(query_bundle_or_str, str):
             query_bundle = QueryBundle(
                 query_str=query_bundle_or_str,
                 custom_embedding_strs=[query_bundle_or_str],
             )
         else:
             query_bundle = query_bundle_or_str
 
-        return self._run(query_bundle, extra_info=extra_info)
+        return self._run(query_bundle, metadata=metadata)
 
     def __call__(
         self,
         query_bundle_or_str: QueryType,
-        extra_info: Optional[Dict] = None,
+        metadata: Optional[Dict] = None,
     ) -> QueryBundle:
         """Run query processor."""
-        return self.run(query_bundle_or_str, extra_info=extra_info)
+        return self.run(query_bundle_or_str, metadata=metadata)
 
 
 class IdentityQueryTransform(BaseQueryTransform):
     """Identity query transform.
 
     Do nothing to the query.
 
     """
 
-    def _run(self, query_bundle: QueryBundle, extra_info: Dict) -> QueryBundle:
+    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:
         """Run query transform."""
         return query_bundle
 
 
 class HyDEQueryTransform(BaseQueryTransform):
     """Hypothetical Document Embeddings (HyDE) query transform.
 
@@ -81,15 +82,15 @@
 
     As described in `[Precise Zero-Shot Dense Retrieval without Relevance Labels]
     (https://arxiv.org/abs/2212.10496)`
     """
 
     def __init__(
         self,
-        llm_predictor: Optional[LLMPredictor] = None,
+        llm_predictor: Optional[BaseLLMPredictor] = None,
         hyde_prompt: Optional[Prompt] = None,
         include_original: bool = True,
     ) -> None:
         """Initialize HyDEQueryTransform.
 
         Args:
             llm_predictor (Optional[LLMPredictor]): LLM for generating
@@ -100,19 +101,19 @@
         """
         super().__init__()
 
         self._llm_predictor = llm_predictor or LLMPredictor()
         self._hyde_prompt = hyde_prompt or DEFAULT_HYDE_PROMPT
         self._include_original = include_original
 
-    def _run(self, query_bundle: QueryBundle, extra_info: Dict) -> QueryBundle:
+    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:
         """Run query transform."""
         # TODO: support generating multiple hypothetical docs
         query_str = query_bundle.query_str
-        hypothetical_doc, _ = self._llm_predictor.predict(
+        hypothetical_doc = self._llm_predictor.predict(
             self._hyde_prompt, context_str=query_str
         )
         embedding_strs = [hypothetical_doc]
         if self._include_original:
             embedding_strs.extend(query_bundle.embedding_strs)
         return QueryBundle(
             query_str=query_str,
@@ -130,35 +131,35 @@
         llm_predictor (Optional[LLMPredictor]): LLM for generating
             hypothetical documents
 
     """
 
     def __init__(
         self,
-        llm_predictor: Optional[LLMPredictor] = None,
+        llm_predictor: Optional[BaseLLMPredictor] = None,
         decompose_query_prompt: Optional[DecomposeQueryTransformPrompt] = None,
         verbose: bool = False,
     ) -> None:
         """Init params."""
         super().__init__()
         self._llm_predictor = llm_predictor or LLMPredictor()
         self._decompose_query_prompt = (
             decompose_query_prompt or DEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT
         )
         self.verbose = verbose
 
-    def _run(self, query_bundle: QueryBundle, extra_info: Dict) -> QueryBundle:
+    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:
         """Run query transform."""
         # currently, just get text from the index structure
-        index_summary = cast(str, extra_info.get("index_summary", "None"))
+        index_summary = cast(str, metadata.get("index_summary", "None"))
 
         # given the text from the index, we can use the query bundle to generate
         # a new query bundle
         query_str = query_bundle.query_str
-        new_query_str, _ = self._llm_predictor.predict(
+        new_query_str = self._llm_predictor.predict(
             self._decompose_query_prompt,
             query_str=query_str,
             context_str=index_summary,
         )
 
         if self.verbose:
             print_text(f"> Current query: {query_str}\n", color="yellow")
@@ -189,17 +190,17 @@
             width (int): desired image display width in pixels
             query_prompt (ImageOutputQueryTransformPrompt): custom prompt for
                 augmenting query with image output instructions.
         """
         self._width = width
         self._query_prompt = query_prompt or DEFAULT_IMAGE_OUTPUT_PROMPT
 
-    def _run(self, query_bundle: QueryBundle, extra_info: Dict) -> QueryBundle:
+    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:
         """Run query transform."""
-        del extra_info  # Unused
+        del metadata  # Unused
         new_query_str = self._query_prompt.format(
             query_str=query_bundle.query_str, image_width=self._width
         )
         new_query_bundle = dataclasses.replace(query_bundle, query_str=new_query_str)
         return new_query_bundle
 
 
@@ -215,45 +216,44 @@
         llm_predictor (Optional[LLMPredictor]): LLM for generating
             hypothetical documents
 
     """
 
     def __init__(
         self,
-        llm_predictor: Optional[LLMPredictor] = None,
+        llm_predictor: Optional[BaseLLMPredictor] = None,
         step_decompose_query_prompt: Optional[StepDecomposeQueryTransformPrompt] = None,
         verbose: bool = False,
     ) -> None:
         """Init params."""
         super().__init__()
         self._llm_predictor = llm_predictor or LLMPredictor()
         self._step_decompose_query_prompt = (
             step_decompose_query_prompt or DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT
         )
         self.verbose = verbose
 
-    def _run(self, query_bundle: QueryBundle, extra_info: Dict) -> QueryBundle:
+    def _run(self, query_bundle: QueryBundle, metadata: Dict) -> QueryBundle:
         """Run query transform."""
         index_summary = cast(
             str,
-            extra_info.get("index_summary", "None"),
+            metadata.get("index_summary", "None"),
         )
-        prev_reasoning = cast(Response, extra_info.get("prev_reasoning"))
+        prev_reasoning = cast(Response, metadata.get("prev_reasoning"))
         fmt_prev_reasoning = f"\n{prev_reasoning}" if prev_reasoning else "None"
 
         # given the text from the index, we can use the query bundle to generate
         # a new query bundle
         query_str = query_bundle.query_str
-        new_query_str, formatted_prompt = self._llm_predictor.predict(
+        new_query_str = self._llm_predictor.predict(
             self._step_decompose_query_prompt,
             prev_reasoning=fmt_prev_reasoning,
             query_str=query_str,
             context_str=index_summary,
         )
         if self.verbose:
             print_text(f"> Current query: {query_str}\n", color="yellow")
-            print_text(f"> Formatted prompt: {formatted_prompt}\n", color="pink")
             print_text(f"> New query: {new_query_str}\n", color="pink")
         return QueryBundle(
             query_str=new_query_str,
             custom_embedding_strs=query_bundle.custom_embedding_strs,
         )
```

### Comparing `llama_index-0.6.9/llama_index/indices/query/query_transform/prompts.py` & `llama_index-0.7.0/llama_index/indices/query/query_transform/prompts.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,33 +1,39 @@
 """Query transform prompts."""
 
 
-from typing import List
-
 from llama_index.prompts.base import Prompt
 from llama_index.prompts.prompt_type import PromptType
 
+# deprecated, kept for backwards compatibility
+"""Decompose prompt for query transformation.
+
+Prompt to "decompose" a query into another query
+given the existing context.
 
-class DecomposeQueryTransformPrompt(Prompt):
-    """Decompose prompt for query transformation.
+Required template variables: `context_str`, `query_str`
+"""
+DecomposeQueryTransformPrompt = Prompt
 
-    Prompt to "decompose" a query into another query
-    given the existing context.
+"""Step Decompose prompt for query transformation.
 
-    Required template variables: `context_str`, `query_str`
+Prompt to "decompose" a query into another query
+given the existing context + previous reasoning (the previous steps).
 
-    Args:
-        template (str): Template for the prompt.
-        **prompt_kwargs: Keyword arguments for the prompt.
+Required template variables: `context_str`, `query_str`, `prev_reasoning`
+"""
+StepDecomposeQueryTransformPrompt = Prompt
 
-    """
+"""Image output prompt for query transformation.
 
-    # TODO: specify a better prompt type
-    prompt_type: PromptType = PromptType.CUSTOM
-    input_variables: List[str] = ["context_str", "query_str"]
+Prompt to add instructions for formatting image output.
+
+Required template variables: `query_str`, `image_width`
+"""
+ImageOutputQueryTransformPrompt = Prompt
 
 
 DEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL = (
     "The original question is as follows: {query_str}\n"
     "We have an opportunity to answer some, or all of the question from a "
     "knowledge source. "
     "Context information for the knowledge source is provided below. \n"
@@ -49,58 +55,26 @@
     "New question: In which city did Paul Graham found his first company, Viaweb? "
     "\n\n"
     "Question: {query_str}\n"
     "Knowledge source context: {context_str}\n"
     "New question: "
 )
 
-DEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT = DecomposeQueryTransformPrompt(
-    DEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL
+DEFAULT_DECOMPOSE_QUERY_TRANSFORM_PROMPT = Prompt(
+    DEFAULT_DECOMPOSE_QUERY_TRANSFORM_TMPL, prompt_type=PromptType.DECOMPOSE
 )
 
 
-class ImageOutputQueryTransformPrompt(Prompt):
-    """Image output prompt for query transformation.
-
-    Prompt to add instructions for formatting image output.
-
-    Required template variables: `query_str`, `image_width`
-    """
-
-    # TODO: specify a better prompt type
-    prompt_type: PromptType = PromptType.CUSTOM
-    input_variables: List[str] = ["query_str", "image_width"]
-
-
 DEFAULT_IMAGE_OUTPUT_TMPL = (
     "{query_str}"
     "Show any image with a HTML <img/> tag with {image_width}."
     'e.g., <image src="data/img.jpg" width="{image_width}" />.'
 )
 
-DEFAULT_IMAGE_OUTPUT_PROMPT = ImageOutputQueryTransformPrompt(DEFAULT_IMAGE_OUTPUT_TMPL)
-
-
-class StepDecomposeQueryTransformPrompt(Prompt):
-    """Step Decompose prompt for query transformation.
-
-    Prompt to "decompose" a query into another query
-    given the existing context + previous reasoning (the previous steps).
-
-    Required template variables: `context_str`, `query_str`, `prev_reasoning`
-
-    Args:
-        template (str): Template for the prompt.
-        **prompt_kwargs: Keyword arguments for the prompt.
-
-    """
-
-    # TODO: specify a better prompt type
-    prompt_type: PromptType = PromptType.CUSTOM
-    input_variables: List[str] = ["context_str", "query_str", "prev_reasoning"]
+DEFAULT_IMAGE_OUTPUT_PROMPT = Prompt(DEFAULT_IMAGE_OUTPUT_TMPL)
 
 
 DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL = (
     "The original question is as follows: {query_str}\n"
     "We have an opportunity to answer some, or all of the question from a "
     "knowledge source. "
     "Context information for the knowledge source is provided below, as "
@@ -147,10 +121,10 @@
     "\n\n"
     "Question: {query_str}\n"
     "Knowledge source context: {context_str}\n"
     "Previous reasoning: {prev_reasoning}\n"
     "New question: "
 )
 
-DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT = StepDecomposeQueryTransformPrompt(
+DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_PROMPT = Prompt(
     DEFAULT_STEP_DECOMPOSE_QUERY_TRANSFORM_TMPL
 )
```

### Comparing `llama_index-0.6.9/llama_index/indices/query/schema.py` & `llama_index-0.7.0/llama_index/indices/query/schema.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/indices/registry.py` & `llama_index-0.7.0/llama_index/indices/registry.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 """Index registry."""
 
 from typing import Dict, Type
 
 from llama_index.data_structs.struct_type import IndexStructType
-from llama_index.indices.base import BaseGPTIndex
-from llama_index.indices.document_summary.base import GPTDocumentSummaryIndex
-from llama_index.indices.empty.base import GPTEmptyIndex
-from llama_index.indices.keyword_table.base import GPTKeywordTableIndex
-from llama_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex
-from llama_index.indices.list.base import GPTListIndex
-from llama_index.indices.struct_store.pandas import GPTPandasIndex
-from llama_index.indices.struct_store.sql import GPTSQLStructStoreIndex
-from llama_index.indices.tree.base import GPTTreeIndex
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
+from llama_index.indices.base import BaseIndex
+from llama_index.indices.document_summary.base import DocumentSummaryIndex
+from llama_index.indices.empty.base import EmptyIndex
+from llama_index.indices.keyword_table.base import KeywordTableIndex
+from llama_index.indices.knowledge_graph.base import KnowledgeGraphIndex
+from llama_index.indices.list.base import ListIndex
+from llama_index.indices.struct_store.pandas import PandasIndex
+from llama_index.indices.struct_store.sql import SQLStructStoreIndex
+from llama_index.indices.tree.base import TreeIndex
+from llama_index.indices.vector_store.base import VectorStoreIndex
 
-INDEX_STRUCT_TYPE_TO_INDEX_CLASS: Dict[IndexStructType, Type[BaseGPTIndex]] = {
-    IndexStructType.TREE: GPTTreeIndex,
-    IndexStructType.LIST: GPTListIndex,
-    IndexStructType.KEYWORD_TABLE: GPTKeywordTableIndex,
-    IndexStructType.VECTOR_STORE: GPTVectorStoreIndex,
-    IndexStructType.SQL: GPTSQLStructStoreIndex,
-    IndexStructType.PANDAS: GPTPandasIndex,
-    IndexStructType.KG: GPTKnowledgeGraphIndex,
-    IndexStructType.EMPTY: GPTEmptyIndex,
-    IndexStructType.DOCUMENT_SUMMARY: GPTDocumentSummaryIndex,
+INDEX_STRUCT_TYPE_TO_INDEX_CLASS: Dict[IndexStructType, Type[BaseIndex]] = {
+    IndexStructType.TREE: TreeIndex,
+    IndexStructType.LIST: ListIndex,
+    IndexStructType.KEYWORD_TABLE: KeywordTableIndex,
+    IndexStructType.VECTOR_STORE: VectorStoreIndex,
+    IndexStructType.SQL: SQLStructStoreIndex,
+    IndexStructType.PANDAS: PandasIndex,
+    IndexStructType.KG: KnowledgeGraphIndex,
+    IndexStructType.EMPTY: EmptyIndex,
+    IndexStructType.DOCUMENT_SUMMARY: DocumentSummaryIndex,
 }
```

### Comparing `llama_index-0.6.9/llama_index/indices/response/accumulate.py` & `llama_index-0.7.0/llama_index/response_synthesizers/accumulate.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,74 +1,77 @@
 import asyncio
-from typing import Any, List, Sequence
+from typing import Any, List, Sequence, Optional
 
 from llama_index.async_utils import run_async_tasks
-from llama_index.indices.response.base_builder import BaseResponseBuilder
 from llama_index.indices.service_context import ServiceContext
+from llama_index.prompts.default_prompts import (
+    DEFAULT_TEXT_QA_PROMPT,
+)
 from llama_index.prompts.prompts import QuestionAnswerPrompt
-from llama_index.token_counter.token_counter import llm_token_counter
+from llama_index.response_synthesizers.base import BaseSynthesizer
 from llama_index.types import RESPONSE_TEXT_TYPE
 
 
-class Accumulate(BaseResponseBuilder):
+class Accumulate(BaseSynthesizer):
+    """Accumulate responses from multiple text chunks."""
+
     def __init__(
         self,
-        service_context: ServiceContext,
-        text_qa_template: QuestionAnswerPrompt,
+        text_qa_template: Optional[QuestionAnswerPrompt] = None,
+        service_context: Optional[ServiceContext] = None,
         streaming: bool = False,
         use_async: bool = False,
     ) -> None:
-        super().__init__(service_context=service_context, streaming=streaming)
-        self.text_qa_template = text_qa_template
+        super().__init__(
+            service_context=service_context,
+            streaming=streaming,
+        )
+        self._text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT
         self._use_async = use_async
 
     def flatten_list(self, md_array: List[List[Any]]) -> List[Any]:
         return list(item for sublist in md_array for item in sublist)
 
-    def format_response(self, outputs: List[Any], separator: str) -> str:
+    def _format_response(self, outputs: List[Any], separator: str) -> str:
         responses: List[str] = []
-        for response, formatted_prompt in outputs:
-            self._log_prompt_and_response(
-                formatted_prompt, response, log_prefix="Initial"
-            )
+        for response in outputs:
             responses.append(response or "Empty Response")
 
         return separator.join(
             [f"Response {index + 1}: {item}" for index, item in enumerate(responses)]
         )
 
-    @llm_token_counter("aget_response")
     async def aget_response(
         self,
         query_str: str,
         text_chunks: Sequence[str],
         separator: str = "\n---------------------\n",
-        **kwargs: Any,
+        **response_kwargs: Any,
     ) -> RESPONSE_TEXT_TYPE:
         """Apply the same prompt to text chunks and return async responses"""
 
         if self._streaming:
             raise ValueError("Unable to stream in Accumulate response mode")
 
         tasks = [
             self._give_responses(query_str, text_chunk, use_async=True)
             for text_chunk in text_chunks
         ]
 
         flattened_tasks = self.flatten_list(tasks)
         outputs = await asyncio.gather(*flattened_tasks)
 
-        return self.format_response(outputs, separator)
+        return self._format_response(outputs, separator)
 
-    @llm_token_counter("get_response")
     def get_response(
         self,
         query_str: str,
         text_chunks: Sequence[str],
         separator: str = "\n---------------------\n",
+        **response_kwargs: Any,
     ) -> RESPONSE_TEXT_TYPE:
         """Apply the same prompt to text chunks and return responses"""
 
         if self._streaming:
             raise ValueError("Unable to stream in Accumulate response mode")
 
         tasks = [
@@ -77,27 +80,25 @@
         ]
 
         outputs = self.flatten_list(tasks)
 
         if self._use_async:
             outputs = run_async_tasks(outputs)
 
-        return self.format_response(outputs, separator)
+        return self._format_response(outputs, separator)
 
     def _give_responses(
         self, query_str: str, text_chunk: str, use_async: bool = False
     ) -> List[Any]:
         """Give responses given a query and a corresponding text chunk."""
-        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)
-        qa_text_splitter = (
-            self._service_context.prompt_helper.get_text_splitter_given_prompt(
-                text_qa_template, 1
-            )
+        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)
+
+        text_chunks = self._service_context.prompt_helper.repack(
+            text_qa_template, [text_chunk]
         )
-        text_chunks = qa_text_splitter.split_text(text_chunk)
 
         predictor = (
             self._service_context.llm_predictor.apredict
             if use_async
             else self._service_context.llm_predictor.predict
         )
```

### Comparing `llama_index-0.6.9/llama_index/indices/response/compact_and_refine.py` & `llama_index-0.7.0/llama_index/response_synthesizers/simple_summarize.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,60 +1,84 @@
-from typing import Any, Optional, Sequence
+from typing import Any, Generator, Optional, Sequence, cast
 
-from llama_index.indices.response.refine import Refine
 from llama_index.indices.service_context import ServiceContext
-from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt
+from llama_index.prompts.default_prompts import (
+    DEFAULT_TEXT_QA_PROMPT,
+)
+from llama_index.prompts.prompts import QuestionAnswerPrompt
+from llama_index.response_synthesizers.base import BaseSynthesizer
 from llama_index.types import RESPONSE_TEXT_TYPE
-from llama_index.utils import temp_set_attrs
 
 
-class CompactAndRefine(Refine):
+class SimpleSummarize(BaseSynthesizer):
     def __init__(
         self,
-        service_context: ServiceContext,
-        text_qa_template: QuestionAnswerPrompt,
-        refine_template: RefinePrompt,
+        text_qa_template: Optional[QuestionAnswerPrompt] = None,
+        service_context: Optional[ServiceContext] = None,
         streaming: bool = False,
     ) -> None:
-        super().__init__(
-            service_context=service_context,
-            text_qa_template=text_qa_template,
-            refine_template=refine_template,
-            streaming=streaming,
-        )
+        super().__init__(service_context=service_context, streaming=streaming)
+        self._text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT
 
     async def aget_response(
         self,
         query_str: str,
         text_chunks: Sequence[str],
-        prev_response: Optional[str] = None,
         **response_kwargs: Any,
     ) -> RESPONSE_TEXT_TYPE:
-        return self.get_response(query_str, text_chunks, prev_response)
+        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)
+        truncated_chunks = self._service_context.prompt_helper.truncate(
+            prompt=text_qa_template,
+            text_chunks=text_chunks,
+        )
+        node_text = "\n".join(truncated_chunks)
+
+        response: RESPONSE_TEXT_TYPE
+        if not self._streaming:
+            response = await self._service_context.llm_predictor.apredict(
+                text_qa_template,
+                context_str=node_text,
+            )
+        else:
+            response = self._service_context.llm_predictor.stream(
+                text_qa_template,
+                context_str=node_text,
+            )
+
+        if isinstance(response, str):
+            response = response or "Empty Response"
+        else:
+            response = cast(Generator, response)
+
+        return response
 
     def get_response(
         self,
         query_str: str,
         text_chunks: Sequence[str],
-        prev_response: Optional[str] = None,
-        **response_kwargs: Any,
+        **kwargs: Any,
     ) -> RESPONSE_TEXT_TYPE:
-        """Get compact response."""
-        # use prompt helper to fix compact text_chunks under the prompt limitation
-        # TODO: This is a temporary fix - reason it's temporary is that
-        # the refine template does not account for size of previous answer.
-        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)
-        refine_template = self._refine_template.partial_format(query_str=query_str)
-
-        max_prompt = self._service_context.prompt_helper.get_biggest_prompt(
-            [text_qa_template, refine_template]
+        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)
+        truncated_chunks = self._service_context.prompt_helper.truncate(
+            prompt=text_qa_template,
+            text_chunks=text_chunks,
         )
-        with temp_set_attrs(
-            self._service_context.prompt_helper, use_chunk_size_limit=False
-        ):
-            new_texts = self._service_context.prompt_helper.compact_text_chunks(
-                max_prompt, text_chunks
+        node_text = "\n".join(truncated_chunks)
+
+        response: RESPONSE_TEXT_TYPE
+        if not self._streaming:
+            response = self._service_context.llm_predictor.predict(
+                text_qa_template,
+                context_str=node_text,
             )
-            response = super().get_response(
-                query_str=query_str, text_chunks=new_texts, prev_response=prev_response
+        else:
+            response = self._service_context.llm_predictor.stream(
+                text_qa_template,
+                context_str=node_text,
             )
+
+        if isinstance(response, str):
+            response = response or "Empty Response"
+        else:
+            response = cast(Generator, response)
+
         return response
```

### Comparing `llama_index-0.6.9/llama_index/indices/response/generation.py` & `llama_index-0.7.0/llama_index/response_synthesizers/generation.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,72 +1,62 @@
 from typing import Any, Optional, Sequence
 
-from llama_index.indices.response.base_builder import BaseResponseBuilder
 from llama_index.indices.service_context import ServiceContext
 from llama_index.prompts.default_prompts import DEFAULT_SIMPLE_INPUT_PROMPT
 from llama_index.prompts.prompts import SimpleInputPrompt
-from llama_index.token_counter.token_counter import llm_token_counter
+from llama_index.response_synthesizers.base import BaseSynthesizer
 from llama_index.types import RESPONSE_TEXT_TYPE
 
 
-class Generation(BaseResponseBuilder):
+class Generation(BaseSynthesizer):
     def __init__(
         self,
-        service_context: ServiceContext,
         simple_template: Optional[SimpleInputPrompt] = None,
+        service_context: Optional[ServiceContext] = None,
         streaming: bool = False,
     ) -> None:
-        super().__init__(service_context, streaming)
+        super().__init__(service_context=service_context, streaming=streaming)
         self._input_prompt = simple_template or DEFAULT_SIMPLE_INPUT_PROMPT
 
-    @llm_token_counter("aget_response")
     async def aget_response(
         self,
         query_str: str,
         text_chunks: Sequence[str],
-        prev_response: Optional[str] = None,
         **response_kwargs: Any,
     ) -> RESPONSE_TEXT_TYPE:
         # NOTE: ignore text chunks and previous response
         del text_chunks
-        del prev_response
 
         if not self._streaming:
-            (
-                response,
-                formatted_prompt,
-            ) = await self._service_context.llm_predictor.apredict(
+            response = await self._service_context.llm_predictor.apredict(
                 self._input_prompt,
                 query_str=query_str,
             )
             return response
         else:
-            stream_response, _ = self._service_context.llm_predictor.stream(
+            stream_response = self._service_context.llm_predictor.stream(
                 self._input_prompt,
                 query_str=query_str,
             )
             return stream_response
 
-    @llm_token_counter("get_response")
     def get_response(
         self,
         query_str: str,
         text_chunks: Sequence[str],
-        prev_response: Optional[str] = None,
         **response_kwargs: Any,
     ) -> RESPONSE_TEXT_TYPE:
         # NOTE: ignore text chunks and previous response
         del text_chunks
-        del prev_response
 
         if not self._streaming:
-            response, formatted_prompt = self._service_context.llm_predictor.predict(
+            response = self._service_context.llm_predictor.predict(
                 self._input_prompt,
                 query_str=query_str,
             )
             return response
         else:
-            stream_response, _ = self._service_context.llm_predictor.stream(
+            stream_response = self._service_context.llm_predictor.stream(
                 self._input_prompt,
                 query_str=query_str,
             )
             return stream_response
```

### Comparing `llama_index-0.6.9/llama_index/indices/response/refine.py` & `llama_index-0.7.0/llama_index/response_synthesizers/refine.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,53 +1,50 @@
 import logging
 from typing import Any, Generator, Optional, Sequence, cast
 
-from llama_index.indices.response.base_builder import BaseResponseBuilder
 from llama_index.indices.service_context import ServiceContext
 from llama_index.indices.utils import truncate_text
+from llama_index.prompts.default_prompt_selectors import DEFAULT_REFINE_PROMPT_SEL
+from llama_index.prompts.default_prompts import (
+    DEFAULT_TEXT_QA_PROMPT,
+)
 from llama_index.prompts.prompts import QuestionAnswerPrompt, RefinePrompt
 from llama_index.response.utils import get_response_text
-from llama_index.token_counter.token_counter import llm_token_counter
+from llama_index.response_synthesizers.base import BaseSynthesizer
 from llama_index.types import RESPONSE_TEXT_TYPE
 
 logger = logging.getLogger(__name__)
 
 
-class Refine(BaseResponseBuilder):
+class Refine(BaseSynthesizer):
+    """Refine a response to a query across text chunks."""
+
     def __init__(
         self,
-        service_context: ServiceContext,
-        text_qa_template: QuestionAnswerPrompt,
-        refine_template: RefinePrompt,
+        service_context: Optional[ServiceContext] = None,
+        text_qa_template: Optional[QuestionAnswerPrompt] = None,
+        refine_template: Optional[RefinePrompt] = None,
         streaming: bool = False,
+        verbose: bool = False,
     ) -> None:
         super().__init__(service_context=service_context, streaming=streaming)
-        self.text_qa_template = text_qa_template
-        self._refine_template = refine_template
+        self._text_qa_template = text_qa_template or DEFAULT_TEXT_QA_PROMPT
+        self._refine_template = refine_template or DEFAULT_REFINE_PROMPT_SEL
+        self._verbose = verbose
 
-    @llm_token_counter("aget_response")
-    async def aget_response(
-        self,
-        query_str: str,
-        text_chunks: Sequence[str],
-        prev_response: Optional[str] = None,
-        **response_kwargs: Any,
-    ) -> RESPONSE_TEXT_TYPE:
-        return self.get_response(query_str, text_chunks, prev_response)
-
-    @llm_token_counter("get_response")
     def get_response(
         self,
         query_str: str,
         text_chunks: Sequence[str],
-        prev_response: Optional[str] = None,
         **response_kwargs: Any,
     ) -> RESPONSE_TEXT_TYPE:
         """Give response over chunks."""
-        prev_response_obj = cast(Optional[RESPONSE_TEXT_TYPE], prev_response)
+        prev_response_obj = cast(
+            Optional[RESPONSE_TEXT_TYPE], response_kwargs.get("prev_response", None)
+        )
         response: Optional[RESPONSE_TEXT_TYPE] = None
         for text_chunk in text_chunks:
             if prev_response_obj is None:
                 # if this is the first chunk, and text chunk already
                 # is an answer, then return it
                 response = self._give_response_single(
                     query_str,
@@ -67,44 +64,32 @@
     def _give_response_single(
         self,
         query_str: str,
         text_chunk: str,
         **response_kwargs: Any,
     ) -> RESPONSE_TEXT_TYPE:
         """Give response given a query and a corresponding text chunk."""
-        text_qa_template = self.text_qa_template.partial_format(query_str=query_str)
-        qa_text_splitter = (
-            self._service_context.prompt_helper.get_text_splitter_given_prompt(
-                text_qa_template, 1
-            )
+        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)
+        text_chunks = self._service_context.prompt_helper.repack(
+            text_qa_template, [text_chunk]
         )
-        text_chunks = qa_text_splitter.split_text(text_chunk)
 
         response: Optional[RESPONSE_TEXT_TYPE] = None
         # TODO: consolidate with loop in get_response_default
         for cur_text_chunk in text_chunks:
             if response is None and not self._streaming:
-                (
-                    response,
-                    formatted_prompt,
-                ) = self._service_context.llm_predictor.predict(
+                response = self._service_context.llm_predictor.predict(
                     text_qa_template,
                     context_str=cur_text_chunk,
                 )
-                self._log_prompt_and_response(
-                    formatted_prompt, response, log_prefix="Initial"
-                )
             elif response is None and self._streaming:
-                response, formatted_prompt = self._service_context.llm_predictor.stream(
+                response = self._service_context.llm_predictor.stream(
                     text_qa_template,
                     context_str=cur_text_chunk,
                 )
-                self._log_prompt_and_response(
-                    formatted_prompt, response, log_prefix="Initial"
-                )
             else:
                 response = self._refine_response_single(
                     cast(RESPONSE_TEXT_TYPE, response),
                     query_str,
                     cur_text_chunk,
                 )
         if isinstance(response, str):
@@ -123,41 +108,133 @@
         """Refine response."""
         # TODO: consolidate with logic in response/schema.py
         if isinstance(response, Generator):
             response = get_response_text(response)
 
         fmt_text_chunk = truncate_text(text_chunk, 50)
         logger.debug(f"> Refine context: {fmt_text_chunk}")
+        if self._verbose:
+            print(f"> Refine context: {fmt_text_chunk}")
         # NOTE: partial format refine template with query_str and existing_answer here
         refine_template = self._refine_template.partial_format(
             query_str=query_str, existing_answer=response
         )
-        refine_text_splitter = (
-            self._service_context.prompt_helper.get_text_splitter_given_prompt(
-                refine_template, 1
-            )
+        text_chunks = self._service_context.prompt_helper.repack(
+            refine_template, text_chunks=[text_chunk]
         )
 
-        text_chunks = refine_text_splitter.split_text(text_chunk)
-
         for cur_text_chunk in text_chunks:
             if not self._streaming:
-                (
-                    response,
-                    formatted_prompt,
-                ) = self._service_context.llm_predictor.predict(
+                response = self._service_context.llm_predictor.predict(
                     refine_template,
                     context_msg=cur_text_chunk,
                 )
             else:
-                response, formatted_prompt = self._service_context.llm_predictor.stream(
+                response = self._service_context.llm_predictor.stream(
                     refine_template,
                     context_msg=cur_text_chunk,
                 )
             refine_template = self._refine_template.partial_format(
                 query_str=query_str, existing_answer=response
             )
 
-            self._log_prompt_and_response(
-                formatted_prompt, response, log_prefix="Refined"
+        return response
+
+    async def aget_response(
+        self,
+        query_str: str,
+        text_chunks: Sequence[str],
+        **response_kwargs: Any,
+    ) -> RESPONSE_TEXT_TYPE:
+        prev_response_obj = cast(
+            Optional[RESPONSE_TEXT_TYPE], response_kwargs.get("prev_response", None)
+        )
+        response: Optional[RESPONSE_TEXT_TYPE] = None
+        for text_chunk in text_chunks:
+            if prev_response_obj is None:
+                # if this is the first chunk, and text chunk already
+                # is an answer, then return it
+                response = await self._agive_response_single(
+                    query_str,
+                    text_chunk,
+                )
+            else:
+                response = await self._arefine_response_single(
+                    prev_response_obj, query_str, text_chunk
+                )
+            prev_response_obj = response
+        if isinstance(response, str):
+            response = response or "Empty Response"
+        else:
+            response = cast(Generator, response)
+        return response
+
+    async def _arefine_response_single(
+        self,
+        response: RESPONSE_TEXT_TYPE,
+        query_str: str,
+        text_chunk: str,
+        **response_kwargs: Any,
+    ) -> RESPONSE_TEXT_TYPE:
+        """Refine response."""
+        # TODO: consolidate with logic in response/schema.py
+        if isinstance(response, Generator):
+            response = get_response_text(response)
+
+        fmt_text_chunk = truncate_text(text_chunk, 50)
+        logger.debug(f"> Refine context: {fmt_text_chunk}")
+        # NOTE: partial format refine template with query_str and existing_answer here
+        refine_template = self._refine_template.partial_format(
+            query_str=query_str, existing_answer=response
+        )
+        text_chunks = self._service_context.prompt_helper.repack(
+            refine_template, text_chunks=[text_chunk]
+        )
+
+        for cur_text_chunk in text_chunks:
+            if not self._streaming:
+                response = await self._service_context.llm_predictor.apredict(
+                    refine_template,
+                    context_msg=cur_text_chunk,
+                )
+            else:
+                raise ValueError("Streaming not supported for async")
+
+            refine_template = self._refine_template.partial_format(
+                query_str=query_str, existing_answer=response
             )
+
+        return response
+
+    async def _agive_response_single(
+        self,
+        query_str: str,
+        text_chunk: str,
+        **response_kwargs: Any,
+    ) -> RESPONSE_TEXT_TYPE:
+        """Give response given a query and a corresponding text chunk."""
+        text_qa_template = self._text_qa_template.partial_format(query_str=query_str)
+        text_chunks = self._service_context.prompt_helper.repack(
+            text_qa_template, [text_chunk]
+        )
+
+        response: Optional[RESPONSE_TEXT_TYPE] = None
+        # TODO: consolidate with loop in get_response_default
+        for cur_text_chunk in text_chunks:
+            if response is None and not self._streaming:
+                response = await self._service_context.llm_predictor.apredict(
+                    text_qa_template,
+                    context_str=cur_text_chunk,
+                )
+            elif response is None and self._streaming:
+                raise ValueError("Streaming not supported for async")
+            else:
+                response = await self._arefine_response_single(
+                    cast(RESPONSE_TEXT_TYPE, response),
+                    query_str,
+                    cur_text_chunk,
+                )
+        if isinstance(response, str):
+            response = response or "Empty Response"
+        else:
+            response = cast(Generator, response)
         return response
```

### Comparing `llama_index-0.6.9/llama_index/indices/response/type.py` & `llama_index-0.7.0/llama_index/response_synthesizers/type.py`

 * *Files 13% similar despite different names*

```diff
@@ -40,7 +40,15 @@
     """Ignore context, just use LLM to generate a response."""
 
     NO_TEXT = "no_text"
     """Return the retrieved context nodes, without synthesizing a final response."""
 
     ACCUMULATE = "accumulate"
     """Synthesize a response for each text chunk, and then return the concatenation."""
+
+    COMPACT_ACCUMULATE = "compact_accumulate"
+    """
+    Compact and accumulate mode first combine text chunks into larger consolidated \
+    chunks that more fully utilize the available context window, then accumulate \
+    answers for each of them and finally return the concatenation. 
+    This mode is faster than accumulate since we make fewer calls to the LLM.
+    """
```

### Comparing `llama_index-0.6.9/llama_index/indices/struct_store/__init__.py` & `llama_index-0.7.0/llama_index/indices/struct_store/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,21 +1,33 @@
 """Structured store indices."""
 
-from llama_index.indices.struct_store.pandas import GPTPandasIndex
-from llama_index.indices.struct_store.pandas_query import GPTNLPandasQueryEngine
+from llama_index.indices.struct_store.json_query import JSONQueryEngine
+from llama_index.indices.struct_store.pandas import GPTPandasIndex, PandasIndex
 from llama_index.indices.struct_store.sql import (
     GPTSQLStructStoreIndex,
     SQLContextContainerBuilder,
+    SQLStructStoreIndex,
 )
 from llama_index.indices.struct_store.sql_query import (
     GPTNLStructStoreQueryEngine,
     GPTSQLStructStoreQueryEngine,
+    NLStructStoreQueryEngine,
+    SQLStructStoreQueryEngine,
+    SQLTableRetrieverQueryEngine,
+    NLSQLTableQueryEngine,
 )
 
 __all__ = [
-    "GPTSQLStructStoreIndex",
+    "SQLStructStoreIndex",
     "SQLContextContainerBuilder",
+    "PandasIndex",
+    "NLStructStoreQueryEngine",
+    "SQLStructStoreQueryEngine",
+    "JSONQueryEngine",
+    # legacy
+    "GPTSQLStructStoreIndex",
     "GPTPandasIndex",
-    "GPTNLPandasQueryEngine",
     "GPTNLStructStoreQueryEngine",
     "GPTSQLStructStoreQueryEngine",
+    "SQLTableRetrieverQueryEngine",
+    "NLSQLTableQueryEngine",
 ]
```

### Comparing `llama_index-0.6.9/llama_index/indices/struct_store/base.py` & `llama_index-0.7.0/llama_index/indices/struct_store/base.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """Struct store."""
 
 import re
 from typing import Any, Callable, Dict, Generic, Optional, Sequence, TypeVar
 
-from llama_index.data_structs.node import Node
 from llama_index.data_structs.table import BaseStructTable
-from llama_index.indices.base import BaseGPTIndex
+from llama_index.indices.base import BaseIndex
 from llama_index.indices.service_context import ServiceContext
 from llama_index.prompts.default_prompts import DEFAULT_SCHEMA_EXTRACT_PROMPT
 from llama_index.prompts.prompts import SchemaExtractPrompt
+from llama_index.schema import BaseNode
+from llama_index.storage.docstore.types import RefDocInfo
 
 BST = TypeVar("BST", bound=BaseStructTable)
 
 
 def default_output_parser(output: str) -> Optional[Dict[str, Any]]:
     """Parse output of schema extraction.
 
@@ -31,20 +32,20 @@
             fields[field] = value
     return fields
 
 
 OUTPUT_PARSER_TYPE = Callable[[str], Optional[Dict[str, Any]]]
 
 
-class BaseGPTStructStoreIndex(BaseGPTIndex[BST], Generic[BST]):
-    """Base GPT Struct Store Index."""
+class BaseStructStoreIndex(BaseIndex[BST], Generic[BST]):
+    """Base Struct Store Index."""
 
     def __init__(
         self,
-        nodes: Optional[Sequence[Node]] = None,
+        nodes: Optional[Sequence[BaseNode]] = None,
         index_struct: Optional[BST] = None,
         service_context: Optional[ServiceContext] = None,
         schema_extract_prompt: Optional[SchemaExtractPrompt] = None,
         output_parser: Optional[OUTPUT_PARSER_TYPE] = None,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
@@ -55,10 +56,15 @@
         super().__init__(
             nodes=nodes,
             index_struct=index_struct,
             service_context=service_context,
             **kwargs,
         )
 
-    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document."""
+    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:
+        """Delete a node."""
         raise NotImplementedError("Delete not implemented for Struct Store Index.")
+
+    @property
+    def ref_doc_info(self) -> Dict[str, RefDocInfo]:
+        """Retrieve a dict mapping of ingested documents and their nodes+metadata."""
+        raise NotImplementedError("Struct Store Index does not support ref_doc_info.")
```

### Comparing `llama_index-0.6.9/llama_index/indices/struct_store/container_builder.py` & `llama_index-0.7.0/llama_index/indices/struct_store/container_builder.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 """SQL Container builder."""
 
 
 from typing import Any, Dict, List, Optional, Type
 
-from llama_index.indices.base import BaseGPTIndex
+from llama_index.indices.base import BaseIndex
 from llama_index.indices.common.struct_store.base import SQLDocumentContextBuilder
 from llama_index.indices.common.struct_store.schema import SQLContextContainer
 from llama_index.indices.query.schema import QueryType
 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase
 from llama_index.readers.base import Document
-from llama_index.schema import BaseDocument
+from llama_index.schema import BaseNode
 
 DEFAULT_CONTEXT_QUERY_TMPL = (
     "Please return the relevant tables (including the full schema) "
     "for the following query: {orig_query_str}"
 )
 
 
@@ -41,30 +41,32 @@
         """Initialize params."""
         self.sql_database = sql_database
 
         # if context_dict provided, validate that all keys are valid table names
         if context_dict is not None:
             # validate context_dict keys are valid table names
             context_keys = set(context_dict.keys())
-            if not context_keys.issubset(set(self.sql_database.get_table_names())):
+            if not context_keys.issubset(
+                set(self.sql_database.get_usable_table_names())
+            ):
                 raise ValueError(
                     "Invalid context table names: "
-                    f"{context_keys - set(self.sql_database.get_table_names())}"
+                    f"{context_keys - set(self.sql_database.get_usable_table_names())}"
                 )
         self.context_dict = context_dict or {}
         # build full context from sql_database
         self.full_context_dict = self._build_context_from_sql_database(
             self.sql_database, current_context=self.context_dict
         )
         self.context_str = context_str
 
     @classmethod
     def from_documents(
         cls,
-        documents_dict: Dict[str, List[BaseDocument]],
+        documents_dict: Dict[str, List[BaseNode]],
         sql_database: SQLDatabase,
         **context_builder_kwargs: Any,
     ) -> "SQLContextContainerBuilder":
         """Build context from documents."""
         context_builder = SQLDocumentContextBuilder(
             sql_database, **context_builder_kwargs
         )
@@ -75,15 +77,15 @@
         self,
         sql_database: SQLDatabase,
         current_context: Optional[Dict[str, str]] = None,
     ) -> Dict[str, str]:
         """Get tables schema + optional context as a single string."""
         current_context = current_context or {}
         result_context = {}
-        for table_name in sql_database.get_table_names():
+        for table_name in sql_database.get_usable_table_names():
             table_desc = sql_database.get_single_table_info(table_name)
             table_text = f"Schema of table {table_name}:\n" f"{table_desc}\n"
             if table_name in current_context:
                 table_text += f"Context of table {table_name}:\n"
                 table_text += current_context[table_name]
             result_context[table_name] = table_text
         return result_context
@@ -93,46 +95,46 @@
         if ignore_db_schema:
             return self.context_dict
         else:
             return self.full_context_dict
 
     def derive_index_from_context(
         self,
-        index_cls: Type[BaseGPTIndex],
+        index_cls: Type[BaseIndex],
         ignore_db_schema: bool = False,
         **index_kwargs: Any,
-    ) -> BaseGPTIndex:
+    ) -> BaseIndex:
         """Derive index from context."""
         full_context_dict = self._get_context_dict(ignore_db_schema)
         context_docs = []
         for table_name, context_str in full_context_dict.items():
-            doc = Document(context_str, extra_info={"table_name": table_name})
+            doc = Document(text=context_str, metadata={"table_name": table_name})
             context_docs.append(doc)
         index = index_cls.from_documents(
             documents=context_docs,
             **index_kwargs,
         )
         return index
 
     def query_index_for_context(
         self,
-        index: BaseGPTIndex,
+        index: BaseIndex,
         query_str: QueryType,
         query_tmpl: Optional[str] = DEFAULT_CONTEXT_QUERY_TMPL,
         store_context_str: bool = True,
         **index_kwargs: Any,
     ) -> str:
         """Query index for context.
 
         A simple wrapper around the index.query call which
         injects a query template to specifically fetch table information,
         and can store a context_str.
 
         Args:
-            index (BaseGPTIndex): index data structure
+            index (BaseIndex): index data structure
             query_str (QueryType): query string
             query_tmpl (Optional[str]): query template
             store_context_str (bool): store context_str
 
         """
         if query_tmpl is None:
             context_query_str = query_str
```

### Comparing `llama_index-0.6.9/llama_index/indices/struct_store/pandas.py` & `llama_index-0.7.0/llama_index/indices/struct_store/pandas.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,24 +1,29 @@
 """Pandas csv structured store."""
 
+import logging
 from typing import Any, Optional, Sequence
 
-from llama_index.data_structs.node import Node
+import pandas as pd
+
 from llama_index.data_structs.table import PandasStructTable
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.query.base import BaseQueryEngine
-from llama_index.indices.struct_store.base import BaseGPTStructStoreIndex
+from llama_index.indices.struct_store.base import BaseStructStoreIndex
+from llama_index.schema import BaseNode
+
+logger = logging.getLogger(__name__)
 
-import pandas as pd
 
+class PandasIndex(BaseStructStoreIndex[PandasStructTable]):
+    """Pandas Index.
 
-class GPTPandasIndex(BaseGPTStructStoreIndex[PandasStructTable]):
-    """Base GPT Pandas Index.
+    Deprecated. Please use :class:`PandasQueryEngine` instead.
 
-    The GPTPandasStructStoreIndex is an index that stores
+    The PandasIndex is an index that stores
     a Pandas dataframe under the hood.
     Currently index "construction" is not supported.
 
     During query time, the user can either specify a raw SQL query
     or a natural language query to retrieve their data.
 
     Args:
@@ -28,19 +33,24 @@
     """
 
     index_struct_cls = PandasStructTable
 
     def __init__(
         self,
         df: pd.DataFrame,
-        nodes: Optional[Sequence[Node]] = None,
+        nodes: Optional[Sequence[BaseNode]] = None,
         index_struct: Optional[PandasStructTable] = None,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
+        logger.warning(
+            "PandasIndex is deprecated. \
+            Please directly use `PandasQueryEngine(df)` instead."
+        )
+
         if nodes is not None:
             raise ValueError("We currently do not support indexing documents or nodes.")
         self.df = df
 
         super().__init__(
             nodes=[],
             index_struct=index_struct,
@@ -48,19 +58,23 @@
         )
 
     def as_retriever(self, **kwargs: Any) -> BaseRetriever:
         raise NotImplementedError("Not supported")
 
     def as_query_engine(self, **kwargs: Any) -> BaseQueryEngine:
         # NOTE: lazy import
-        from llama_index.indices.struct_store.pandas_query import GPTNLPandasQueryEngine
+        from llama_index.query_engine.pandas_query_engine import PandasQueryEngine
 
-        return GPTNLPandasQueryEngine(self, **kwargs)
+        return PandasQueryEngine.from_index(self, **kwargs)
 
-    def _build_index_from_nodes(self, nodes: Sequence[Node]) -> PandasStructTable:
+    def _build_index_from_nodes(self, nodes: Sequence[BaseNode]) -> PandasStructTable:
         """Build index from documents."""
         index_struct = self.index_struct_cls()
         return index_struct
 
-    def _insert(self, nodes: Sequence[Node], **insert_kwargs: Any) -> None:
+    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
         """Insert a document."""
         raise NotImplementedError("We currently do not support inserting documents.")
+
+
+# legacy
+GPTPandasIndex = PandasIndex
```

### Comparing `llama_index-0.6.9/llama_index/indices/struct_store/pandas_query.py` & `llama_index-0.7.0/llama_index/query_engine/pandas_query_engine.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,18 +1,19 @@
-"""Default query for GPTPandasIndex."""
+"""Default query for PandasIndex."""
 
 import logging
 from typing import Any, Callable, Optional
 
 import pandas as pd
-from langchain.input import print_text
+from llama_index.bridge.langchain import print_text
 
 from llama_index.indices.query.base import BaseQueryEngine
 from llama_index.indices.query.schema import QueryBundle
-from llama_index.indices.struct_store.pandas import GPTPandasIndex
+from llama_index.indices.service_context import ServiceContext
+from llama_index.indices.struct_store.pandas import PandasIndex
 from llama_index.prompts.default_prompts import DEFAULT_PANDAS_PROMPT
 from llama_index.prompts.prompts import PandasPrompt
 from llama_index.response.schema import Response
 
 logger = logging.getLogger(__name__)
 
 
@@ -60,15 +61,15 @@
             "There was an error running the output as Python code. "
             f"Error message: {e}"
         )
         traceback.print_exc()
         return err_string
 
 
-class GPTNLPandasQueryEngine(BaseQueryEngine):
+class PandasQueryEngine(BaseQueryEngine):
     """GPT Pandas query.
 
     Convert natural language to Pandas python code.
 
     Args:
         df (pd.DataFrame): Pandas dataframe to use.
         instruction_str (Optional[str]): Instruction string to use.
@@ -78,63 +79,76 @@
         pandas_prompt (Optional[PandasPrompt]): Pandas prompt to use.
         head (int): Number of rows to show in the table context.
 
     """
 
     def __init__(
         self,
-        index: GPTPandasIndex,
+        df: pd.DataFrame,
         instruction_str: Optional[str] = None,
         output_processor: Optional[Callable] = None,
         pandas_prompt: Optional[PandasPrompt] = None,
         output_kwargs: Optional[dict] = None,
         head: int = 5,
         verbose: bool = False,
+        service_context: Optional[ServiceContext] = None,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
-        self.df = index.df
-        self._service_context = index.service_context
+        self._df = df
 
         self._head = head
         self._pandas_prompt = pandas_prompt or DEFAULT_PANDAS_PROMPT
         self._instruction_str = instruction_str or DEFAULT_INSTRUCTION_STR
         self._output_processor = output_processor or default_output_processor
         self._output_kwargs = output_kwargs or {}
         self._verbose = verbose
+        self._service_context = service_context or ServiceContext.from_defaults()
+
+        super().__init__(self._service_context.callback_manager)
+
+    @classmethod
+    def from_index(cls, index: PandasIndex, **kwargs: Any) -> "PandasQueryEngine":
+        logger.warning(
+            "PandasIndex is deprecated. "
+            "Directly construct PandasQueryEngine with df instead."
+        )
+        return cls(df=index.df, service_context=index.service_context, **kwargs)
 
     def _get_table_context(self) -> str:
         """Get table context."""
-        return str(self.df.head(self._head))
+        return str(self._df.head(self._head))
 
     def _query(self, query_bundle: QueryBundle) -> Response:
         """Answer a query."""
         context = self._get_table_context()
 
-        (
-            pandas_response_str,
-            formatted_prompt,
-        ) = self._service_context.llm_predictor.predict(
+        pandas_response_str = self._service_context.llm_predictor.predict(
             self._pandas_prompt,
             df_str=context,
             query_str=query_bundle.query_str,
             instruction_str=self._instruction_str,
         )
 
         if self._verbose:
             print_text(f"> Pandas Instructions:\n" f"```\n{pandas_response_str}\n```\n")
         pandas_output = self._output_processor(
             pandas_response_str,
-            self.df,
+            self._df,
             **self._output_kwargs,
         )
         if self._verbose:
             print_text(f"> Pandas Output: {pandas_output}\n")
 
-        response_extra_info = {
+        response_metadata = {
             "pandas_instruction_str": pandas_response_str,
         }
 
-        return Response(response=pandas_output, extra_info=response_extra_info)
+        return Response(response=pandas_output, metadata=response_metadata)
 
     async def _aquery(self, query_bundle: QueryBundle) -> Response:
         return self._query(query_bundle)
+
+
+# legacy
+NLPandasQueryEngine = PandasQueryEngine
+GPTNLPandasQueryEngine = PandasQueryEngine
```

### Comparing `llama_index-0.6.9/llama_index/indices/struct_store/sql.py` & `llama_index-0.7.0/llama_index/indices/struct_store/sql.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,43 +1,45 @@
 """SQL Structured Store."""
 from enum import Enum
 from typing import Any, Optional, Sequence, Union
 
 from collections import defaultdict
-from llama_index.data_structs.node import Node
 from llama_index.data_structs.table import SQLStructTable
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.common.struct_store.schema import SQLContextContainer
 from llama_index.indices.common.struct_store.sql import SQLStructDatapointExtractor
 from llama_index.indices.query.base import BaseQueryEngine
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.struct_store.base import BaseGPTStructStoreIndex
+from llama_index.indices.struct_store.base import BaseStructStoreIndex
 from llama_index.indices.struct_store.container_builder import (
     SQLContextContainerBuilder,
 )
 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase
+from llama_index.schema import BaseNode
 from sqlalchemy import Table
 
 
 class SQLQueryMode(str, Enum):
     SQL = "sql"
     NL = "nl"
 
 
-class GPTSQLStructStoreIndex(BaseGPTStructStoreIndex[SQLStructTable]):
-    """Base GPT SQL Struct Store Index.
+class SQLStructStoreIndex(BaseStructStoreIndex[SQLStructTable]):
+    """SQL Struct Store Index.
 
-    The GPTSQLStructStoreIndex is an index that uses a SQL database
+    The SQLStructStoreIndex is an index that uses a SQL database
     under the hood. During index construction, the data can be inferred
     from unstructured documents given a schema extract prompt,
     or it can be pre-loaded in the database.
 
     During query time, the user can either specify a raw SQL query
     or a natural language query to retrieve their data.
 
+    NOTE: this is deprecated.
+
     Args:
         documents (Optional[Sequence[DOCUMENTS_INPUT]]): Documents to index.
             NOTE: in the SQL index, this is an optional field.
         sql_database (Optional[SQLDatabase]): SQL database to use,
             including table names to specify.
             See :ref:`Ref-Struct-Store` for more details.
         table_name (Optional[str]): Name of the table to use
@@ -53,15 +55,15 @@
 
     """
 
     index_struct_cls = SQLStructTable
 
     def __init__(
         self,
-        nodes: Optional[Sequence[Node]] = None,
+        nodes: Optional[Sequence[BaseNode]] = None,
         index_struct: Optional[SQLStructTable] = None,
         service_context: Optional[ServiceContext] = None,
         sql_database: Optional[SQLDatabase] = None,
         table_name: Optional[str] = None,
         table: Optional[Table] = None,
         ref_doc_id_column: Optional[str] = None,
         sql_context_container: Optional[SQLContextContainer] = None,
@@ -94,15 +96,15 @@
             sql_context_container = container_builder.build_context_container()
         self.sql_context_container = sql_context_container
 
     @property
     def ref_doc_id_column(self) -> Optional[str]:
         return self._ref_doc_id_column
 
-    def _build_index_from_nodes(self, nodes: Sequence[Node]) -> SQLStructTable:
+    def _build_index_from_nodes(self, nodes: Sequence[BaseNode]) -> SQLStructTable:
         """Build index from nodes."""
         index_struct = self.index_struct_cls()
         if len(nodes) == 0:
             return index_struct
         else:
             data_extractor = SQLStructDatapointExtractor(
                 self._service_context.llm_predictor,
@@ -118,15 +120,15 @@
             for node in nodes:
                 source_to_node[node.ref_doc_id].append(node)
 
             for _, node_set in source_to_node.items():
                 data_extractor.insert_datapoint_from_nodes(node_set)
         return index_struct
 
-    def _insert(self, nodes: Sequence[Node], **insert_kwargs: Any) -> None:
+    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
         """Insert a document."""
         data_extractor = SQLStructDatapointExtractor(
             self._service_context.llm_predictor,
             self.schema_extract_prompt,
             self.output_parser,
             self.sql_database,
             table_name=self._table_name,
@@ -139,17 +141,20 @@
         raise NotImplementedError("Not supported")
 
     def as_query_engine(
         self, query_mode: Union[str, SQLQueryMode] = SQLQueryMode.NL, **kwargs: Any
     ) -> BaseQueryEngine:
         # NOTE: lazy import
         from llama_index.indices.struct_store.sql_query import (
-            GPTNLStructStoreQueryEngine,
-            GPTSQLStructStoreQueryEngine,
+            NLStructStoreQueryEngine,
+            SQLStructStoreQueryEngine,
         )
 
         if query_mode == SQLQueryMode.NL:
-            return GPTNLStructStoreQueryEngine(self, **kwargs)
+            return NLStructStoreQueryEngine(self, **kwargs)
         elif query_mode == SQLQueryMode.SQL:
-            return GPTSQLStructStoreQueryEngine(self, **kwargs)
+            return SQLStructStoreQueryEngine(self, **kwargs)
         else:
             raise ValueError(f"Unknown query mode: {query_mode}")
+
+
+GPTSQLStructStoreIndex = SQLStructStoreIndex
```

### Comparing `llama_index-0.6.9/llama_index/indices/struct_store/sql_query.py` & `llama_index-0.7.0/llama_index/indices/struct_store/json_query.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,152 +1,178 @@
-"""Default query for GPTSQLStructStoreIndex."""
+import json
 import logging
-from typing import Any, Optional
+from typing import Any, Callable, Dict, List, Optional, Union
+
+from llama_index.bridge.langchain import print_text
 
 from llama_index.indices.query.base import BaseQueryEngine
 from llama_index.indices.query.schema import QueryBundle
-from llama_index.indices.struct_store.container_builder import (
-    SQLContextContainerBuilder,
-)
-from llama_index.indices.struct_store.sql import GPTSQLStructStoreIndex
-from llama_index.prompts.default_prompts import DEFAULT_TEXT_TO_SQL_PROMPT
-from llama_index.prompts.prompts import TextToSQLPrompt
+from llama_index.indices.service_context import ServiceContext
+from llama_index.prompts.base import Prompt
+from llama_index.prompts.default_prompts import DEFAULT_JSON_PATH_PROMPT
+from llama_index.prompts.prompt_type import PromptType
 from llama_index.response.schema import Response
-from llama_index.token_counter.token_counter import llm_token_counter
 
 logger = logging.getLogger(__name__)
+IMPORT_ERROR_MSG = (
+    "`jsonpath_ng` package not found, please run `pip install jsonpath-ng`"
+)
+
+JSONType = Union[Dict[str, "JSONType"], List["JSONType"], str, int, float, bool, None]
 
 
-class GPTSQLStructStoreQueryEngine(BaseQueryEngine):
-    """GPT SQL query engine over a structured database.
+DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL = (
+    "Given an input question about a JSON value, synthesize a response "
+    "from the query results.\n"
+    "Query: {query_str}\n"
+    "JSON Schema: {json_schema}\n"
+    "JSON Path: {json_path}\n"
+    "Value at path: {json_path_value}\n"
+    "Response: "
+)
+DEFAULT_RESPONSE_SYNTHESIS_PROMPT = Prompt(
+    DEFAULT_RESPONSE_SYNTHESIS_PROMPT_TMPL,
+    prompt_type=PromptType.SQL_RESPONSE_SYNTHESIS,
+)
 
-    Runs raw SQL over a GPTSQLStructStoreIndex. No LLM calls are made here.
-    NOTE: this query cannot work with composed indices - if the index
-    contains subindices, those subindices will not be queried.
+
+def default_output_processor(llm_output: str, json_value: JSONType) -> JSONType:
+    """Default output processor that executes the JSON Path query."""
+    try:
+        from jsonpath_ng.ext import parse
+        from jsonpath_ng.jsonpath import DatumInContext
+    except ImportError as exc:
+        raise ImportError(IMPORT_ERROR_MSG) from exc
+
+    datum: List[DatumInContext] = parse(llm_output).find(json_value)
+    return [d.value for d in datum]
+
+
+class JSONQueryEngine(BaseQueryEngine):
+    """GPT JSON Query Engine.
+
+    Converts natural language to JSON Path queries.
+
+    Args:
+        json_value (JSONType): JSON value
+        json_schema (JSONType): JSON schema
+        service_context (ServiceContext): ServiceContext
+        json_path_prompt (Prompt): The JSON Path prompt to use.
+        output_processor (Callable): The output processor that executes the
+            JSON Path query.
+        output_kwargs (dict): Additional output processor kwargs for the
+            output_processor function.
+        verbose (bool): Whether to print verbose output.
     """
 
     def __init__(
         self,
-        index: GPTSQLStructStoreIndex,
-        sql_context_container: Optional[SQLContextContainerBuilder] = None,
+        json_value: JSONType,
+        json_schema: JSONType,
+        service_context: ServiceContext,
+        json_path_prompt: Optional[Prompt] = None,
+        output_processor: Optional[Callable] = None,
+        output_kwargs: Optional[dict] = None,
+        synthesize_response: bool = True,
+        response_synthesis_prompt: Optional[Prompt] = None,
+        verbose: bool = False,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
-        self._sql_database = index.sql_database
-        self._sql_context_container = (
-            sql_context_container or index.sql_context_container
+        self._json_value = json_value
+        self._json_schema = json_schema
+        self._service_context = service_context
+        self._json_path_prompt = json_path_prompt or DEFAULT_JSON_PATH_PROMPT
+        self._output_processor = output_processor or default_output_processor
+        self._output_kwargs = output_kwargs or {}
+        self._verbose = verbose
+        self._synthesize_response = synthesize_response
+        self._response_synthesis_prompt = (
+            response_synthesis_prompt or DEFAULT_RESPONSE_SYNTHESIS_PROMPT
         )
 
+        super().__init__(self._service_context.callback_manager)
+
+    def _get_schema_context(self) -> str:
+        """Get JSON schema context."""
+        return json.dumps(self._json_schema)
+
     def _query(self, query_bundle: QueryBundle) -> Response:
         """Answer a query."""
-        # NOTE: override query method in order to fetch the right results.
-        # NOTE: since the query_str is a SQL query, it doesn't make sense
-        # to use ResponseBuilder anywhere.
-        response_str, extra_info = self._sql_database.run_sql(query_bundle.query_str)
-        response = Response(response=response_str, extra_info=extra_info)
-        return response
-
-    async def _aquery(self, query_bundle: QueryBundle) -> Response:
-        return self._query(query_bundle)
+        schema = self._get_schema_context()
 
+        json_path_response_str = self._service_context.llm_predictor.predict(
+            self._json_path_prompt,
+            schema=schema,
+            query_str=query_bundle.query_str,
+        )
 
-class GPTNLStructStoreQueryEngine(BaseQueryEngine):
-    """GPT natural language query engine over a structured database.
+        if self._verbose:
+            print_text(
+                f"> JSONPath Instructions:\n" f"```\n{json_path_response_str}\n```\n"
+            )
+
+        json_path_output = self._output_processor(
+            json_path_response_str,
+            self._json_value,
+            **self._output_kwargs,
+        )
 
-    Given a natural language query, we will extract the query to SQL.
-    Runs raw SQL over a GPTSQLStructStoreIndex. No LLM calls are made during
-    the SQL execution.
-    NOTE: this query cannot work with composed indices - if the index
-    contains subindices, those subindices will not be queried.
-    """
+        if self._verbose:
+            print_text(f"> JSONPath Output: {json_path_output}\n")
 
-    def __init__(
-        self,
-        index: GPTSQLStructStoreIndex,
-        text_to_sql_prompt: Optional[TextToSQLPrompt] = None,
-        context_query_kwargs: Optional[dict] = None,
-        **kwargs: Any,
-    ) -> None:
-        """Initialize params."""
-        self._index = index
-        self._sql_database = index.sql_database
-        self._sql_context_container = index.sql_context_container
-        self._service_context = index.service_context
-        self._ref_doc_id_column = index.ref_doc_id_column
-
-        self._text_to_sql_prompt = text_to_sql_prompt or DEFAULT_TEXT_TO_SQL_PROMPT
-        self._context_query_kwargs = context_query_kwargs or {}
-
-    def _parse_response_to_sql(self, response: str) -> str:
-        """Parse response to SQL."""
-        result_response = response.strip()
-        return result_response
-
-    def _get_table_context(self, query_bundle: QueryBundle) -> str:
-        """Get table context.
-
-        Get tables schema + optional context as a single string. Taken from
-        SQLContextContainer.
-
-        """
-        if self._sql_context_container.context_str is not None:
-            tables_desc_str = self._sql_context_container.context_str
+        if self._synthesize_response:
+            response_str = self._service_context.llm_predictor.predict(
+                self._response_synthesis_prompt,
+                query_str=query_bundle.query_str,
+                json_schema=self._json_schema,
+                json_path=json_path_response_str,
+                json_path_value=json_path_output,
+            )
         else:
-            table_desc_list = []
-            context_dict = self._sql_context_container.context_dict
-            if context_dict is None:
-                raise ValueError(
-                    "context_dict must be provided. There is currently no "
-                    "table context."
-                )
-            for table_desc in context_dict.values():
-                table_desc_list.append(table_desc)
-            tables_desc_str = "\n\n".join(table_desc_list)
+            response_str = json.dumps(json_path_output)
 
-        return tables_desc_str
+        response_metadata = {
+            "json_path_response_str": json_path_response_str,
+        }
 
-    @llm_token_counter("query")
-    def _query(self, query_bundle: QueryBundle) -> Response:
-        """Answer a query."""
-        table_desc_str = self._get_table_context(query_bundle)
-        logger.info(f"> Table desc str: {table_desc_str}")
+        return Response(response=response_str, metadata=response_metadata)
+
+    async def _aquery(self, query_bundle: QueryBundle) -> Response:
+        schema = self._get_schema_context()
 
-        response_str, _ = self._service_context.llm_predictor.predict(
-            self._text_to_sql_prompt,
+        json_path_response_str = await self._service_context.llm_predictor.apredict(
+            self._json_path_prompt,
+            schema=schema,
             query_str=query_bundle.query_str,
-            schema=table_desc_str,
-            dialect=self._sql_database.dialect,
         )
 
-        sql_query_str = self._parse_response_to_sql(response_str)
-        # assume that it's a valid SQL query
-        logger.debug(f"> Predicted SQL query: {sql_query_str}")
-
-        response_str, extra_info = self._sql_database.run_sql(sql_query_str)
-        extra_info["sql_query"] = sql_query_str
-        response = Response(response=response_str, extra_info=extra_info)
-        return response
+        if self._verbose:
+            print_text(
+                f"> JSONPath Instructions:\n" f"```\n{json_path_response_str}\n```\n"
+            )
+
+        json_path_output = self._output_processor(
+            json_path_response_str,
+            self._json_value,
+            **self._output_kwargs,
+        )
 
-    @llm_token_counter("aquery")
-    async def _aquery(self, query_bundle: QueryBundle) -> Response:
-        """Answer a query."""
-        table_desc_str = self._get_table_context(query_bundle)
-        logger.info(f"> Table desc str: {table_desc_str}")
+        if self._verbose:
+            print_text(f"> JSONPath Output: {json_path_output}\n")
 
-        (
-            response_str,
-            formatted_prompt,
-        ) = await self._service_context.llm_predictor.apredict(
-            self._text_to_sql_prompt,
-            query_str=query_bundle.query_str,
-            schema=table_desc_str,
-            dialect=self._sql_database.dialect,
-        )
+        if self._synthesize_response:
+            response_str = await self._service_context.llm_predictor.apredict(
+                self._response_synthesis_prompt,
+                query_str=query_bundle.query_str,
+                json_schema=self._json_schema,
+                json_path=json_path_response_str,
+                json_path_value=json_path_output,
+            )
+        else:
+            response_str = json.dumps(json_path_output)
+
+        response_metadata = {
+            "json_path_response_str": json_path_response_str,
+        }
 
-        sql_query_str = self._parse_response_to_sql(response_str)
-        # assume that it's a valid SQL query
-        logger.debug(f"> Predicted SQL query: {sql_query_str}")
-
-        response_str, extra_info = self._sql_database.run_sql(sql_query_str)
-        extra_info["sql_query"] = sql_query_str
-        response = Response(response=response_str, extra_info=extra_info)
-        return response
+        return Response(response=response_str, metadata=response_metadata)
```

### Comparing `llama_index-0.6.9/llama_index/indices/tree/__init__.py` & `llama_index-0.7.0/llama_index/indices/tree/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 """Tree-structured Index Data Structures."""
 
 # indices
-from llama_index.indices.tree.base import GPTTreeIndex
+from llama_index.indices.tree.base import TreeIndex, GPTTreeIndex
 from llama_index.indices.tree.select_leaf_embedding_retriever import (
     TreeSelectLeafEmbeddingRetriever,
 )
 from llama_index.indices.tree.select_leaf_retriever import TreeSelectLeafRetriever
 from llama_index.indices.tree.all_leaf_retriever import TreeAllLeafRetriever
 from llama_index.indices.tree.tree_root_retriever import TreeRootRetriever
 
 __all__ = [
-    "GPTTreeIndex",
+    "TreeIndex",
     "TreeSelectLeafEmbeddingRetriever",
     "TreeSelectLeafRetriever",
     "TreeAllLeafRetriever",
     "TreeRootRetriever",
+    # legacy
+    "GPTTreeIndex",
 ]
```

### Comparing `llama_index-0.6.9/llama_index/indices/tree/all_leaf_retriever.py` & `llama_index-0.7.0/llama_index/indices/tree/all_leaf_retriever.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 """Summarize query."""
 
 import logging
-from typing import Any, List, cast
-
+from typing import List, cast
 
 from llama_index.data_structs.data_structs import IndexGraph
-from llama_index.data_structs.node import NodeWithScore
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.query.schema import QueryBundle
+from llama_index.indices.tree.base import TreeIndex
 from llama_index.indices.utils import get_sorted_node_list
+from llama_index.schema import NodeWithScore
 
 logger = logging.getLogger(__name__)
 
 DEFAULT_NUM_CHILDREN = 10
 
 
 class TreeAllLeafRetriever(BaseRetriever):
@@ -24,25 +24,22 @@
 
     Args:
         text_qa_template (Optional[QuestionAnswerPrompt]): Question-Answer Prompt
             (see :ref:`Prompt-Templates`).
 
     """
 
-    def __init__(self, index: Any):
-        from llama_index.indices.tree.base import GPTTreeIndex
-
-        assert isinstance(index, GPTTreeIndex)
+    def __init__(self, index: TreeIndex):
         self._index = index
         self._index_struct = index.index_struct
         self._docstore = index.docstore
 
     def _retrieve(
         self,
         query_bundle: QueryBundle,
     ) -> List[NodeWithScore]:
         """Get nodes for response."""
         logger.info(f"> Starting query: {query_bundle.query_str}")
         index_struct = cast(IndexGraph, self._index_struct)
         all_nodes = self._docstore.get_node_dict(index_struct.all_nodes)
         sorted_node_list = get_sorted_node_list(all_nodes)
-        return [NodeWithScore(node) for node in sorted_node_list]
+        return [NodeWithScore(node=node) for node in sorted_node_list]
```

### Comparing `llama_index-0.6.9/llama_index/indices/tree/inserter.py` & `llama_index-0.7.0/llama_index/indices/tree/inserter.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,28 +1,29 @@
-"""GPT Tree Index inserter."""
+"""Tree Index inserter."""
 
 from typing import Optional, Sequence
 
 from llama_index.data_structs.data_structs import IndexGraph
-from llama_index.data_structs.node import Node
+from llama_index.indices.tree.utils import get_numbered_text_from_nodes
 from llama_index.storage.docstore import BaseDocumentStore
 from llama_index.storage.docstore.registry import get_default_docstore
 from llama_index.indices.service_context import ServiceContext
 from llama_index.indices.utils import (
     extract_numbers_given_response,
     get_sorted_node_list,
 )
 from llama_index.prompts.base import Prompt
 from llama_index.prompts.default_prompts import (
     DEFAULT_INSERT_PROMPT,
     DEFAULT_SUMMARY_PROMPT,
 )
+from llama_index.schema import BaseNode, TextNode, MetadataMode
 
 
-class GPTTreeIndexInserter:
+class TreeIndexInserter:
     """LlamaIndex inserter."""
 
     def __init__(
         self,
         index_graph: IndexGraph,
         service_context: ServiceContext,
         num_children: int = 10,
@@ -37,15 +38,15 @@
         self.summary_prompt = summary_prompt
         self.insert_prompt = insert_prompt
         self.index_graph = index_graph
         self._service_context = service_context
         self._docstore = docstore or get_default_docstore()
 
     def _insert_under_parent_and_consolidate(
-        self, text_node: Node, parent_node: Optional[Node]
+        self, text_node: BaseNode, parent_node: Optional[BaseNode]
     ) -> None:
         """Insert node under parent and consolidate.
 
         Consolidation will happen by dividing up child nodes, and creating a new
         intermediate layer of nodes.
 
         """
@@ -62,75 +63,84 @@
             cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)
             # this layer is all leaf nodes, consolidate and split leaf nodes
             # consolidate and split leaf nodes in half
             # TODO: do better splitting (with a GPT prompt etc.)
             half1 = cur_graph_node_list[: len(cur_graph_nodes) // 2]
             half2 = cur_graph_node_list[len(cur_graph_nodes) // 2 :]
 
-            text_chunk1 = self._service_context.prompt_helper.get_text_from_nodes(
-                half1, prompt=self.summary_prompt
+            truncated_chunks = self._service_context.prompt_helper.truncate(
+                prompt=self.summary_prompt,
+                text_chunks=[
+                    node.get_content(metadata_mode=MetadataMode.LLM) for node in half1
+                ],
             )
-            summary1, _ = self._service_context.llm_predictor.predict(
+            text_chunk1 = "\n".join(truncated_chunks)
+
+            summary1 = self._service_context.llm_predictor.predict(
                 self.summary_prompt, context_str=text_chunk1
             )
-            node1 = Node(
-                text=summary1,
-            )
+            node1 = TextNode(text=summary1)
             self.index_graph.insert(node1, children_nodes=half1)
 
-            text_chunk2 = self._service_context.prompt_helper.get_text_from_nodes(
-                half2, prompt=self.summary_prompt
+            truncated_chunks = self._service_context.prompt_helper.truncate(
+                prompt=self.summary_prompt,
+                text_chunks=[
+                    node.get_content(metadata_mode=MetadataMode.LLM) for node in half2
+                ],
             )
-            summary2, _ = self._service_context.llm_predictor.predict(
+            text_chunk2 = "\n".join(truncated_chunks)
+            summary2 = self._service_context.llm_predictor.predict(
                 self.summary_prompt, context_str=text_chunk2
             )
-            node2 = Node(
-                text=summary2,
-            )
+            node2 = TextNode(text=summary2)
             self.index_graph.insert(node2, children_nodes=half2)
 
             # insert half1 and half2 as new children of parent_node
             # first remove child indices from parent node
             if parent_node is not None:
-                self.index_graph.node_id_to_children_ids[
-                    parent_node.get_doc_id()
-                ] = list()
+                self.index_graph.node_id_to_children_ids[parent_node.node_id] = list()
             else:
                 self.index_graph.root_nodes = {}
             self.index_graph.insert_under_parent(
                 node1, parent_node, new_index=self.index_graph.get_index(node1)
             )
             self._docstore.add_documents([node1], allow_update=False)
             self.index_graph.insert_under_parent(
                 node2, parent_node, new_index=self.index_graph.get_index(node2)
             )
             self._docstore.add_documents([node2], allow_update=False)
 
-    def _insert_node(self, node: Node, parent_node: Optional[Node] = None) -> None:
+    def _insert_node(
+        self, node: BaseNode, parent_node: Optional[BaseNode] = None
+    ) -> None:
         """Insert node."""
         cur_graph_node_ids = self.index_graph.get_children(parent_node)
         cur_graph_nodes = self._docstore.get_node_dict(cur_graph_node_ids)
         cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)
         # if cur_graph_nodes is empty (start with empty graph), then insert under
         # parent (insert new root node)
         if len(cur_graph_nodes) == 0:
             self._insert_under_parent_and_consolidate(node, parent_node)
         # check if leaf nodes, then just insert under parent
         elif len(self.index_graph.get_children(cur_graph_node_list[0])) == 0:
             self._insert_under_parent_and_consolidate(node, parent_node)
         # else try to find the right summary node to insert under
         else:
-            numbered_text = (
-                self._service_context.prompt_helper.get_numbered_text_from_nodes(
-                    cur_graph_node_list, prompt=self.insert_prompt
+            text_splitter = (
+                self._service_context.prompt_helper.get_text_splitter_given_prompt(
+                    prompt=self.insert_prompt,
+                    num_chunks=len(cur_graph_node_list),
                 )
             )
-            response, _ = self._service_context.llm_predictor.predict(
+            numbered_text = get_numbered_text_from_nodes(
+                cur_graph_node_list, text_splitter=text_splitter
+            )
+            response = self._service_context.llm_predictor.predict(
                 self.insert_prompt,
-                new_chunk_text=node.get_text(),
+                new_chunk_text=node.get_content(metadata_mode=MetadataMode.LLM),
                 num_chunks=len(cur_graph_node_list),
                 context_list=numbered_text,
             )
             numbers = extract_numbers_given_response(response)
             if numbers is None or len(numbers) == 0:
                 # NOTE: if we can't extract a number, then we just insert under parent
                 self._insert_under_parent_and_consolidate(node, parent_node)
@@ -144,22 +154,25 @@
         # now we need to update summary for parent node, since we
         # need to bubble updated summaries up the tree
         if parent_node is not None:
             # refetch children
             cur_graph_node_ids = self.index_graph.get_children(parent_node)
             cur_graph_nodes = self._docstore.get_node_dict(cur_graph_node_ids)
             cur_graph_node_list = get_sorted_node_list(cur_graph_nodes)
-            text_chunk = self._service_context.prompt_helper.get_text_from_nodes(
-                cur_graph_node_list, prompt=self.summary_prompt
+            truncated_chunks = self._service_context.prompt_helper.truncate(
+                prompt=self.summary_prompt,
+                text_chunks=[
+                    node.get_content(metadata_mode=MetadataMode.LLM)
+                    for node in cur_graph_node_list
+                ],
             )
-            new_summary, _ = self._service_context.llm_predictor.predict(
+            text_chunk = "\n".join(truncated_chunks)
+            new_summary = self._service_context.llm_predictor.predict(
                 self.summary_prompt, context_str=text_chunk
             )
 
-            parent_node.text = new_summary
+            parent_node.set_content(new_summary)
 
-    def insert(self, nodes: Sequence[Node]) -> None:
+    def insert(self, nodes: Sequence[BaseNode]) -> None:
         """Insert into index_graph."""
-        print("calling insert")
-        print(nodes)
         for node in nodes:
             self._insert_node(node)
```

### Comparing `llama_index-0.6.9/llama_index/indices/tree/select_leaf_embedding_retriever.py` & `llama_index-0.7.0/llama_index/indices/tree/select_leaf_embedding_retriever.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 """Query Tree using embedding similarity between query and node text."""
 
 import logging
 from typing import Dict, List, Tuple, cast
 
 
-from llama_index.data_structs.node import Node
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.tree.select_leaf_retriever import (
     TreeSelectLeafRetriever,
 )
 from llama_index.indices.utils import get_sorted_node_list
+from llama_index.schema import BaseNode, MetadataMode
 
 logger = logging.getLogger(__name__)
 
 
 class TreeSelectLeafEmbeddingRetriever(TreeSelectLeafRetriever):
     """Tree select leaf embedding retriever.
 
@@ -57,26 +57,26 @@
             cur_node_list, query_bundle
         )
 
         result_response = None
         for node, index in zip(selected_nodes, selected_indices):
             logger.debug(
                 f">[Level {level}] Node [{index+1}] Summary text: "
-                f"{' '.join(node.get_text().splitlines())}"
+                f"{' '.join(node.get_content().splitlines())}"
             )
 
             # Get the response for the selected node
             result_response = self._query_with_selected_node(
                 node, query_bundle, level=level, prev_response=result_response
             )
 
         return cast(str, result_response)
 
     def _get_query_text_embedding_similarities(
-        self, query_bundle: QueryBundle, nodes: List[Node]
+        self, query_bundle: QueryBundle, nodes: List[BaseNode]
     ) -> List[float]:
         """
         Get query text embedding similarity.
 
         Cache the query embedding and the node text embedding.
 
         """
@@ -86,43 +86,43 @@
                     query_bundle.embedding_strs
                 )
             )
         similarities = []
         for node in nodes:
             if node.embedding is None:
                 node.embedding = self._service_context.embed_model.get_text_embedding(
-                    node.get_text()
+                    node.get_content(metadata_mode=MetadataMode.EMBED)
                 )
 
             similarity = self._service_context.embed_model.similarity(
                 query_bundle.embedding, node.embedding
             )
             similarities.append(similarity)
         return similarities
 
     def _get_most_similar_nodes(
-        self, nodes: List[Node], query_bundle: QueryBundle
-    ) -> Tuple[List[Node], List[int]]:
+        self, nodes: List[BaseNode], query_bundle: QueryBundle
+    ) -> Tuple[List[BaseNode], List[int]]:
         """Get the node with the highest similarity to the query."""
         similarities = self._get_query_text_embedding_similarities(query_bundle, nodes)
 
-        selected_nodes: List[Node] = []
+        selected_nodes: List[BaseNode] = []
         selected_indices: List[int] = []
         for node, _ in sorted(
             zip(nodes, similarities), key=lambda x: x[1], reverse=True
         ):
             if len(selected_nodes) < self.child_branch_factor:
                 selected_nodes.append(node)
                 selected_indices.append(nodes.index(node))
             else:
                 break
 
         return selected_nodes, selected_indices
 
     def _select_nodes(
         self,
-        cur_node_list: List[Node],
+        cur_node_list: List[BaseNode],
         query_bundle: QueryBundle,
         level: int = 0,
-    ) -> List[Node]:
+    ) -> List[BaseNode]:
         selected_nodes, _ = self._get_most_similar_nodes(cur_node_list, query_bundle)
         return selected_nodes
```

### Comparing `llama_index-0.6.9/llama_index/indices/tree/select_leaf_retriever.py` & `llama_index-0.7.0/llama_index/indices/tree/select_leaf_retriever.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,19 +1,18 @@
 """Leaf query mechanism."""
 
 import logging
 from typing import Any, Dict, List, Optional, cast
 
-from langchain.input import print_text
+from llama_index.bridge.langchain import print_text
 
-from llama_index.data_structs.node import Node, NodeWithScore
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.query.schema import QueryBundle
-from llama_index.indices.response import get_response_builder
-from llama_index.indices.tree.base import GPTTreeIndex
+from llama_index.indices.tree.base import TreeIndex
+from llama_index.indices.tree.utils import get_numbered_text_from_nodes
 from llama_index.indices.utils import (
     extract_numbers_given_response,
     get_sorted_node_list,
 )
 from llama_index.prompts.default_prompt_selectors import DEFAULT_REFINE_PROMPT_SEL
 from llama_index.prompts.default_prompts import (
     DEFAULT_QUERY_PROMPT,
@@ -23,31 +22,32 @@
 from llama_index.prompts.prompts import (
     QuestionAnswerPrompt,
     RefinePrompt,
     TreeSelectMultiplePrompt,
     TreeSelectPrompt,
 )
 from llama_index.response.schema import Response
-from llama_index.token_counter.token_counter import llm_token_counter
+from llama_index.response_synthesizers import get_response_synthesizer
+from llama_index.schema import BaseNode, NodeWithScore, MetadataMode
 from llama_index.utils import truncate_text
 
 logger = logging.getLogger(__name__)
 
 
 def get_text_from_node(
-    node: Node,
+    node: BaseNode,
     level: Optional[int] = None,
     verbose: bool = False,
 ) -> str:
     """Get text from node."""
     level_str = "" if level is None else f"[Level {level}]"
-    fmt_text_chunk = truncate_text(node.get_text(), 50)
+    fmt_text_chunk = truncate_text(node.get_content(metadata_mode=MetadataMode.LLM), 50)
     logger.debug(f">{level_str} Searching in chunk: {fmt_text_chunk}")
 
-    response_txt = node.get_text()
+    response_txt = node.get_content(metadata_mode=MetadataMode.LLM)
     fmt_response = truncate_text(response_txt, 200)
     if verbose:
         print_text(f">{level_str} Got node text: {fmt_response}\n", color="blue")
     return response_txt
 
 
 class TreeSelectLeafRetriever(BaseRetriever):
@@ -67,15 +67,15 @@
             to traverse for any given parent node.
             If child_branch_factor is 2, then the query will choose two child nodes.
 
     """
 
     def __init__(
         self,
-        index: GPTTreeIndex,
+        index: TreeIndex,
         query_template: Optional[TreeSelectPrompt] = None,
         text_qa_template: Optional[QuestionAnswerPrompt] = None,
         refine_template: Optional[RefinePrompt] = None,
         query_template_multiple: Optional[TreeSelectMultiplePrompt] = None,
         child_branch_factor: int = 1,
         verbose: bool = False,
         **kwargs: Any,
@@ -92,32 +92,32 @@
             query_template_multiple or DEFAULT_QUERY_PROMPT_MULTIPLE
         )
         self.child_branch_factor = child_branch_factor
         self._verbose = verbose
 
     def _query_with_selected_node(
         self,
-        selected_node: Node,
+        selected_node: BaseNode,
         query_bundle: QueryBundle,
         prev_response: Optional[str] = None,
         level: int = 0,
     ) -> str:
         """Get response for selected node.
 
         If not leaf node, it will recursively call _query on the child nodes.
         If prev_response is provided, we will update prev_response with the answer.
 
         """
         query_str = query_bundle.query_str
 
         if len(self._index_struct.get_children(selected_node)) == 0:
-            response_builder = get_response_builder(
-                self._service_context,
-                self._text_qa_template,
-                self._refine_template,
+            response_builder = get_response_synthesizer(
+                service_context=self._service_context,
+                text_qa_template=self._text_qa_template,
+                refine_template=self._refine_template,
             )
             # use response builder to get answer from node
             node_text = get_text_from_node(selected_node, level=level)
             cur_response = response_builder.get_response(
                 query_str, [node_text], prev_response=prev_response
             )
             cur_response = cast(str, cur_response)
@@ -128,26 +128,22 @@
                 query_bundle,
                 level=level + 1,
             )
 
         if prev_response is None:
             return cur_response
         else:
-            context_msg = selected_node.get_text()
-            (
-                cur_response,
-                formatted_refine_prompt,
-            ) = self._service_context.llm_predictor.predict(
+            context_msg = selected_node.get_content(metadata_mode=MetadataMode.LLM)
+            cur_response = self._service_context.llm_predictor.predict(
                 self._refine_template,
                 query_str=query_str,
                 existing_answer=prev_response,
                 context_msg=context_msg,
             )
 
-            logger.debug(f">[Level {level}] Refine prompt: {formatted_refine_prompt}")
             logger.debug(f">[Level {level}] Current refined response: {cur_response} ")
             return cur_response
 
     def _query_level(
         self,
         cur_node_ids: Dict[int, str],
         query_bundle: QueryBundle,
@@ -166,51 +162,50 @@
             return self._query_with_selected_node(
                 cur_node_list[0], query_bundle, level=level
             )
         elif self.child_branch_factor == 1:
             query_template = self.query_template.partial_format(
                 num_chunks=len(cur_node_list), query_str=query_str
             )
-            numbered_node_text = (
-                self._service_context.prompt_helper.get_numbered_text_from_nodes(
-                    cur_node_list, prompt=query_template
+            text_splitter = (
+                self._service_context.prompt_helper.get_text_splitter_given_prompt(
+                    prompt=query_template,
+                    num_chunks=len(cur_node_list),
                 )
             )
-            (
-                response,
-                formatted_query_prompt,
-            ) = self._service_context.llm_predictor.predict(
+            numbered_node_text = get_numbered_text_from_nodes(
+                cur_node_list, text_splitter=text_splitter
+            )
+
+            response = self._service_context.llm_predictor.predict(
                 query_template,
                 context_list=numbered_node_text,
             )
         else:
             query_template_multiple = self.query_template_multiple.partial_format(
                 num_chunks=len(cur_node_list),
                 query_str=query_str,
                 branching_factor=self.child_branch_factor,
             )
-            numbered_node_text = (
-                self._service_context.prompt_helper.get_numbered_text_from_nodes(
-                    cur_node_list, prompt=query_template_multiple
+
+            text_splitter = (
+                self._service_context.prompt_helper.get_text_splitter_given_prompt(
+                    prompt=query_template_multiple,
+                    num_chunks=len(cur_node_list),
                 )
             )
-            (
-                response,
-                formatted_query_prompt,
-            ) = self._service_context.llm_predictor.predict(
+            numbered_node_text = get_numbered_text_from_nodes(
+                cur_node_list, text_splitter=text_splitter
+            )
+
+            response = self._service_context.llm_predictor.predict(
                 query_template_multiple,
                 context_list=numbered_node_text,
             )
 
-        logger.debug(
-            f">[Level {level}] current prompt template: {formatted_query_prompt}"
-        )
-        self._service_context.llama_logger.add_log(
-            {"formatted_prompt_template": formatted_query_prompt, "level": level}
-        )
         debug_str = f">[Level {level}] Current response: {response}"
         logger.debug(debug_str)
         if self._verbose:
             print_text(debug_str, end="\n")
 
         numbers = extract_numbers_given_response(response, n=self.child_branch_factor)
         if numbers is None:
@@ -238,19 +233,21 @@
             info_str = (
                 f">[Level {level}] Selected node: "
                 f"[{number}]/[{','.join([str(int(n)) for n in numbers])}]"
             )
             logger.info(info_str)
             if self._verbose:
                 print_text(info_str, end="\n")
-            debug_str = " ".join(selected_node.get_text().splitlines())
+            debug_str = " ".join(
+                selected_node.get_content(metadata_mode=MetadataMode.LLM).splitlines()
+            )
             full_debug_str = (
                 f">[Level {level}] Node "
                 f"[{number}] Summary text: "
-                f"{ selected_node.get_text() }"
+                f"{ selected_node.get_content(metadata_mode=MetadataMode.LLM) }"
             )
             logger.debug(full_debug_str)
             if self._verbose:
                 print_text(full_debug_str, end="\n")
             result_response = self._query_with_selected_node(
                 selected_node,
                 query_bundle,
@@ -273,61 +270,60 @@
             level=0,
         ).strip()
         # TODO: fix source nodes
         return Response(response_str, source_nodes=[])
 
     def _select_nodes(
         self,
-        cur_node_list: List[Node],
+        cur_node_list: List[BaseNode],
         query_bundle: QueryBundle,
         level: int = 0,
-    ) -> List[Node]:
+    ) -> List[BaseNode]:
         query_str = query_bundle.query_str
 
         if self.child_branch_factor == 1:
             query_template = self.query_template.partial_format(
                 num_chunks=len(cur_node_list), query_str=query_str
             )
-            numbered_node_text = (
-                self._service_context.prompt_helper.get_numbered_text_from_nodes(
-                    cur_node_list, prompt=query_template
+            text_splitter = (
+                self._service_context.prompt_helper.get_text_splitter_given_prompt(
+                    prompt=query_template,
+                    num_chunks=len(cur_node_list),
                 )
             )
-            (
-                response,
-                formatted_query_prompt,
-            ) = self._service_context.llm_predictor.predict(
+            numbered_node_text = get_numbered_text_from_nodes(
+                cur_node_list, text_splitter=text_splitter
+            )
+
+            response = self._service_context.llm_predictor.predict(
                 query_template,
                 context_list=numbered_node_text,
             )
         else:
             query_template_multiple = self.query_template_multiple.partial_format(
                 num_chunks=len(cur_node_list),
                 query_str=query_str,
                 branching_factor=self.child_branch_factor,
             )
-            numbered_node_text = (
-                self._service_context.prompt_helper.get_numbered_text_from_nodes(
-                    cur_node_list, prompt=query_template_multiple
+
+            text_splitter = (
+                self._service_context.prompt_helper.get_text_splitter_given_prompt(
+                    prompt=query_template_multiple,
+                    num_chunks=len(cur_node_list),
                 )
             )
-            (
-                response,
-                formatted_query_prompt,
-            ) = self._service_context.llm_predictor.predict(
+            numbered_node_text = get_numbered_text_from_nodes(
+                cur_node_list, text_splitter=text_splitter
+            )
+
+            response = self._service_context.llm_predictor.predict(
                 query_template_multiple,
                 context_list=numbered_node_text,
             )
 
-        logger.debug(
-            f">[Level {level}] current prompt template: {formatted_query_prompt}"
-        )
-        self._service_context.llama_logger.add_log(
-            {"formatted_prompt_template": formatted_query_prompt, "level": level}
-        )
         debug_str = f">[Level {level}] Current response: {response}"
         logger.debug(debug_str)
         if self._verbose:
             print_text(debug_str, end="\n")
 
         numbers = extract_numbers_given_response(response, n=self.child_branch_factor)
         if numbers is None:
@@ -356,33 +352,35 @@
             info_str = (
                 f">[Level {level}] Selected node: "
                 f"[{number}]/[{','.join([str(int(n)) for n in numbers])}]"
             )
             logger.info(info_str)
             if self._verbose:
                 print_text(info_str, end="\n")
-            debug_str = " ".join(selected_node.get_text().splitlines())
+            debug_str = " ".join(
+                selected_node.get_content(metadata_mode=MetadataMode.LLM).splitlines()
+            )
             full_debug_str = (
                 f">[Level {level}] Node "
                 f"[{number}] Summary text: "
-                f"{ selected_node.get_text() }"
+                f"{ selected_node.get_content(metadata_mode=MetadataMode.LLM) }"
             )
             logger.debug(full_debug_str)
             if self._verbose:
                 print_text(full_debug_str, end="\n")
             selected_nodes.append(selected_node)
 
         return selected_nodes
 
     def _retrieve_level(
         self,
         cur_node_ids: Dict[int, str],
         query_bundle: QueryBundle,
         level: int = 0,
-    ) -> List[Node]:
+    ) -> List[BaseNode]:
         """Answer a query recursively."""
         cur_nodes = {
             index: self._docstore.get_node(node_id)
             for index, node_id in cur_node_ids.items()
         }
         cur_node_list = get_sorted_node_list(cur_nodes)
 
@@ -402,19 +400,18 @@
 
         if len(children_nodes) == 0:
             # NOTE: leaf level
             return selected_nodes
         else:
             return self._retrieve_level(children_nodes, query_bundle, level + 1)
 
-    @llm_token_counter("retrieve")
     def _retrieve(
         self,
         query_bundle: QueryBundle,
     ) -> List[NodeWithScore]:
         """Get nodes for response."""
         nodes = self._retrieve_level(
             self._index_struct.root_nodes,
             query_bundle,
             level=0,
         )
-        return [NodeWithScore(node) for node in nodes]
+        return [NodeWithScore(node=node) for node in nodes]
```

### Comparing `llama_index-0.6.9/llama_index/indices/tree/tree_root_retriever.py` & `llama_index-0.7.0/llama_index/indices/tree/tree_root_retriever.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,40 +1,37 @@
 """Retrieve query."""
 import logging
-from typing import Any, List
+from typing import List
 
-from llama_index.data_structs.node import NodeWithScore
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.query.schema import QueryBundle
+from llama_index.indices.tree.base import TreeIndex
 from llama_index.indices.utils import get_sorted_node_list
+from llama_index.schema import NodeWithScore
 
 logger = logging.getLogger(__name__)
 
 
 class TreeRootRetriever(BaseRetriever):
-    """GPT Tree Index retrieve query.
+    """Tree root retriever.
 
     This class directly retrieves the answer from the root nodes.
 
     Unlike GPTTreeIndexLeafQuery, this class assumes the graph already stores
     the answer (because it was constructed with a query_str), so it does not
     attempt to parse information down the graph in order to synthesize an answer.
     """
 
-    def __init__(self, index: Any):
-        from llama_index.indices.tree.base import GPTTreeIndex
-
-        assert isinstance(index, GPTTreeIndex)
-
+    def __init__(self, index: TreeIndex):
         self._index = index
         self._index_struct = index.index_struct
         self._docstore = index.docstore
 
     def _retrieve(
         self,
         query_bundle: QueryBundle,
     ) -> List[NodeWithScore]:
         """Get nodes for response."""
         logger.info(f"> Starting query: {query_bundle.query_str}")
         root_nodes = self._docstore.get_node_dict(self._index_struct.root_nodes)
         sorted_nodes = get_sorted_node_list(root_nodes)
-        return [NodeWithScore(node) for node in sorted_nodes]
+        return [NodeWithScore(node=node) for node in sorted_nodes]
```

### Comparing `llama_index-0.6.9/llama_index/indices/utils.py` & `llama_index-0.7.0/llama_index/indices/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 """Utilities for GPT indices."""
 import logging
 import re
 from typing import Dict, List, Optional, Set, Tuple
 
-from llama_index.data_structs.node import Node
+from llama_index.schema import BaseNode, MetadataMode
 from llama_index.utils import globals_helper, truncate_text
 from llama_index.vector_stores.types import VectorStoreQueryResult
 
 _logger = logging.getLogger(__name__)
 
 
-def get_sorted_node_list(node_dict: Dict[int, Node]) -> List[Node]:
+def get_sorted_node_list(node_dict: Dict[int, BaseNode]) -> List[BaseNode]:
     """Get sorted node list. Used by tree-strutured indices."""
     sorted_indices = sorted(node_dict.keys())
     return [node_dict[index] for index in sorted_indices]
 
 
 def extract_numbers_given_response(response: str, n: int = 1) -> Optional[List[int]]:
     """Extract number given the GPT-generated response.
@@ -54,32 +54,35 @@
         if result.similarities is not None and len(result.similarities) > 0
         else [1.0 for _ in result.ids]
     )
 
     fmt_txts = []
     for node_idx, node_similarity, node in zip(result.ids, similarities, result.nodes):
         fmt_txt = f"> [Node {node_idx}] [Similarity score: \
-            {float(node_similarity):.6}] {truncate_text(node.get_text(), 100)}"
+            {float(node_similarity):.6}] {truncate_text(node.get_content(), 100)}"
         fmt_txts.append(fmt_txt)
     top_k_node_text = "\n".join(fmt_txts)
     logger.debug(f"> Top {len(result.nodes)} nodes:\n{top_k_node_text}")
 
 
 def default_format_node_batch_fn(
-    summary_nodes: List[Node],
+    summary_nodes: List[BaseNode],
 ) -> str:
     """Default format node batch function.
 
     Assign each summary node a number, and format the batch of nodes.
 
     """
     fmt_node_txts = []
     for idx in range(len(summary_nodes)):
         number = idx + 1
-        fmt_node_txts.append(f"Document {number}:\n{summary_nodes[idx].get_text()}")
+        fmt_node_txts.append(
+            f"Document {number}:\n"
+            f"{summary_nodes[idx].get_content(metadata_mode=MetadataMode.LLM)}"
+        )
     return "\n\n".join(fmt_node_txts)
 
 
 def default_parse_choice_select_answer_fn(
     answer: str, num_choices: int, raise_error: bool = False
 ) -> Tuple[List[int], Optional[List[float]]]:
     """Default parse choice select answer function."""
```

### Comparing `llama_index-0.6.9/llama_index/indices/vector_store/base.py` & `llama_index-0.7.0/llama_index/indices/vector_store/base.py`

 * *Files 22% similar despite different names*

```diff
@@ -4,144 +4,178 @@
 
 """
 
 from typing import Any, Dict, List, Optional, Sequence, Tuple
 
 from llama_index.async_utils import run_async_tasks
 from llama_index.data_structs.data_structs import IndexDict
-from llama_index.data_structs.node import ImageNode, IndexNode, Node
-from llama_index.indices.base import BaseGPTIndex
+from llama_index.indices.base import BaseIndex
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.service_context import ServiceContext
+from llama_index.schema import BaseNode, ImageNode, IndexNode, MetadataMode
+from llama_index.storage.docstore.types import RefDocInfo
 from llama_index.storage.storage_context import StorageContext
-from llama_index.token_counter.token_counter import llm_token_counter
 from llama_index.vector_stores.types import NodeWithEmbedding, VectorStore
 
 
-class GPTVectorStoreIndex(BaseGPTIndex[IndexDict]):
-    """Base GPT Vector Store Index.
+class VectorStoreIndex(BaseIndex[IndexDict]):
+    """Vector Store Index.
 
     Args:
         use_async (bool): Whether to use asynchronous calls. Defaults to False.
+        show_progress (bool): Whether to show tqdm progress bars. Defaults to False.
         store_nodes_override (bool): set to True to always store Node objects in index
             store and document store even if vector store keeps text. Defaults to False
     """
 
     index_struct_cls = IndexDict
 
     def __init__(
         self,
-        nodes: Optional[Sequence[Node]] = None,
+        nodes: Optional[Sequence[BaseNode]] = None,
         index_struct: Optional[IndexDict] = None,
         service_context: Optional[ServiceContext] = None,
         storage_context: Optional[StorageContext] = None,
         use_async: bool = False,
         store_nodes_override: bool = False,
+        show_progress: bool = False,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
         self._use_async = use_async
         self._store_nodes_override = store_nodes_override
         super().__init__(
             nodes=nodes,
             index_struct=index_struct,
             service_context=service_context,
             storage_context=storage_context,
+            show_progress=show_progress,
             **kwargs,
         )
 
+    @classmethod
+    def from_vector_store(
+        cls,
+        vector_store: VectorStore,
+        service_context: Optional[ServiceContext] = None,
+        **kwargs: Any,
+    ) -> "VectorStoreIndex":
+        if not vector_store.stores_text:
+            raise ValueError(
+                "Cannot initialize from a vector store that does not store text."
+            )
+
+        storage_context = StorageContext.from_defaults(vector_store=vector_store)
+        return cls(
+            nodes=[], service_context=service_context, storage_context=storage_context
+        )
+
     @property
     def vector_store(self) -> VectorStore:
         return self._vector_store
 
     def as_retriever(self, **kwargs: Any) -> BaseRetriever:
         # NOTE: lazy import
         from llama_index.indices.vector_store.retrievers import VectorIndexRetriever
 
-        return VectorIndexRetriever(self, **kwargs)
+        return VectorIndexRetriever(
+            self,
+            node_ids=list(self.index_struct.nodes_dict.values()),
+            **kwargs,
+        )
 
     def _get_node_embedding_results(
-        self, nodes: Sequence[Node]
+        self,
+        nodes: Sequence[BaseNode],
+        show_progress: bool = False,
     ) -> List[NodeWithEmbedding]:
         """Get tuples of id, node, and embedding.
 
         Allows us to store these nodes in a vector store.
         Embeddings are called in batches.
 
         """
         id_to_embed_map: Dict[str, List[float]] = {}
 
         for n in nodes:
             if n.embedding is None:
                 self._service_context.embed_model.queue_text_for_embedding(
-                    n.get_doc_id(), n.get_text()
+                    n.node_id, n.get_content(metadata_mode=MetadataMode.EMBED)
                 )
             else:
-                id_to_embed_map[n.get_doc_id()] = n.embedding
+                id_to_embed_map[n.node_id] = n.embedding
 
         # call embedding model to get embeddings
         (
             result_ids,
             result_embeddings,
-        ) = self._service_context.embed_model.get_queued_text_embeddings()
+        ) = self._service_context.embed_model.get_queued_text_embeddings(show_progress)
         for new_id, text_embedding in zip(result_ids, result_embeddings):
             id_to_embed_map[new_id] = text_embedding
 
         results = []
         for node in nodes:
-            embedding = id_to_embed_map[node.get_doc_id()]
+            embedding = id_to_embed_map[node.node_id]
             result = NodeWithEmbedding(node=node, embedding=embedding)
             results.append(result)
         return results
 
     async def _aget_node_embedding_results(
         self,
-        nodes: Sequence[Node],
+        nodes: Sequence[BaseNode],
+        show_progress: bool = False,
     ) -> List[NodeWithEmbedding]:
         """Asynchronously get tuples of id, node, and embedding.
 
         Allows us to store these nodes in a vector store.
         Embeddings are called in batches.
 
         """
         id_to_embed_map: Dict[str, List[float]] = {}
 
         text_queue: List[Tuple[str, str]] = []
         for n in nodes:
             if n.embedding is None:
-                text_queue.append((n.get_doc_id(), n.get_text()))
+                text_queue.append(
+                    (n.node_id, n.get_content(metadata_mode=MetadataMode.EMBED))
+                )
             else:
-                id_to_embed_map[n.get_doc_id()] = n.embedding
+                id_to_embed_map[n.node_id] = n.embedding
 
         # call embedding model to get embeddings
         (
             result_ids,
             result_embeddings,
         ) = await self._service_context.embed_model.aget_queued_text_embeddings(
-            text_queue
+            text_queue, show_progress
         )
 
         for new_id, text_embedding in zip(result_ids, result_embeddings):
             id_to_embed_map[new_id] = text_embedding
 
         results = []
         for node in nodes:
-            embedding = id_to_embed_map[node.get_doc_id()]
+            embedding = id_to_embed_map[node.node_id]
             result = NodeWithEmbedding(node=node, embedding=embedding)
             results.append(result)
         return results
 
     async def _async_add_nodes_to_index(
-        self, index_struct: IndexDict, nodes: Sequence[Node]
+        self,
+        index_struct: IndexDict,
+        nodes: Sequence[BaseNode],
+        show_progress: bool = False,
     ) -> None:
         """Asynchronously add nodes to index."""
         if not nodes:
             return
 
-        embedding_results = await self._aget_node_embedding_results(nodes)
+        embedding_results = await self._aget_node_embedding_results(
+            nodes, show_progress
+        )
         new_ids = self._vector_store.add(embedding_results)
 
         # if the vector store doesn't store text, we need to add the nodes to the
         # index struct and document store
         if not self._vector_store.stores_text or self._store_nodes_override:
             for result, new_id in zip(embedding_results, new_ids):
                 index_struct.add_node(result.node, text_id=new_id)
@@ -153,21 +187,22 @@
                 if isinstance(result.node, (ImageNode, IndexNode)):
                     index_struct.add_node(result.node, text_id=new_id)
                     self._docstore.add_documents([result.node], allow_update=True)
 
     def _add_nodes_to_index(
         self,
         index_struct: IndexDict,
-        nodes: Sequence[Node],
+        nodes: Sequence[BaseNode],
+        show_progress: bool = False,
     ) -> None:
         """Add document to index."""
         if not nodes:
             return
 
-        embedding_results = self._get_node_embedding_results(nodes)
+        embedding_results = self._get_node_embedding_results(nodes, show_progress)
         new_ids = self._vector_store.add(embedding_results)
 
         if not self._vector_store.stores_text or self._store_nodes_override:
             # NOTE: if the vector store doesn't store text,
             # we need to add the nodes to the index struct and document store
             for result, new_id in zip(embedding_results, new_ids):
                 index_struct.add_node(result.node, text_id=new_id)
@@ -176,46 +211,114 @@
             # NOTE: if the vector store keeps text,
             # we only need to add image and index nodes
             for result, new_id in zip(embedding_results, new_ids):
                 if isinstance(result.node, (ImageNode, IndexNode)):
                     index_struct.add_node(result.node, text_id=new_id)
                     self._docstore.add_documents([result.node], allow_update=True)
 
-    def _build_index_from_nodes(self, nodes: Sequence[Node]) -> IndexDict:
+    def _build_index_from_nodes(self, nodes: Sequence[BaseNode]) -> IndexDict:
         """Build index from nodes."""
         index_struct = self.index_struct_cls()
         if self._use_async:
-            tasks = [self._async_add_nodes_to_index(index_struct, nodes)]
+            tasks = [
+                self._async_add_nodes_to_index(
+                    index_struct, nodes, show_progress=self._show_progress
+                )
+            ]
             run_async_tasks(tasks)
         else:
-            self._add_nodes_to_index(index_struct, nodes)
+            self._add_nodes_to_index(
+                index_struct, nodes, show_progress=self._show_progress
+            )
         return index_struct
 
-    @llm_token_counter("build_index_from_nodes")
-    def build_index_from_nodes(self, nodes: Sequence[Node]) -> IndexDict:
+    def build_index_from_nodes(self, nodes: Sequence[BaseNode]) -> IndexDict:
         """Build the index from nodes.
 
-        NOTE: Overrides BaseGPTIndex.build_index_from_nodes.
-            GPTVectorStoreIndex only stores nodes in document store
+        NOTE: Overrides BaseIndex.build_index_from_nodes.
+            VectorStoreIndex only stores nodes in document store
             if vector store does not store text
         """
         return self._build_index_from_nodes(nodes)
 
-    def _insert(self, nodes: Sequence[Node], **insert_kwargs: Any) -> None:
+    def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
         """Insert a document."""
         self._add_nodes_to_index(self._index_struct, nodes)
 
-    @llm_token_counter("insert")
-    def insert_nodes(self, nodes: Sequence[Node], **insert_kwargs: Any) -> None:
+    def insert_nodes(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
         """Insert nodes.
 
-        NOTE: overrides BaseGPTIndex.insert_nodes.
-            GPTVectorStoreIndex only stores nodes in document store
+        NOTE: overrides BaseIndex.insert_nodes.
+            VectorStoreIndex only stores nodes in document store
             if vector store does not store text
         """
         self._insert(nodes, **insert_kwargs)
         self._storage_context.index_store.add_index_struct(self._index_struct)
 
-    def _delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document."""
-        self._index_struct.delete(doc_id)
-        self._vector_store.delete(doc_id)
+    def _delete_node(self, node_id: str, **delete_kwargs: Any) -> None:
+        pass
+
+    def delete_nodes(
+        self,
+        node_ids: List[str],
+        delete_from_docstore: bool = False,
+        **delete_kwargs: Any,
+    ) -> None:
+        """Delete a list of nodes from the index.
+
+        Args:
+            doc_ids (List[str]): A list of doc_ids from the nodes to delete
+
+        """
+        raise NotImplementedError(
+            "Vector indices currently only support delete_ref_doc, which "
+            "deletes nodes using the ref_doc_id of ingested documents."
+        )
+
+    def delete_ref_doc(
+        self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any
+    ) -> None:
+        """Delete a document and it's nodes by using ref_doc_id."""
+        self._vector_store.delete(ref_doc_id)
+
+        # delete from index_struct only if needed
+        if not self._vector_store.stores_text or self._store_nodes_override:
+            ref_doc_info = self._docstore.get_ref_doc_info(ref_doc_id)
+            if ref_doc_info is not None:
+                for node_id in ref_doc_info.node_ids:
+                    self._index_struct.delete(node_id)
+
+        # delete from docstore only if needed
+        if (
+            not self._vector_store.stores_text or self._store_nodes_override
+        ) and delete_from_docstore:
+            self._docstore.delete_ref_doc(ref_doc_id, raise_error=False)
+
+        self._storage_context.index_store.add_index_struct(self._index_struct)
+
+    @property
+    def ref_doc_info(self) -> Dict[str, RefDocInfo]:
+        """Retrieve a dict mapping of ingested documents and their nodes+metadata."""
+        if not self._vector_store.stores_text or self._store_nodes_override:
+            node_doc_ids = list(self.index_struct.nodes_dict.values())
+            nodes = self.docstore.get_nodes(node_doc_ids)
+
+            all_ref_doc_info = {}
+            for node in nodes:
+                ref_node = node.source_node
+                if not ref_node:
+                    continue
+
+                ref_doc_info = self.docstore.get_ref_doc_info(ref_node.node_id)
+                if not ref_doc_info:
+                    continue
+
+                all_ref_doc_info[ref_node.node_id] = ref_doc_info
+            return all_ref_doc_info
+        else:
+            raise NotImplementedError(
+                "Vector store integrations that store text in the vector store are "
+                "not supported by ref_doc_info yet."
+            )
+
+
+GPTVectorStoreIndex = VectorStoreIndex
```

### Comparing `llama_index-0.6.9/llama_index/indices/vector_store/retrievers/auto_retriever/auto_retriever.py` & `llama_index-0.7.0/llama_index/indices/vector_store/retrievers/auto_retriever/auto_retriever.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,25 @@
 import logging
 from typing import List, Optional, cast
 
 from llama_index.constants import DEFAULT_SIMILARITY_TOP_K
-from llama_index.data_structs.node import NodeWithScore
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
+from llama_index.indices.vector_store.base import VectorStoreIndex
 from llama_index.indices.vector_store.retrievers import VectorIndexRetriever
 from llama_index.indices.vector_store.retrievers.auto_retriever.output_parser import (
     VectorStoreQueryOutputParser,
 )
 from llama_index.indices.vector_store.retrievers.auto_retriever.prompts import (
     DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL,
     VectorStoreQueryPrompt,
 )
 from llama_index.output_parsers.base import OutputParserException, StructuredOutput
+from llama_index.schema import NodeWithScore
 from llama_index.vector_stores.types import (
     MetadataFilters,
     VectorStoreInfo,
     VectorStoreQuerySpec,
 )
 
 _logger = logging.getLogger(__name__)
@@ -28,30 +28,30 @@
 class VectorIndexAutoRetriever(BaseRetriever):
     """Vector store auto retriever.
 
     A retriever for vector store index that uses an LLM to automatically set
     vector store query parameters.
 
     Args:
-        index (GPTVectorStoreIndex): vector store index
+        index (VectorStoreIndex): vector store index
         vector_store_info (VectorStoreInfo): additional information information about
             vector store content and supported metadata filters. The natural language
             description is used by an LLM to automatically set vector store query
             parameters.
         prompt_template_str: custom prompt template string for LLM.
             Uses default template string if None.
         service_context: service context containing reference to LLMPredictor.
             Uses service context from index be default if None.
         max_top_k: the maximum top_k allowed. The top_k set by LLM will be clamped
             to this value.
     """
 
     def __init__(
         self,
-        index: GPTVectorStoreIndex,
+        index: VectorStoreIndex,
         vector_store_info: VectorStoreInfo,
         prompt_template_str: Optional[str] = None,
         service_context: Optional[ServiceContext] = None,
         max_top_k: int = 10,
     ) -> None:
         self._index = index
         self._vector_store_info = vector_store_info
@@ -72,15 +72,15 @@
 
     def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
         # prepare input
         info_str = self._vector_store_info.json(indent=4)
         schema_str = VectorStoreQuerySpec.schema_json(indent=4)
 
         # call LLM
-        output, _ = self._service_context.llm_predictor.predict(
+        output = self._service_context.llm_predictor.predict(
             self._prompt,
             schema_str=schema_str,
             info_str=info_str,
             query_str=query_bundle.query_str,
         )
 
         # parse output
```

### Comparing `llama_index-0.6.9/llama_index/indices/vector_store/retrievers/auto_retriever/output_parser.py` & `llama_index-0.7.0/llama_index/indices/vector_store/retrievers/auto_retriever/output_parser.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from typing import Any
 
-from llama_index.output_parsers.base import BaseOutputParser, StructuredOutput
+from llama_index.output_parsers.base import StructuredOutput
+from llama_index.types import BaseOutputParser
 from llama_index.output_parsers.utils import parse_json_markdown
 from llama_index.vector_stores.types import VectorStoreQuerySpec
 
 
 class VectorStoreQueryOutputParser(BaseOutputParser):
     def parse(self, output: str) -> Any:
         json_dict = parse_json_markdown(output)
```

### Comparing `llama_index-0.6.9/llama_index/indices/vector_store/retrievers/auto_retriever/prompts.py` & `llama_index-0.7.0/llama_index/indices/vector_store/retrievers/auto_retriever/prompts.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,9 @@
 """Autoretriever prompts."""
 
-from typing import List
 
 from llama_index.prompts.base import Prompt
 from llama_index.prompts.prompt_type import PromptType
 from llama_index.vector_stores.types import (
     ExactMatchFilter,
     MetadataInfo,
     VectorStoreInfo,
@@ -98,17 +97,15 @@
 
 Structured Request:
 """
 
 DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL = PREFIX + EXAMPLES + SUFFIX
 
 
-class VectorStoreQueryPrompt(Prompt):
-    """Vector store query prompt."""
+# deprecated, kept for backwards compatibility
+"""Vector store query prompt."""
+VectorStoreQueryPrompt = Prompt
 
-    prompt_type: PromptType = PromptType.VECTOR_STORE_QUERY
-    input_variables: List[str] = ["schema_str", "info_str", "query_str"]
-
-
-DEFAULT_VECTOR_STORE_QUERY_PROMPT = VectorStoreQueryPrompt(
+DEFAULT_VECTOR_STORE_QUERY_PROMPT = Prompt(
     template=DEFAULT_VECTOR_STORE_QUERY_PROMPT_TMPL,
+    prompt_type=PromptType.VECTOR_STORE_QUERY,
 )
```

### Comparing `llama_index-0.6.9/llama_index/indices/vector_store/retrievers/retriever.py` & `llama_index-0.7.0/llama_index/indices/vector_store/retrievers/retriever.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,69 +1,69 @@
 """Base vector store index query."""
 
 
 from typing import Any, Dict, List, Optional
 
 from llama_index.constants import DEFAULT_SIMILARITY_TOP_K
 from llama_index.data_structs.data_structs import IndexDict
-from llama_index.data_structs.node import NodeWithScore
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.utils import log_vector_store_query_result
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
-from llama_index.token_counter.token_counter import llm_token_counter
+from llama_index.indices.vector_store.base import VectorStoreIndex
+from llama_index.schema import NodeWithScore, ObjectType
 from llama_index.vector_stores.types import (
     MetadataFilters,
     VectorStoreQuery,
     VectorStoreQueryMode,
 )
 
 
 class VectorIndexRetriever(BaseRetriever):
     """Vector index retriever.
 
     Args:
-        index (GPTVectorStoreIndex): vector store index.
+        index (VectorStoreIndex): vector store index.
         similarity_top_k (int): number of top k results to return.
         vector_store_query_mode (str): vector store query mode
             See reference for VectorStoreQueryMode for full list of supported modes.
         filters (Optional[MetadataFilters]): metadata filters, defaults to None
         alpha (float): weight for sparse/dense retrieval, only used for
             hybrid query mode.
         doc_ids (Optional[List[str]]): list of documents to constrain search.
         vector_store_kwargs (dict): Additional vector store specific kwargs to pass
             through to the vector store at query time.
 
     """
 
     def __init__(
         self,
-        index: GPTVectorStoreIndex,
+        index: VectorStoreIndex,
         similarity_top_k: int = DEFAULT_SIMILARITY_TOP_K,
         vector_store_query_mode: VectorStoreQueryMode = VectorStoreQueryMode.DEFAULT,
         filters: Optional[MetadataFilters] = None,
         alpha: Optional[float] = None,
+        node_ids: Optional[List[str]] = None,
         doc_ids: Optional[List[str]] = None,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
         self._index = index
         self._vector_store = self._index.vector_store
         self._service_context = self._index.service_context
         self._docstore = self._index.docstore
 
         self._similarity_top_k = similarity_top_k
         self._vector_store_query_mode = VectorStoreQueryMode(vector_store_query_mode)
         self._alpha = alpha
+        self._node_ids = node_ids
         self._doc_ids = doc_ids
         self._filters = filters
 
         self._kwargs: Dict[str, Any] = kwargs.get("vector_store_kwargs", {})
 
-    @llm_token_counter("retrieve")
     def _retrieve(
         self,
         query_bundle: QueryBundle,
     ) -> List[NodeWithScore]:
         if self._vector_store.is_embedding_query:
             if query_bundle.embedding is None:
                 query_bundle.embedding = (
@@ -71,14 +71,15 @@
                         query_bundle.embedding_strs
                     )
                 )
 
         query = VectorStoreQuery(
             query_embedding=query_bundle.embedding,
             similarity_top_k=self._similarity_top_k,
+            node_ids=self._node_ids,
             doc_ids=self._doc_ids,
             query_str=query_bundle.query_str,
             mode=self._vector_store_query_mode,
             alpha=self._alpha,
             filters=self._filters,
         )
         query_result = self._vector_store.query(query, **self._kwargs)
@@ -97,21 +98,29 @@
             ]
             nodes = self._docstore.get_nodes(node_ids)
             query_result.nodes = nodes
         else:
             # NOTE: vector store keeps text, returns nodes.
             # Only need to recover image or index nodes from docstore
             for i in range(len(query_result.nodes)):
-                node_id = query_result.nodes[i].get_doc_id()
-                if node_id in self._docstore.docs:
-                    query_result.nodes[i] = self._docstore.get_node(node_id)
+                source_node = query_result.nodes[i].source_node
+                if (not self._vector_store.stores_text) or (
+                    source_node is not None and source_node.node_type != ObjectType.TEXT
+                ):
+                    node_id = query_result.nodes[i].node_id
+                    if node_id in self._docstore.docs:
+                        query_result.nodes[
+                            i
+                        ] = self._docstore.get_node(  # type: ignore[index]
+                            node_id
+                        )
 
         log_vector_store_query_result(query_result)
 
         node_with_scores: List[NodeWithScore] = []
         for ind, node in enumerate(query_result.nodes):
             score: Optional[float] = None
             if query_result.similarities is not None:
                 score = query_result.similarities[ind]
-            node_with_scores.append(NodeWithScore(node, score=score))
+            node_with_scores.append(NodeWithScore(node=node, score=score))
 
         return node_with_scores
```

### Comparing `llama_index-0.6.9/llama_index/langchain_helpers/agents/__init__.py` & `llama_index-0.7.0/llama_index/langchain_helpers/agents/__init__.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/langchain_helpers/agents/agents.py` & `llama_index-0.7.0/llama_index/langchain_helpers/agents/agents.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,18 @@
 """Create LlamaIndex agents."""
 
 from typing import Any, Optional
 
-from langchain.agents import AgentExecutor, initialize_agent
-from langchain.callbacks.base import BaseCallbackManager
-from langchain.llms.base import BaseLLM
-from langchain.agents.agent_types import AgentType
+from llama_index.bridge.langchain import (
+    BaseLLM,
+    AgentType,
+    AgentExecutor,
+    initialize_agent,
+    BaseCallbackManager,
+)
 
 from llama_index.langchain_helpers.agents.toolkits import LlamaToolkit
 
 
 def create_llama_agent(
     toolkit: LlamaToolkit,
     llm: BaseLLM,
```

### Comparing `llama_index-0.6.9/llama_index/langchain_helpers/agents/toolkits.py` & `llama_index-0.7.0/llama_index/langchain_helpers/agents/toolkits.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 """LlamaIndex toolkit."""
 
 from typing import List
 
-from langchain.agents.agent_toolkits.base import BaseToolkit
-from langchain.tools import BaseTool
+from llama_index.bridge.langchain import BaseTool, BaseToolkit
 from pydantic import Field
 
 from llama_index.langchain_helpers.agents.tools import (
     IndexToolConfig,
     LlamaIndexTool,
 )
```

### Comparing `llama_index-0.6.9/llama_index/langchain_helpers/agents/tools.py` & `llama_index-0.7.0/llama_index/langchain_helpers/agents/tools.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,30 @@
 """LlamaIndex Tool classes."""
 
-from typing import Dict
+from typing import Any, Dict, List
 
-from langchain.tools import BaseTool
+from llama_index.bridge.langchain import BaseTool
 from pydantic import BaseModel, Field
 
 from llama_index.indices.query.base import BaseQueryEngine
 from llama_index.response.schema import RESPONSE_TYPE
+from llama_index.schema import TextNode
 
 
 def _get_response_with_sources(response: RESPONSE_TYPE) -> str:
     """Return a response with source node info."""
-    source_data = []
+    source_data: List[Dict[str, Any]] = []
     for source_node in response.source_nodes:
-        metadata = (
-            source_node.node.node_info if source_node.node.node_info is not None else {}
-        )
+        metadata = {}
+        if isinstance(source_node.node, TextNode):
+            start = source_node.node.start_char_idx
+            end = source_node.node.end_char_idx
+            if start is not None and end is not None:
+                metadata.update({"start_char_idx": start, "end_char_idx": end})
+
         source_data.append(metadata)
         source_data[-1]["ref_doc_id"] = source_node.node.ref_doc_id
         source_data[-1]["score"] = source_node.score
     return str({"answer": str(response), "sources": source_data})
 
 
 class IndexToolConfig(BaseModel):
```

### Comparing `llama_index-0.6.9/llama_index/langchain_helpers/memory_wrapper.py` & `llama_index-0.7.0/llama_index/langchain_helpers/memory_wrapper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,19 +1,22 @@
 """Langchain memory wrapper (for LlamaIndex)."""
 
 from typing import Any, Dict, List, Optional
 
-from langchain.memory.chat_memory import BaseChatMemory
-from langchain.schema import AIMessage
-from langchain.schema import BaseMemory as Memory
-from langchain.schema import BaseMessage, HumanMessage
+from llama_index.bridge.langchain import (
+    BaseChatMemory,
+    AIMessage,
+    BaseMemory as Memory,
+    BaseMessage,
+    HumanMessage,
+)
 from pydantic import Field
 
-from llama_index.indices.base import BaseGPTIndex
-from llama_index.readers.schema.base import Document
+from llama_index.indices.base import BaseIndex
+from llama_index.schema import Document
 from llama_index.utils import get_new_id
 
 
 def get_prompt_input_key(inputs: Dict[str, Any], memory_variables: List[str]) -> str:
     """Get prompt input key.
 
     Copied over from langchain.
@@ -30,25 +33,25 @@
 class GPTIndexMemory(Memory):
     """Langchain memory wrapper (for LlamaIndex).
 
     Args:
         human_prefix (str): Prefix for human input. Defaults to "Human".
         ai_prefix (str): Prefix for AI output. Defaults to "AI".
         memory_key (str): Key for memory. Defaults to "history".
-        index (BaseGPTIndex): LlamaIndex instance.
+        index (BaseIndex): LlamaIndex instance.
         query_kwargs (Dict[str, Any]): Keyword arguments for LlamaIndex query.
         input_key (Optional[str]): Input key. Defaults to None.
         output_key (Optional[str]): Output key. Defaults to None.
 
     """
 
     human_prefix: str = "Human"
     ai_prefix: str = "AI"
     memory_key: str = "history"
-    index: BaseGPTIndex
+    index: BaseIndex
     query_kwargs: Dict = Field(default_factory=dict)
     output_key: Optional[str] = None
     input_key: Optional[str] = None
 
     @property
     def memory_variables(self) -> List[str]:
         """Return memory variables."""
@@ -100,25 +103,25 @@
 class GPTIndexChatMemory(BaseChatMemory):
     """Langchain chat memory wrapper (for LlamaIndex).
 
     Args:
         human_prefix (str): Prefix for human input. Defaults to "Human".
         ai_prefix (str): Prefix for AI output. Defaults to "AI".
         memory_key (str): Key for memory. Defaults to "history".
-        index (BaseGPTIndex): LlamaIndex instance.
+        index (BaseIndex): LlamaIndex instance.
         query_kwargs (Dict[str, Any]): Keyword arguments for LlamaIndex query.
         input_key (Optional[str]): Input key. Defaults to None.
         output_key (Optional[str]): Output key. Defaults to None.
 
     """
 
     human_prefix: str = "Human"
     ai_prefix: str = "AI"
     memory_key: str = "history"
-    index: BaseGPTIndex
+    index: BaseIndex
     query_kwargs: Dict = Field(default_factory=dict)
     output_key: Optional[str] = None
     input_key: Optional[str] = None
 
     return_source: bool = False
     id_to_message: Dict[str, BaseMessage] = Field(default_factory=dict)
 
@@ -141,22 +144,22 @@
 
         query_engine = self.index.as_query_engine(**self.query_kwargs)
         response_obj = query_engine.query(query_str)
         if self.return_source:
             source_nodes = response_obj.source_nodes
             if self.return_messages:
                 # get source messages from ids
-                source_ids = [sn.doc_id for sn in source_nodes]
+                source_ids = [sn.node.node_id for sn in source_nodes]
                 source_messages = [
                     m for id, m in self.id_to_message.items() if id in source_ids
                 ]
                 # NOTE: type List[BaseMessage]
                 response: Any = source_messages
             else:
-                source_texts = [sn.source_text for sn in source_nodes]
+                source_texts = [sn.node.get_content() for sn in source_nodes]
                 response = "\n\n".join(source_texts)
         else:
             response = str(response_obj)
         return {self.memory_key: response}
 
     def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -> None:
         """Save the context of this model run to memory."""
@@ -181,16 +184,16 @@
         self.chat_memory.messages.append(ai_message)
 
         self.id_to_message[human_message_id] = human_message
         self.id_to_message[ai_message_id] = ai_message
 
         human_txt = f"{self.human_prefix}: " + inputs[prompt_input_key]
         ai_txt = f"{self.ai_prefix}: " + outputs[output_key]
-        human_doc = Document(text=human_txt, doc_id=human_message_id)
-        ai_doc = Document(text=ai_txt, doc_id=ai_message_id)
+        human_doc = Document(text=human_txt, id_=human_message_id)
+        ai_doc = Document(text=ai_txt, id_=ai_message_id)
         self.index.insert(human_doc)
         self.index.insert(ai_doc)
 
     def clear(self) -> None:
         """Clear memory contents."""
         pass
```

### Comparing `llama_index-0.6.9/llama_index/langchain_helpers/sql_wrapper.py` & `llama_index-0.7.0/llama_index/langchain_helpers/sql_wrapper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 """SQL wrapper around SQLDatabase in langchain."""
 from typing import Any, Dict, List, Tuple, Optional
 
-from langchain.sql_database import SQLDatabase as LangchainSQLDatabase
+from llama_index.bridge.langchain import SQLDatabase as LangchainSQLDatabase
 from sqlalchemy import MetaData, create_engine, insert, text
 from sqlalchemy.engine import Engine
 
 
 class SQLDatabase(LangchainSQLDatabase):
     """SQL Database.
```

### Comparing `llama_index-0.6.9/llama_index/langchain_helpers/streaming.py` & `llama_index-0.7.0/llama_index/langchain_helpers/streaming.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from queue import Queue
 from threading import Event
 from typing import Any, Generator, Union
 
-from langchain.callbacks.base import BaseCallbackHandler
-from langchain.schema import LLMResult
+from llama_index.bridge.langchain import BaseCallbackHandler, LLMResult
 
 
 class StreamingGeneratorCallbackHandler(BaseCallbackHandler):
     """Streaming callback handler."""
 
     def __init__(self) -> None:
         self._token_queue: Queue = Queue()
```

### Comparing `llama_index-0.6.9/llama_index/langchain_helpers/text_splitter.py` & `llama_index-0.7.0/llama_index/langchain_helpers/text_splitter.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 """Text splitter implementations."""
 from dataclasses import dataclass
 from typing import Callable, List, Optional
 
-from langchain.text_splitter import TextSplitter
 from llama_index.constants import DEFAULT_CHUNK_OVERLAP, DEFAULT_CHUNK_SIZE
+from llama_index.bridge.langchain import TextSplitter
 
 from llama_index.callbacks.base import CallbackManager
-from llama_index.callbacks.schema import CBEventType
+from llama_index.callbacks.schema import CBEventType, EventPayload
 from llama_index.utils import globals_helper
 
 
 @dataclass
 class TextSplit:
     """Text split with overlap.
 
@@ -122,46 +122,48 @@
         new_docs = []
         for doc in docs:
             if doc.text_chunk.replace(" ", "") == "":
                 continue
             new_docs.append(doc)
         return new_docs
 
-    def split_text(self, text: str, extra_info_str: Optional[str] = None) -> List[str]:
+    def split_text(self, text: str, metadata_str: Optional[str] = None) -> List[str]:
         """Split incoming text and return chunks."""
         event_id = self.callback_manager.on_event_start(
-            CBEventType.CHUNKING, payload={"text": text}
+            CBEventType.CHUNKING, payload={EventPayload.CHUNKS: text}
         )
-        text_splits = self.split_text_with_overlaps(text, extra_info_str=extra_info_str)
+        text_splits = self.split_text_with_overlaps(text, metadata_str=metadata_str)
         chunks = [text_split.text_chunk for text_split in text_splits]
         self.callback_manager.on_event_end(
-            CBEventType.CHUNKING, payload={"chunks": chunks}, event_id=event_id
+            CBEventType.CHUNKING,
+            payload={EventPayload.CHUNKS: chunks},
+            event_id=event_id,
         )
         return chunks
 
     def split_text_with_overlaps(
-        self, text: str, extra_info_str: Optional[str] = None
+        self, text: str, metadata_str: Optional[str] = None
     ) -> List[TextSplit]:
         """Split incoming text and return chunks with overlap size."""
         if text == "":
             return []
         event_id = self.callback_manager.on_event_start(
-            CBEventType.CHUNKING, payload={"text": text}
+            CBEventType.CHUNKING, payload={EventPayload.CHUNKS: text}
         )
 
-        # NOTE: Consider extra info str that will be added to the chunk at query time
+        # NOTE: Consider metadata info str that will be added to the chunk at query time
         #       This reduces the effective chunk size that we can have
-        if extra_info_str is not None:
+        if metadata_str is not None:
             # NOTE: extra 2 newline chars for formatting when prepending in query
-            num_extra_tokens = len(self.tokenizer(f"{extra_info_str}\n\n")) + 1
+            num_extra_tokens = len(self.tokenizer(f"{metadata_str}\n\n")) + 1
             effective_chunk_size = self._chunk_size - num_extra_tokens
 
             if effective_chunk_size <= 0:
                 raise ValueError(
-                    "Effective chunk size is non positive after considering extra_info"
+                    "Effective chunk size is non positive after considering metadata"
                 )
         else:
             effective_chunk_size = self._chunk_size
 
         # First we naively split the large input into a bunch of smaller ones.
         splits = text.split(self._separator)
         splits = self._preprocess_splits(splits, effective_chunk_size)
@@ -233,15 +235,15 @@
             )
         docs.append(TextSplit(self._separator.join(splits[start_idx:cur_idx]), overlap))
 
         # run postprocessing to remove blank spaces
         docs = self._postprocess_splits(docs)
         self.callback_manager.on_event_end(
             CBEventType.CHUNKING,
-            payload={"chunks": [x.text_chunk for x in docs]},
+            payload={EventPayload.CHUNKS: [x.text_chunk for x in docs]},
             event_id=event_id,
         )
         return docs
 
     def truncate_text(self, text: str) -> str:
         """Truncate text in order to fit the underlying chunk size."""
         if text == "":
@@ -332,37 +334,37 @@
         for doc in docs:
             if doc.text_chunk.replace(" ", "") == "":
                 continue
             new_docs.append(doc)
         return new_docs
 
     def split_text_with_overlaps(
-        self, text: str, extra_info_str: Optional[str] = None
+        self, text: str, metadata_str: Optional[str] = None
     ) -> List[TextSplit]:
         """
         Split incoming text and return chunks with overlap size.
 
         Has a preference for complete sentences, phrases, and minimal overlap.
         """
         if text == "":
             return []
         event_id = self.callback_manager.on_event_start(
-            CBEventType.CHUNKING, payload={"text": text}
+            CBEventType.CHUNKING, payload={EventPayload.CHUNKS: text}
         )
 
-        # NOTE: Consider extra info str that will be added to the chunk at query time
+        # NOTE: Consider metadata info str that will be added to the chunk at query time
         #       This reduces the effective chunk size that we can have
-        if extra_info_str is not None:
+        if metadata_str is not None:
             # NOTE: extra 2 newline chars for formatting when prepending in query
-            num_extra_tokens = len(self.tokenizer(f"{extra_info_str}\n\n")) + 1
+            num_extra_tokens = len(self.tokenizer(f"{metadata_str}\n\n")) + 1
             effective_chunk_size = self._chunk_size - num_extra_tokens
 
             if effective_chunk_size <= 0:
                 raise ValueError(
-                    "Effective chunk size is non positive after considering extra_info"
+                    "Effective chunk size is non positive after considering metadata"
                 )
         else:
             effective_chunk_size = self._chunk_size
 
         # First we split paragraphs using separator
         splits = text.split(self.paragraph_separator)
 
@@ -439,26 +441,28 @@
         docs.append(TextSplit("".join(cur_doc_list).strip()))
 
         # run postprocessing to remove blank spaces
         docs = self._postprocess_splits(docs)
 
         self.callback_manager.on_event_end(
             CBEventType.CHUNKING,
-            payload={"chunks": [x.text_chunk for x in docs]},
+            payload={EventPayload.CHUNKS: [x.text_chunk for x in docs]},
             event_id=event_id,
         )
         return docs
 
-    def split_text(self, text: str, extra_info_str: Optional[str] = None) -> List[str]:
+    def split_text(self, text: str, metadata_str: Optional[str] = None) -> List[str]:
         """Split incoming text and return chunks."""
         event_id = self.callback_manager.on_event_start(
-            CBEventType.CHUNKING, payload={"text": text}
+            CBEventType.CHUNKING, payload={EventPayload.CHUNKS: text}
         )
-        text_splits = self.split_text_with_overlaps(text, extra_info_str=extra_info_str)
+        text_splits = self.split_text_with_overlaps(text, metadata_str=metadata_str)
         chunks = [text_split.text_chunk for text_split in text_splits]
         self.callback_manager.on_event_end(
-            CBEventType.CHUNKING, payload={"chunks": chunks}, event_id=event_id
+            CBEventType.CHUNKING,
+            payload={EventPayload.CHUNKS: chunks},
+            event_id=event_id,
         )
         return chunks
 
 
 __all__ = ["TextSplitter", "TokenTextSplitter", "SentenceSplitter"]
```

### Comparing `llama_index-0.6.9/llama_index/llm_predictor/structured.py` & `llama_index-0.7.0/llama_index/llm_predictor/structured.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,48 +1,49 @@
 """Structured LLM Predictor."""
 
 
 import logging
-from typing import Any, Generator, Tuple
+from typing import Any
 
 from llama_index.llm_predictor.base import LLMPredictor
 from llama_index.prompts.base import Prompt
+from llama_index.types import TokenGen
 
 logger = logging.getLogger(__name__)
 
 
 class StructuredLLMPredictor(LLMPredictor):
     """Structured LLM predictor class.
 
     Args:
         llm_predictor (BaseLLMPredictor): LLM Predictor to use.
 
     """
 
-    def predict(self, prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:
+    def predict(self, prompt: Prompt, **prompt_args: Any) -> str:
         """Predict the answer to a query.
 
         Args:
             prompt (Prompt): Prompt to use for prediction.
 
         Returns:
             Tuple[str, str]: Tuple of the predicted answer and the formatted prompt.
 
         """
-        llm_prediction, formatted_prompt = super().predict(prompt, **prompt_args)
+        llm_prediction = super().predict(prompt, **prompt_args)
         # run output parser
         if prompt.output_parser is not None:
             # TODO: return other formats
             parsed_llm_prediction = str(prompt.output_parser.parse(llm_prediction))
         else:
             parsed_llm_prediction = llm_prediction
 
-        return parsed_llm_prediction, formatted_prompt
+        return parsed_llm_prediction
 
-    def stream(self, prompt: Prompt, **prompt_args: Any) -> Tuple[Generator, str]:
+    def stream(self, prompt: Prompt, **prompt_args: Any) -> TokenGen:
         """Stream the answer to a query.
 
         NOTE: this is a beta feature. Will try to build or use
         better abstractions about response handling.
 
         Args:
             prompt (Prompt): Prompt to use for prediction.
@@ -51,23 +52,23 @@
             str: The predicted answer.
 
         """
         raise NotImplementedError(
             "Streaming is not supported for structured LLM predictor."
         )
 
-    async def apredict(self, prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:
+    async def apredict(self, prompt: Prompt, **prompt_args: Any) -> str:
         """Async predict the answer to a query.
 
         Args:
             prompt (Prompt): Prompt to use for prediction.
 
         Returns:
             Tuple[str, str]: Tuple of the predicted answer and the formatted prompt.
 
         """
-        llm_prediction, formatted_prompt = await super().apredict(prompt, **prompt_args)
+        llm_prediction = await super().apredict(prompt, **prompt_args)
         if prompt.output_parser is not None:
             parsed_llm_prediction = str(prompt.output_parser.parse(llm_prediction))
         else:
             parsed_llm_prediction = llm_prediction
-        return parsed_llm_prediction, formatted_prompt
+        return parsed_llm_prediction
```

### Comparing `llama_index-0.6.9/llama_index/logger/base.py` & `llama_index-0.7.0/llama_index/logger/base.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/node_parser/node_utils.py` & `llama_index-0.7.0/llama_index/node_parser/node_utils.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,101 +1,116 @@
 """General node utils."""
 
 
 import logging
 from typing import List
 
-from llama_index.data_structs.node import DocumentRelationship, ImageNode, Node
 from llama_index.langchain_helpers.text_splitter import (
     TextSplit,
     TextSplitter,
     TokenTextSplitter,
 )
-from llama_index.readers.schema.base import ImageDocument
-from llama_index.schema import BaseDocument
+from llama_index.schema import Document, ImageDocument
+from llama_index.schema import (
+    BaseNode,
+    NodeRelationship,
+    ImageNode,
+    TextNode,
+    MetadataMode,
+)
 from llama_index.utils import truncate_text
 
 logger = logging.getLogger(__name__)
 
 
 def get_text_splits_from_document(
-    document: BaseDocument,
+    document: BaseNode,
     text_splitter: TextSplitter,
-    include_extra_info: bool = True,
+    include_metadata: bool = True,
 ) -> List[TextSplit]:
     """Break the document into chunks with additional info."""
     # TODO: clean up since this only exists due to the diff w LangChain's TextSplitter
     if isinstance(text_splitter, TokenTextSplitter):
         # use this to extract extra information about the chunks
         text_splits = text_splitter.split_text_with_overlaps(
-            document.get_text(),
-            extra_info_str=document.extra_info_str if include_extra_info else None,
+            document.get_content(metadata_mode=MetadataMode.NONE),
+            metadata_str=document.get_metadata_str() if include_metadata else None,
         )
     else:
         text_chunks = text_splitter.split_text(
-            document.get_text(),
+            document.get_content(),
         )
         text_splits = [TextSplit(text_chunk=text_chunk) for text_chunk in text_chunks]
 
     return text_splits
 
 
 def get_nodes_from_document(
-    document: BaseDocument,
+    document: BaseNode,
     text_splitter: TextSplitter,
-    include_extra_info: bool = True,
+    include_metadata: bool = True,
     include_prev_next_rel: bool = False,
-) -> List[Node]:
+) -> List[TextNode]:
     """Get nodes from document."""
     text_splits = get_text_splits_from_document(
         document=document,
         text_splitter=text_splitter,
-        include_extra_info=include_extra_info,
+        include_metadata=include_metadata,
     )
 
-    nodes: List[Node] = []
+    nodes: List[TextNode] = []
     index_counter = 0
     for i, text_split in enumerate(text_splits):
         text_chunk = text_split.text_chunk
         logger.debug(f"> Adding chunk: {truncate_text(text_chunk, 50)}")
-        index_pos_info = None
+        start_char_idx = None
+        end_char_idx = None
         if text_split.num_char_overlap is not None:
-            index_pos_info = {
-                # NOTE: start is inclusive, end is exclusive
-                "start": index_counter - text_split.num_char_overlap,
-                "end": index_counter - text_split.num_char_overlap + len(text_chunk),
-            }
+            start_char_idx = index_counter - text_split.num_char_overlap
+            end_char_idx = index_counter - text_split.num_char_overlap + len(text_chunk)
         index_counter += len(text_chunk) + 1
 
         if isinstance(document, ImageDocument):
             image_node = ImageNode(
                 text=text_chunk,
                 embedding=document.embedding,
-                extra_info=document.extra_info if include_extra_info else None,
-                node_info=index_pos_info,
+                metadata=document.metadata if include_metadata else {},
+                start_char_idx=start_char_idx,
+                end_char_idx=end_char_idx,
                 image=document.image,
-                relationships={DocumentRelationship.SOURCE: document.get_doc_id()},
+                relationships={
+                    NodeRelationship.SOURCE: document.as_related_node_info()
+                },
             )
             nodes.append(image_node)  # type: ignore
-        else:
-            node = Node(
+        elif isinstance(document, Document):
+            node = TextNode(
                 text=text_chunk,
                 embedding=document.embedding,
-                extra_info=document.extra_info if include_extra_info else None,
-                node_info=index_pos_info,
-                relationships={DocumentRelationship.SOURCE: document.get_doc_id()},
+                start_char_idx=start_char_idx,
+                end_char_idx=end_char_idx,
+                metadata=document.metadata if include_metadata else {},
+                excluded_embed_metadata_keys=document.excluded_embed_metadata_keys,
+                excluded_llm_metadata_keys=document.excluded_llm_metadata_keys,
+                metadata_seperator=document.metadata_seperator,
+                text_template=document.text_template,
+                relationships={
+                    NodeRelationship.SOURCE: document.as_related_node_info()
+                },
             )
             nodes.append(node)
+        else:
+            raise ValueError(f"Unknown document type: {type(document)}")
 
     # if include_prev_next_rel, then add prev/next relationships
     if include_prev_next_rel:
         for i, node in enumerate(nodes):
             if i > 0:
-                node.relationships[DocumentRelationship.PREVIOUS] = nodes[
+                node.relationships[NodeRelationship.PREVIOUS] = nodes[
                     i - 1
-                ].get_doc_id()
+                ].as_related_node_info()
             if i < len(nodes) - 1:
-                node.relationships[DocumentRelationship.NEXT] = nodes[
+                node.relationships[NodeRelationship.NEXT] = nodes[
                     i + 1
-                ].get_doc_id()
+                ].as_related_node_info()
 
     return nodes
```

### Comparing `llama_index-0.6.9/llama_index/node_parser/simple.py` & `llama_index-0.7.0/llama_index/node_parser/simple.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,66 +1,103 @@
 """Simple node parser."""
 from typing import List, Optional, Sequence
 
 from llama_index.callbacks.base import CallbackManager
-from llama_index.callbacks.schema import CBEventType
-from llama_index.data_structs.node import Node
+from llama_index.callbacks.schema import CBEventType, EventPayload
+from llama_index.constants import DEFAULT_CHUNK_OVERLAP, DEFAULT_CHUNK_SIZE
 from llama_index.langchain_helpers.text_splitter import TextSplitter, TokenTextSplitter
-from llama_index.node_parser.node_utils import get_nodes_from_document
-from llama_index.readers.schema.base import Document
 from llama_index.node_parser.interface import NodeParser
+from llama_index.node_parser.node_utils import get_nodes_from_document
+from llama_index.utils import get_tqdm_iterable
+from llama_index.schema import Document
+from llama_index.schema import BaseNode
 
 
 class SimpleNodeParser(NodeParser):
     """Simple node parser.
 
     Splits a document into Nodes using a TextSplitter.
 
     Args:
         text_splitter (Optional[TextSplitter]): text splitter
-        include_extra_info (bool): whether to include extra info in nodes
+        include_metadata (bool): whether to include metadata in nodes
         include_prev_next_rel (bool): whether to include prev/next relationships
 
     """
 
     def __init__(
         self,
         text_splitter: Optional[TextSplitter] = None,
-        include_extra_info: bool = True,
+        include_metadata: bool = True,
         include_prev_next_rel: bool = True,
         callback_manager: Optional[CallbackManager] = None,
     ) -> None:
         """Init params."""
         self.callback_manager = callback_manager or CallbackManager([])
         self._text_splitter = text_splitter or TokenTextSplitter(
             callback_manager=self.callback_manager
         )
-        self._include_extra_info = include_extra_info
+        self._include_metadata = include_metadata
         self._include_prev_next_rel = include_prev_next_rel
 
+    @classmethod
+    def from_defaults(
+        cls,
+        chunk_size: Optional[int] = None,
+        chunk_overlap: Optional[int] = None,
+        include_metadata: bool = True,
+        include_prev_next_rel: bool = True,
+        callback_manager: Optional[CallbackManager] = None,
+    ) -> "SimpleNodeParser":
+        callback_manager = callback_manager or CallbackManager([])
+        chunk_size = chunk_size or DEFAULT_CHUNK_SIZE
+        chunk_overlap = (
+            chunk_overlap if chunk_overlap is not None else DEFAULT_CHUNK_OVERLAP
+        )
+
+        token_text_splitter = TokenTextSplitter(
+            chunk_size=chunk_size,
+            chunk_overlap=chunk_overlap,
+            callback_manager=callback_manager,
+        )
+        return cls(
+            text_splitter=token_text_splitter,
+            include_metadata=include_metadata,
+            include_prev_next_rel=include_prev_next_rel,
+            callback_manager=callback_manager,
+        )
+
     def get_nodes_from_documents(
         self,
         documents: Sequence[Document],
-    ) -> List[Node]:
+        show_progress: bool = False,
+    ) -> List[BaseNode]:
         """Parse document into nodes.
 
         Args:
             documents (Sequence[Document]): documents to parse
-            include_extra_info (bool): whether to include extra info in nodes
+            include_metadata (bool): whether to include metadata in nodes
 
         """
         event_id = self.callback_manager.on_event_start(
-            CBEventType.NODE_PARSING, payload={"documents": documents}
+            CBEventType.NODE_PARSING, payload={EventPayload.DOCUMENTS: documents}
         )
-        all_nodes: List[Node] = []
-        for document in documents:
+
+        all_nodes: List[BaseNode] = []
+        documents_with_progress = get_tqdm_iterable(
+            documents, show_progress, "Parsing documents into nodes"
+        )
+
+        for document in documents_with_progress:
             nodes = get_nodes_from_document(
                 document,
                 self._text_splitter,
-                self._include_extra_info,
+                self._include_metadata,
                 include_prev_next_rel=self._include_prev_next_rel,
             )
             all_nodes.extend(nodes)
         self.callback_manager.on_event_end(
-            CBEventType.NODE_PARSING, payload={"nodes": all_nodes}, event_id=event_id
+            CBEventType.NODE_PARSING,
+            payload={EventPayload.NODES: all_nodes},
+            event_id=event_id,
         )
         return all_nodes
```

### Comparing `llama_index-0.6.9/llama_index/optimization/optimizer.py` & `llama_index-0.7.0/llama_index/indices/postprocessor/optimizer.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,30 +1,22 @@
 """Optimization related classes and functions."""
 import logging
-from abc import abstractmethod
 from typing import Callable, List, Optional
 
 from llama_index.embeddings.base import BaseEmbedding
 from llama_index.embeddings.openai import OpenAIEmbedding
+from llama_index.indices.postprocessor.types import BaseNodePostprocessor
 from llama_index.indices.query.embedding_utils import get_top_k_embeddings
 from llama_index.indices.query.schema import QueryBundle
+from llama_index.schema import NodeWithScore, MetadataMode
 
 logger = logging.getLogger(__name__)
 
 
-class BaseTokenUsageOptimizer:
-    """Base class for optimizers that should be overwritten."""
-
-    @abstractmethod
-    def optimize(self, query_bundle: QueryBundle, text: str) -> str:
-        """Optimize the input text given the query."""
-        raise NotImplementedError("Not implemented yet.")
-
-
-class SentenceEmbeddingOptimizer(BaseTokenUsageOptimizer):
+class SentenceEmbeddingOptimizer(BaseNodePostprocessor):
     """Optimization of a text chunk given the query by shortening the input text."""
 
     def __init__(
         self,
         embed_model: Optional[BaseEmbedding] = None,
         percentile_cutoff: Optional[float] = None,
         threshold_cutoff: Optional[float] = None,
@@ -62,43 +54,66 @@
                 nltk.data.find("tokenizers/punkt")
             except LookupError:
                 nltk.download("punkt")
             tokenizer = nltk.data.load("tokenizers/punkt/english.pickle")
             tokenizer_fn = tokenizer.tokenize
         self._tokenizer_fn = tokenizer_fn
 
-    def optimize(self, query_bundle: QueryBundle, text: str) -> str:
-        """Optimize a text chunk given the query by shortening the input text."""
-        split_text = self._tokenizer_fn(text)
-
-        start_embed_token_ct = self.embed_model.total_tokens_used
-        if query_bundle.embedding is None:
-            query_bundle.embedding = self.embed_model.get_agg_embedding_from_queries(
-                query_bundle.embedding_strs
+    def postprocess_nodes(
+        self,
+        nodes: List[NodeWithScore],
+        query_bundle: Optional[QueryBundle] = None,
+    ) -> List[NodeWithScore]:
+        """Optimize a node text given the query by shortening the node text."""
+        if query_bundle is None:
+            return nodes
+
+        for i in range(len(nodes)):
+            text = nodes[i].node.get_content(metadata_mode=MetadataMode.LLM)
+
+            split_text = self._tokenizer_fn(text)
+
+            start_embed_token_ct = self.embed_model.total_tokens_used
+            if query_bundle.embedding is None:
+                query_bundle.embedding = (
+                    self.embed_model.get_agg_embedding_from_queries(
+                        query_bundle.embedding_strs
+                    )
+                )
+
+            text_embeddings = self.embed_model._get_text_embeddings(split_text)
+
+            num_top_k = None
+            threshold = None
+            if self._percentile_cutoff is not None:
+                num_top_k = int(len(split_text) * self._percentile_cutoff)
+            if self._threshold_cutoff is not None:
+                threshold = self._threshold_cutoff
+
+            top_similarities, top_idxs = get_top_k_embeddings(
+                query_embedding=query_bundle.embedding,
+                embeddings=text_embeddings,
+                similarity_fn=self.embed_model.similarity,
+                similarity_top_k=num_top_k,
+                embedding_ids=list(range(len(text_embeddings))),
+                similarity_cutoff=threshold,
             )
-        text_embeddings = self.embed_model._get_text_embeddings(split_text)
-        num_top_k = None
-        threshold = None
-        if self._percentile_cutoff is not None:
-            num_top_k = int(len(split_text) * self._percentile_cutoff)
-        if self._threshold_cutoff is not None:
-            threshold = self._threshold_cutoff
-        top_similarities, top_idxs = get_top_k_embeddings(
-            query_embedding=query_bundle.embedding,
-            embeddings=text_embeddings,
-            similarity_fn=self.embed_model.similarity,
-            similarity_top_k=num_top_k,
-            embedding_ids=[i for i in range(len(text_embeddings))],
-            similarity_cutoff=threshold,
-        )
-        net_embed_tokens = self.embed_model.total_tokens_used - start_embed_token_ct
-        logger.info(
-            f"> [optimize] Total embedding token usage: " f"{net_embed_tokens} tokens"
-        )
-        if len(top_idxs) == 0:
-            raise ValueError("Optimizer returned zero sentences.")
-        top_sentences = [split_text[i] for i in top_idxs]
-
-        logger.debug(f"> Top {len(top_idxs)} sentences with scores:\n")
-        for i in range(len(top_idxs)):
-            logger.debug(f"{i}. {top_sentences[i]} ({top_similarities[i]})")
-        return " ".join(top_sentences)
+
+            net_embed_tokens = self.embed_model.total_tokens_used - start_embed_token_ct
+            logger.info(
+                f"> [optimize] Total embedding token usage: "
+                f"{net_embed_tokens} tokens"
+            )
+
+            if len(top_idxs) == 0:
+                raise ValueError("Optimizer returned zero sentences.")
+
+            top_sentences = [split_text[i] for i in top_idxs]
+
+            logger.debug(f"> Top {len(top_idxs)} sentences with scores:\n")
+            if logger.isEnabledFor(logging.DEBUG):
+                for i in range(len(top_idxs)):
+                    logger.debug(f"{i}. {top_sentences[i]} ({top_similarities[i]})")
+
+            nodes[i].node.set_content(" ".join(top_sentences))
+
+        return nodes
```

### Comparing `llama_index-0.6.9/llama_index/output_parsers/guardrails.py` & `llama_index-0.7.0/llama_index/output_parsers/guardrails.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,17 +8,17 @@
 except ImportError:
     Guard = None
     PromptCallable = None
 
 from copy import deepcopy
 from typing import Any, Callable, Optional
 
-from langchain.llms.base import BaseLLM
+from llama_index.bridge.langchain import BaseLLM
 
-from llama_index.output_parsers.base import BaseOutputParser
+from llama_index.types import BaseOutputParser
 
 
 def get_callable(llm: Optional[BaseLLM]) -> Optional[Callable]:
     """Get callable."""
     if llm is None:
         return None
```

### Comparing `llama_index-0.6.9/llama_index/output_parsers/langchain.py` & `llama_index-0.7.0/llama_index/output_parsers/langchain.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Base output parser class."""
 
 from string import Formatter
 from typing import Any, Optional
 
-from langchain.schema import BaseOutputParser as LCOutputParser
+from llama_index.bridge.langchain import BaseOutputParser as LCOutputParser
 
-from llama_index.output_parsers.base import BaseOutputParser
+from llama_index.types import BaseOutputParser
 
 
 class LangchainOutputParser(BaseOutputParser):
     """Langchain output parser."""
 
     def __init__(
         self, output_parser: LCOutputParser, format_key: Optional[str] = None
```

### Comparing `llama_index-0.6.9/llama_index/output_parsers/selection.py` & `llama_index-0.7.0/llama_index/output_parsers/selection.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 import json
 from dataclasses import dataclass
 from typing import Any
 
 from dataclasses_json import DataClassJsonMixin
 
-from llama_index.output_parsers.base import BaseOutputParser, StructuredOutput
+from llama_index.output_parsers.base import StructuredOutput
+from llama_index.types import BaseOutputParser
 
 
 def _escape_curly_braces(input_string: str) -> str:
     # Replace '{' with '{{' and '}' with '}}' to escape curly braces
     escaped_string = input_string.replace("{", "{{").replace("}", "}}")
     return escaped_string
 
@@ -43,33 +44,43 @@
 class Answer(DataClassJsonMixin):
     choice: int
     reason: str
 
 
 class SelectionOutputParser(BaseOutputParser):
     def _marshal_llm_to_json(self, output: str) -> str:
-        """Extract a valid JSON array from a string.
-        Extracts a substring that represents a valid JSON array.
+        """Extract a valid JSON object or array from a string.
+        Extracts a substring that represents a valid JSON object or array.
 
         Args:
-            output: A string that may contain a valid JSON array surrounded by
+            output: A string that may contain a valid JSON object or array surrounded by
             extraneous characters or information.
 
         Returns:
-            A string representing a valid JSON array.
+            A string representing a valid JSON object or array.
 
         """
         output = output.strip()
-        left = output.find("[")
-        right = output.rfind("]")
+        left_square = output.find("[")
+        left_brace = output.find("{")
+
+        if left_square < left_brace:
+            left = left_square
+            right = output.rfind("]")
+        else:
+            left = left_brace
+            right = output.rfind("}")
+
         output = output[left : right + 1]
         return output
 
     def parse(self, output: str) -> Any:
         output = self._marshal_llm_to_json(output)
-        json_list = json.loads(output)
-        answers = [Answer.from_dict(json_dict) for json_dict in json_list]
+        json_output = json.loads(output)
+        if isinstance(json_output, dict):
+            json_output = [json_output]
+        answers = [Answer.from_dict(json_dict) for json_dict in json_output]
         return StructuredOutput(raw_output=output, parsed_output=answers)
 
     def format(self, prompt_template: str) -> str:
         fmt = prompt_template + "\n\n" + _escape_curly_braces(FORMAT_STR)
         return fmt
```

### Comparing `llama_index-0.6.9/llama_index/playground/base.py` & `llama_index-0.7.0/llama_index/playground/base.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,49 +1,49 @@
 """Experiment with different indices, models, and more."""
 from __future__ import annotations
 
 import time
 from typing import Any, Dict, List, Optional, Type, Union
 
 import pandas as pd
-from langchain.input import get_color_mapping, print_text
+from llama_index.bridge.langchain import get_color_mapping, print_text
 
-from llama_index.indices.base import BaseGPTIndex
-from llama_index.indices.list.base import GPTListIndex, ListRetrieverMode
-from llama_index.indices.tree.base import GPTTreeIndex, TreeRetrieverMode
-from llama_index.indices.vector_store import GPTVectorStoreIndex
-from llama_index.readers.schema.base import Document
-
-DEFAULT_INDEX_CLASSES: List[Type[BaseGPTIndex]] = [
-    GPTVectorStoreIndex,
-    GPTTreeIndex,
-    GPTListIndex,
+from llama_index.indices.base import BaseIndex
+from llama_index.indices.list.base import ListIndex, ListRetrieverMode
+from llama_index.indices.tree.base import TreeIndex, TreeRetrieverMode
+from llama_index.indices.vector_store import VectorStoreIndex
+from llama_index.schema import Document
+
+DEFAULT_INDEX_CLASSES: List[Type[BaseIndex]] = [
+    VectorStoreIndex,
+    TreeIndex,
+    ListIndex,
 ]
 
-INDEX_SPECIFIC_QUERY_MODES_TYPE = Dict[Type[BaseGPTIndex], List[str]]
+INDEX_SPECIFIC_QUERY_MODES_TYPE = Dict[Type[BaseIndex], List[str]]
 
 DEFAULT_MODES: INDEX_SPECIFIC_QUERY_MODES_TYPE = {
-    GPTTreeIndex: [e.value for e in TreeRetrieverMode],
-    GPTListIndex: [e.value for e in ListRetrieverMode],
-    GPTVectorStoreIndex: ["default"],
+    TreeIndex: [e.value for e in TreeRetrieverMode],
+    ListIndex: [e.value for e in ListRetrieverMode],
+    VectorStoreIndex: ["default"],
 }
 
 
 class Playground:
     """Experiment with indices, models, embeddings, retriever_modes, and more."""
 
     def __init__(
         self,
-        indices: List[BaseGPTIndex],
+        indices: List[BaseIndex],
         retriever_modes: INDEX_SPECIFIC_QUERY_MODES_TYPE = DEFAULT_MODES,
     ):
         """Initialize with indices to experiment with.
 
         Args:
-            indices: A list of BaseGPTIndex's to experiment with
+            indices: A list of BaseIndex's to experiment with
             retriever_modes: A list of retriever_modes that specify which nodes are
                 chosen from the index when a query is made. A full list of
                 retriever_modes available to each index can be found here:
                 https://gpt-index.readthedocs.io/en/latest/reference/query.html
         """
         self._validate_indices(indices)
         self._indices = indices
@@ -53,15 +53,15 @@
         index_range = [str(i) for i in range(len(indices))]
         self.index_colors = get_color_mapping(index_range)
 
     @classmethod
     def from_docs(
         cls,
         documents: List[Document],
-        index_classes: List[Type[BaseGPTIndex]] = DEFAULT_INDEX_CLASSES,
+        index_classes: List[Type[BaseIndex]] = DEFAULT_INDEX_CLASSES,
         retriever_modes: INDEX_SPECIFIC_QUERY_MODES_TYPE = DEFAULT_MODES,
         **kwargs: Any,
     ) -> Playground:
         """Initialize with Documents using the default list of indices.
 
         Args:
             documents: A List of Documents to experiment with.
@@ -73,31 +73,31 @@
 
         indices = [
             index_class.from_documents(documents, **kwargs)
             for index_class in index_classes
         ]
         return cls(indices, retriever_modes)
 
-    def _validate_indices(self, indices: List[BaseGPTIndex]) -> None:
+    def _validate_indices(self, indices: List[BaseIndex]) -> None:
         """Validate a list of indices."""
         if len(indices) == 0:
             raise ValueError("Playground must have a non-empty list of indices.")
         for index in indices:
-            if not isinstance(index, BaseGPTIndex):
+            if not isinstance(index, BaseIndex):
                 raise ValueError(
-                    "Every index in Playground should be an instance of BaseGPTIndex."
+                    "Every index in Playground should be an instance of BaseIndex."
                 )
 
     @property
-    def indices(self) -> List[BaseGPTIndex]:
+    def indices(self) -> List[BaseIndex]:
         """Get Playground's indices."""
         return self._indices
 
     @indices.setter
-    def indices(self, indices: List[BaseGPTIndex]) -> None:
+    def indices(self, indices: List[BaseIndex]) -> None:
         """Set Playground's indices."""
         self._validate_indices(indices)
         self._indices = indices
 
     def _validate_modes(self, retriever_modes: INDEX_SPECIFIC_QUERY_MODES_TYPE) -> None:
         """Validate a list of retriever_modes."""
         if len(retriever_modes) == 0:
@@ -150,25 +150,20 @@
                     continue
 
                 output = query_engine.query(query_text)
                 print_text(str(output), color=self.index_colors[str(i)], end="\n\n")
 
                 duration = time.time() - start_time
 
-                llm_token_usage = index.service_context.llm_predictor.last_token_usage
-                embed_token_usage = index.service_context.embed_model.last_token_usage
-
                 result.append(
                     {
                         "Index": index_name,
                         "Retriever Mode": retriever_mode,
                         "Output": str(output),
                         "Duration": duration,
-                        "LLM Tokens": llm_token_usage,
-                        "Embedding Tokens": embed_token_usage,
                     }
                 )
         print(f"\nRan {len(result)} combinations in total.")
 
         if to_pandas:
             return pd.DataFrame(result)
         else:
```

### Comparing `llama_index-0.6.9/llama_index/prompts/base.py` & `llama_index-0.7.0/llama_index/prompts/base.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,122 +1,112 @@
 """Base module for prompts."""
 from copy import deepcopy
-from string import Formatter
-from typing import Any, Dict, List, Optional, Type, TypeVar
+from typing import Any, Dict, List, Optional
 
-from langchain import BasePromptTemplate as BaseLangchainPrompt
-from langchain import PromptTemplate as LangchainPrompt
-from langchain.chains.prompt_selector import ConditionalPromptSelector
-from langchain.base_language import BaseLanguageModel
-
-from llama_index.output_parsers.base import BaseOutputParser
+from llama_index.bridge.langchain import BasePromptTemplate as BaseLangchainPrompt
+from llama_index.bridge.langchain import PromptTemplate as LangchainPrompt
+from llama_index.llms.base import LLM, ChatMessage
+from llama_index.llms.langchain_utils import from_lc_messages
+from llama_index.prompts.prompt_selector import PromptSelector
 from llama_index.prompts.prompt_type import PromptType
-
-PMT = TypeVar("PMT", bound="Prompt")
+from llama_index.types import BaseOutputParser
 
 
 class Prompt:
     """Prompt class for LlamaIndex.
 
     Wrapper around langchain's prompt class. Adds ability to:
         - enforce certain prompt types
         - partially fill values
         - define stop token
 
     """
 
-    input_variables: List[str]
-    prompt_type: str = PromptType.CUSTOM
-
     def __init__(
         self,
         template: Optional[str] = None,
         langchain_prompt: Optional[BaseLangchainPrompt] = None,
-        langchain_prompt_selector: Optional[ConditionalPromptSelector] = None,
+        langchain_prompt_selector: Optional[PromptSelector] = None,
         stop_token: Optional[str] = None,
         output_parser: Optional[BaseOutputParser] = None,
+        prompt_type: str = PromptType.CUSTOM,
+        metadata: Optional[Dict[str, Any]] = None,
         **prompt_kwargs: Any,
     ) -> None:
         """Init params."""
         # first check if langchain_prompt_selector is provided
         # TODO: self.prompt is deprecated, switch to prompt_selector under the hood
         if langchain_prompt_selector is not None:
             self.prompt_selector = langchain_prompt_selector
             self.prompt: BaseLangchainPrompt = self.prompt_selector.default_prompt
         # then check if template is provided
         elif langchain_prompt is None:
             if template is None:
                 raise ValueError(
                     "`template` must be specified if `langchain_prompt` is None"
                 )
-            # validate
-            tmpl_vars = {
-                v for _, v, _, _ in Formatter().parse(template) if v is not None
-            }
-            if tmpl_vars != set(self.input_variables):
-                raise ValueError(
-                    f"Invalid template: {template}, variables do not match the "
-                    f"required input_variables: {self.input_variables}"
-                )
 
-            self.prompt = LangchainPrompt(
-                input_variables=self.input_variables, template=template, **prompt_kwargs
+            self.prompt = LangchainPrompt.from_template(
+                template=template, **prompt_kwargs
             )
-            self.prompt_selector = ConditionalPromptSelector(default_prompt=self.prompt)
+            self.prompt_selector = PromptSelector(default_prompt=self.prompt)
         # finally, check if langchain_prompt is provided
         else:
             if template:
                 raise ValueError(
                     f"Both template ({template}) and langchain_prompt "
                     f"({langchain_prompt}) are provided, only one should be."
                 )
             self.prompt = langchain_prompt
-            self.prompt_selector = ConditionalPromptSelector(default_prompt=self.prompt)
+            self.prompt_selector = PromptSelector(default_prompt=self.prompt)
 
-        # validate all prompts in prompt selector
-        all_lc_prompts = [self.prompt_selector.default_prompt]
-        for _, prompt in self.prompt_selector.conditionals:
-            all_lc_prompts.append(prompt)
-        for lc_prompt in all_lc_prompts:
-            if set(lc_prompt.input_variables) != set(self.input_variables):
-                raise ValueError(
-                    f"Invalid prompt: {langchain_prompt}, variables do not match the "
-                    f"required input_variables: {self.input_variables}"
-                )
         self.partial_dict: Dict[str, Any] = {}
         self.prompt_kwargs = prompt_kwargs
         self.stop_token = stop_token
+        # NOTE: this is only used for token counting and testing
+        self.prompt_type = prompt_type
 
         self.output_parser = output_parser
 
+        self._original_template = template
+
+        # Metadata is used to pass arbitrary information to other consumers of the
+        # prompt. For example, VellumPromptRegistry uses this to access vellum-specific
+        # identifiers that users can pass along with the prompt.
+        self.metadata = metadata or {}
+
+    @property
+    def original_template(self) -> str:
+        """Return the originally specified template, if supplied."""
+
+        if not self._original_template:
+            raise ValueError("No original template specified.")
+
+        return self._original_template
+
     @classmethod
     def from_langchain_prompt(
-        cls: Type[PMT], prompt: BaseLangchainPrompt, **kwargs: Any
-    ) -> PMT:
+        cls, prompt: BaseLangchainPrompt, **kwargs: Any
+    ) -> "Prompt":
         """Load prompt from LangChain prompt."""
         return cls(langchain_prompt=prompt, **kwargs)
 
     @classmethod
     def from_langchain_prompt_selector(
-        cls: Type[PMT], prompt_selector: ConditionalPromptSelector, **kwargs: Any
-    ) -> PMT:
+        cls, prompt_selector: PromptSelector, **kwargs: Any
+    ) -> "Prompt":
         """Load prompt from LangChain prompt."""
         return cls(langchain_prompt_selector=prompt_selector, **kwargs)
 
-    def partial_format(self: PMT, **kwargs: Any) -> PMT:
+    def partial_format(self, **kwargs: Any) -> "Prompt":
         """Format the prompt partially.
 
         Return an instance of itself.
 
         """
-        for k in kwargs.keys():
-            if k not in self.input_variables:
-                raise ValueError(
-                    f"Invalid input variable: {k}, not found in input_variables"
-                )
         try:
             # NOTE: this is a hack to get around deepcopy failing on output parser
             output_parser = self.output_parser
             self.output_parser = None
 
             copy_obj = deepcopy(self)
             copy_obj.output_parser = output_parser
@@ -125,16 +115,19 @@
         except Exception as e:
             raise e
 
         return copy_obj
 
     @classmethod
     def from_prompt(
-        cls: Type[PMT], prompt: "Prompt", llm: Optional[BaseLanguageModel] = None
-    ) -> PMT:
+        cls,
+        prompt: "Prompt",
+        llm: Optional[LLM] = None,
+        prompt_type: Optional[PromptType] = None,
+    ) -> "Prompt":
         """Create a prompt from an existing prompt.
 
         Use case: If the existing prompt is already partially filled,
         and the remaining fields satisfy the requirements of the
         prompt class, then we can create a new prompt from the existing
         partially filled prompt.
 
@@ -143,31 +136,41 @@
         tmpl_vars = lc_prompt.input_variables
         format_dict = {}
         for var in tmpl_vars:
             if var not in prompt.partial_dict:
                 format_dict[var] = f"{{{var}}}"
 
         template_str = prompt.format(llm=llm, **format_dict)
-        cls_obj: PMT = cls(template_str, **prompt.prompt_kwargs)
+        cls_obj = cls(
+            template_str,
+            prompt_type=prompt_type or PromptType.CUSTOM,
+            **prompt.prompt_kwargs,
+        )
         return cls_obj
 
-    def get_langchain_prompt(
-        self, llm: Optional[BaseLanguageModel] = None
-    ) -> BaseLangchainPrompt:
+    def get_langchain_prompt(self, llm: Optional[LLM] = None) -> BaseLangchainPrompt:
         """Get langchain prompt."""
-        if llm is None:
-            return self.prompt_selector.default_prompt
-        return self.prompt_selector.get_prompt(llm=llm)
+        return self.prompt_selector.select(llm=llm)
 
-    def format(self, llm: Optional[BaseLanguageModel] = None, **kwargs: Any) -> str:
-        """Format the prompt."""
+    def format(self, llm: Optional[LLM] = None, **kwargs: Any) -> str:
+        """Format the prompt into a string."""
         kwargs.update(self.partial_dict)
         lc_prompt = self.get_langchain_prompt(llm=llm)
         return lc_prompt.format(**kwargs)
 
+    def format_messages(
+        self, llm: Optional[LLM] = None, **kwargs: Any
+    ) -> List[ChatMessage]:
+        """Format the prompt into a list of chat messages."""
+        kwargs.update(self.partial_dict)
+        lc_template = self.get_langchain_prompt(llm=llm)
+        lc_value = lc_template.format_prompt(**kwargs)
+        lc_messages = lc_value.to_messages()
+        return from_lc_messages(lc_messages)
+
     def get_full_format_args(self, kwargs: Dict) -> Dict[str, Any]:
         """Get dict of all format args.
 
         Hack to pass into Langchain to pass validation.
 
         """
         kwargs.update(self.partial_dict)
```

### Comparing `llama_index-0.6.9/llama_index/prompts/chat_prompts.py` & `llama_index-0.7.0/llama_index/prompts/chat_prompts.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,30 +1,29 @@
 """Prompts for ChatGPT."""
 
-from langchain.prompts.chat import (
+from llama_index.bridge.langchain import (
     AIMessagePromptTemplate,
     ChatPromptTemplate,
     HumanMessagePromptTemplate,
 )
 
 from llama_index.prompts.prompts import RefinePrompt, RefineTableContextPrompt
 
 # Refine Prompt
 CHAT_REFINE_PROMPT_TMPL_MSGS = [
-    HumanMessagePromptTemplate.from_template("{query_str}"),
-    AIMessagePromptTemplate.from_template("{existing_answer}"),
     HumanMessagePromptTemplate.from_template(
         "We have the opportunity to refine the above answer "
         "(only if needed) with some more context below.\n"
         "------------\n"
         "{context_msg}\n"
         "------------\n"
         "Given the new context, refine the original answer to better "
-        "answer the question. "
-        "If the context isn't useful, output the original answer again.",
+        "answer the question: {query_str}. "
+        "If the context isn't useful, output the original answer again.\n"
+        "Original Answer: {existing_answer}"
     ),
 ]
 
 
 CHAT_REFINE_PROMPT_LC = ChatPromptTemplate.from_messages(CHAT_REFINE_PROMPT_TMPL_MSGS)
 CHAT_REFINE_PROMPT = RefinePrompt.from_langchain_prompt(CHAT_REFINE_PROMPT_LC)
```

### Comparing `llama_index-0.6.9/llama_index/prompts/default_choice_select.py` & `llama_index-0.7.0/llama_index/prompts/choice_select.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,10 +1,14 @@
 """Default choice select prompt."""
 
-from llama_index.prompts.prompts import QuestionAnswerPrompt
+from llama_index.prompts.base import Prompt
+from llama_index.prompts.prompt_type import PromptType
+
+# deprecated, kept for backward compatibility
+ChoiceSelectPrompt = Prompt
 
 DEFAULT_CHOICE_SELECT_PROMPT_TMPL = (
     "A list of documents is shown below. Each document has a number next to it along "
     "with a summary of the document. A question is also provided. \n"
     "Respond with the numbers of the documents "
     "you should consult to answer the question, in order of relevance, as well \n"
     "as the relevance score. The relevance score is a number from 1-10 based on "
@@ -21,8 +25,10 @@
     "Doc: 3, Relevance: 4\n"
     "Doc: 7, Relevance: 3\n\n"
     "Let's try this now: \n\n"
     "{context_str}\n"
     "Question: {query_str}\n"
     "Answer:\n"
 )
-DEFAULT_CHOICE_SELECT_PROMPT = QuestionAnswerPrompt(DEFAULT_CHOICE_SELECT_PROMPT_TMPL)
+DEFAULT_CHOICE_SELECT_PROMPT = Prompt(
+    DEFAULT_CHOICE_SELECT_PROMPT_TMPL, prompt_type=PromptType.CHOICE_SELECT
+)
```

### Comparing `llama_index-0.6.9/llama_index/prompts/default_prompt_selectors.py` & `llama_index-0.7.0/llama_index/prompts/default_prompt_selectors.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,31 +1,33 @@
 """Prompt selectors."""
-from langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model
-
 from llama_index.prompts.chat_prompts import (
     CHAT_REFINE_PROMPT,
     CHAT_REFINE_TABLE_CONTEXT_PROMPT,
 )
 from llama_index.prompts.default_prompts import (
     DEFAULT_REFINE_PROMPT,
     DEFAULT_REFINE_TABLE_CONTEXT_PROMPT,
 )
+from llama_index.prompts.prompt_selector import PromptSelector, is_chat_model
+from llama_index.prompts.prompt_type import PromptType
 from llama_index.prompts.prompts import RefinePrompt, RefineTableContextPrompt
 
-DEFAULT_REFINE_PROMPT_SEL_LC = ConditionalPromptSelector(
+DEFAULT_REFINE_PROMPT_SEL_LC = PromptSelector(
     default_prompt=DEFAULT_REFINE_PROMPT.get_langchain_prompt(),
     conditionals=[(is_chat_model, CHAT_REFINE_PROMPT.get_langchain_prompt())],
 )
 DEFAULT_REFINE_PROMPT_SEL = RefinePrompt(
-    langchain_prompt_selector=DEFAULT_REFINE_PROMPT_SEL_LC
+    langchain_prompt_selector=DEFAULT_REFINE_PROMPT_SEL_LC,
+    prompt_type=PromptType.REFINE,
 )
 
-DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL_LC = ConditionalPromptSelector(
+DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL_LC = PromptSelector(
     default_prompt=DEFAULT_REFINE_TABLE_CONTEXT_PROMPT.get_langchain_prompt(),
     conditionals=[
         (is_chat_model, CHAT_REFINE_TABLE_CONTEXT_PROMPT.get_langchain_prompt())
     ],
 )
 
 DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL = RefineTableContextPrompt(
-    langchain_prompt_selector=DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL_LC
+    langchain_prompt_selector=DEFAULT_REFINE_TABLE_CONTEXT_PROMPT_SEL_LC,
+    prompt_type=PromptType.TABLE_CONTEXT,
 )
```

### Comparing `llama_index-0.6.9/llama_index/prompts/default_prompts.py` & `llama_index-0.7.0/llama_index/prompts/default_prompts.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,26 +1,11 @@
 """Set of default prompts."""
 
-from llama_index.prompts.prompts import (
-    KeywordExtractPrompt,
-    KnowledgeGraphPrompt,
-    PandasPrompt,
-    QueryKeywordExtractPrompt,
-    QuestionAnswerPrompt,
-    RefinePrompt,
-    RefineTableContextPrompt,
-    SchemaExtractPrompt,
-    SimpleInputPrompt,
-    SummaryPrompt,
-    TableContextPrompt,
-    TextToSQLPrompt,
-    TreeInsertPrompt,
-    TreeSelectMultiplePrompt,
-    TreeSelectPrompt,
-)
+from llama_index.prompts.base import Prompt
+from llama_index.prompts.prompt_type import PromptType
 
 ############################################
 # Tree
 ############################################
 
 DEFAULT_SUMMARY_PROMPT_TMPL = (
     "Write a summary of the following. Try to use only the "
@@ -30,15 +15,17 @@
     "\n"
     "{context_str}\n"
     "\n"
     "\n"
     'SUMMARY:"""\n'
 )
 
-DEFAULT_SUMMARY_PROMPT = SummaryPrompt(DEFAULT_SUMMARY_PROMPT_TMPL)
+DEFAULT_SUMMARY_PROMPT = Prompt(
+    DEFAULT_SUMMARY_PROMPT_TMPL, prompt_type=PromptType.SUMMARY
+)
 
 # insert prompts
 DEFAULT_INSERT_PROMPT_TMPL = (
     "Context information is below. It is provided in a numbered list "
     "(1 to {num_chunks}),"
     "where each item in the list corresponds to a summary.\n"
     "---------------------\n"
@@ -46,15 +33,17 @@
     "---------------------\n"
     "Given the context information, here is a new piece of "
     "information: {new_chunk_text}\n"
     "Answer with the number corresponding to the summary that should be updated. "
     "The answer should be the number corresponding to the "
     "summary that is most relevant to the question.\n"
 )
-DEFAULT_INSERT_PROMPT = TreeInsertPrompt(DEFAULT_INSERT_PROMPT_TMPL)
+DEFAULT_INSERT_PROMPT = Prompt(
+    DEFAULT_INSERT_PROMPT_TMPL, prompt_type=PromptType.TREE_INSERT
+)
 
 
 # # single choice
 DEFAULT_QUERY_PROMPT_TMPL = (
     "Some choices are given below. It is provided in a numbered list "
     "(1 to {num_chunks}),"
     "where each item in the list corresponds to a summary.\n"
@@ -62,15 +51,17 @@
     "{context_list}"
     "\n---------------------\n"
     "Using only the choices above and not prior knowledge, return "
     "the choice that is most relevant to the question: '{query_str}'\n"
     "Provide choice in the following format: 'ANSWER: <number>' and explain why "
     "this summary was selected in relation to the question.\n"
 )
-DEFAULT_QUERY_PROMPT = TreeSelectPrompt(DEFAULT_QUERY_PROMPT_TMPL)
+DEFAULT_QUERY_PROMPT = Prompt(
+    DEFAULT_QUERY_PROMPT_TMPL, prompt_type=PromptType.TREE_SELECT
+)
 
 # multiple choice
 DEFAULT_QUERY_PROMPT_MULTIPLE_TMPL = (
     "Some choices are given below. It is provided in a numbered "
     "list (1 to {num_chunks}), "
     "where each item in the list corresponds to a summary.\n"
     "---------------------\n"
@@ -78,16 +69,16 @@
     "\n---------------------\n"
     "Using only the choices above and not prior knowledge, return the top choices "
     "(no more than {branching_factor}, ranked by most relevant to least) that "
     "are most relevant to the question: '{query_str}'\n"
     "Provide choices in the following format: 'ANSWER: <numbers>' and explain why "
     "these summaries were selected in relation to the question.\n"
 )
-DEFAULT_QUERY_PROMPT_MULTIPLE = TreeSelectMultiplePrompt(
-    DEFAULT_QUERY_PROMPT_MULTIPLE_TMPL
+DEFAULT_QUERY_PROMPT_MULTIPLE = Prompt(
+    DEFAULT_QUERY_PROMPT_MULTIPLE_TMPL, prompt_type=PromptType.TREE_SELECT_MULTIPLE
 )
 
 
 DEFAULT_REFINE_PROMPT_TMPL = (
     "The original question is as follows: {query_str}\n"
     "We have provided an existing answer: {existing_answer}\n"
     "We have the opportunity to refine the existing answer "
@@ -95,58 +86,63 @@
     "------------\n"
     "{context_msg}\n"
     "------------\n"
     "Given the new context, refine the original answer to better "
     "answer the question. "
     "If the context isn't useful, return the original answer."
 )
-DEFAULT_REFINE_PROMPT = RefinePrompt(DEFAULT_REFINE_PROMPT_TMPL)
+DEFAULT_REFINE_PROMPT = Prompt(
+    DEFAULT_REFINE_PROMPT_TMPL, prompt_type=PromptType.REFINE
+)
 
 
 DEFAULT_TEXT_QA_PROMPT_TMPL = (
-    "Context information is below. \n"
+    "Context information is below.\n"
+    "---------------------\n"
+    "{context_str}\n"
     "---------------------\n"
-    "{context_str}"
-    "\n---------------------\n"
     "Given the context information and not prior knowledge, "
     "answer the question: {query_str}\n"
 )
-DEFAULT_TEXT_QA_PROMPT = QuestionAnswerPrompt(DEFAULT_TEXT_QA_PROMPT_TMPL)
+DEFAULT_TEXT_QA_PROMPT = Prompt(
+    DEFAULT_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER
+)
 
 
 ############################################
 # Keyword Table
 ############################################
 
 DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL = (
     "Some text is provided below. Given the text, extract up to {max_keywords} "
     "keywords from the text. Avoid stopwords."
     "---------------------\n"
     "{text}\n"
     "---------------------\n"
     "Provide keywords in the following comma-separated format: 'KEYWORDS: <keywords>'\n"
 )
-DEFAULT_KEYWORD_EXTRACT_TEMPLATE = KeywordExtractPrompt(
-    DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL
+DEFAULT_KEYWORD_EXTRACT_TEMPLATE = Prompt(
+    DEFAULT_KEYWORD_EXTRACT_TEMPLATE_TMPL, prompt_type=PromptType.KEYWORD_EXTRACT
 )
 
 
 # NOTE: the keyword extraction for queries can be the same as
 # the one used to build the index, but here we tune it to see if performance is better.
 DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL = (
     "A question is provided below. Given the question, extract up to {max_keywords} "
     "keywords from the text. Focus on extracting the keywords that we can use "
     "to best lookup answers to the question. Avoid stopwords.\n"
     "---------------------\n"
     "{question}\n"
     "---------------------\n"
     "Provide keywords in the following comma-separated format: 'KEYWORDS: <keywords>'\n"
 )
-DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE = QueryKeywordExtractPrompt(
-    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL
+DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE = Prompt(
+    DEFAULT_QUERY_KEYWORD_EXTRACT_TEMPLATE_TMPL,
+    prompt_type=PromptType.QUERY_KEYWORD_EXTRACT,
 )
 
 
 ############################################
 # Structured Store
 ############################################
 
@@ -162,24 +158,26 @@
     "Given the text and schema, extract the relevant fields from the text in "
     "the following format: "
     "field1: <value>\nfield2: <value>\n...\n\n"
     "If a field is not present in the text, don't include it in the output."
     "If no fields are present in the text, return a blank string.\n"
     "Fields: "
 )
-DEFAULT_SCHEMA_EXTRACT_PROMPT = SchemaExtractPrompt(DEFAULT_SCHEMA_EXTRACT_TMPL)
+DEFAULT_SCHEMA_EXTRACT_PROMPT = Prompt(
+    DEFAULT_SCHEMA_EXTRACT_TMPL, prompt_type=PromptType.SCHEMA_EXTRACT
+)
 
 # NOTE: taken from langchain and adapted
 # https://tinyurl.com/b772sd77
 DEFAULT_TEXT_TO_SQL_TMPL = (
     "Given an input question, first create a syntactically correct {dialect} "
     "query to run, then look at the results of the query and return the answer. "
     "You can order the results by a relevant column to return the most "
     "interesting examples in the database.\n"
-    "Never query for all the columns from a specific table, only ask for a the "
+    "Never query for all the columns from a specific table, only ask for a "
     "few relevant columns given the question.\n"
     "Pay attention to use only the column names that you can see in the schema "
     "description. "
     "Be careful to not query for columns that do not exist. "
     "Pay attention to which column is in which table. "
     "Also, qualify column names with the table name when needed.\n"
     "Use the following format:\n"
@@ -189,16 +187,18 @@
     "Answer: Final answer here\n"
     "Only use the tables listed below.\n"
     "{schema}\n"
     "Question: {query_str}\n"
     "SQLQuery: "
 )
 
-DEFAULT_TEXT_TO_SQL_PROMPT = TextToSQLPrompt(
-    DEFAULT_TEXT_TO_SQL_TMPL, stop_token="\nSQLResult:"
+DEFAULT_TEXT_TO_SQL_PROMPT = Prompt(
+    DEFAULT_TEXT_TO_SQL_TMPL,
+    stop_token="\nSQLResult:",
+    prompt_type=PromptType.TEXT_TO_SQL,
 )
 
 
 # NOTE: by partially filling schema, we can reduce to a QuestionAnswer prompt
 # that we can feed to ur table
 DEFAULT_TABLE_CONTEXT_TMPL = (
     "We have provided a table schema below. "
@@ -218,15 +218,17 @@
     "Provide answers in the following format:\n"
     "TableDescription: <description>\n"
     "Column1Description: <description>\n"
     "Column2Description: <description>\n"
     "...\n\n"
 )
 
-DEFAULT_TABLE_CONTEXT_PROMPT = TableContextPrompt(DEFAULT_TABLE_CONTEXT_TMPL)
+DEFAULT_TABLE_CONTEXT_PROMPT = Prompt(
+    DEFAULT_TABLE_CONTEXT_TMPL, prompt_type=PromptType.TABLE_CONTEXT
+)
 
 # NOTE: by partially filling schema, we can reduce to a RefinePrompt
 # that we can feed to ur table
 DEFAULT_REFINE_TABLE_CONTEXT_TMPL = (
     "We have provided a table schema below. "
     "---------------------\n"
     "{schema}\n"
@@ -237,16 +239,16 @@
     "Given the context information and the table schema, "
     "give a response to the following task: {query_str}\n"
     "We have provided an existing answer: {existing_answer}\n"
     "Given the new context, refine the original answer to better "
     "answer the question. "
     "If the context isn't useful, return the original answer."
 )
-DEFAULT_REFINE_TABLE_CONTEXT_PROMPT = RefineTableContextPrompt(
-    DEFAULT_REFINE_TABLE_CONTEXT_TMPL
+DEFAULT_REFINE_TABLE_CONTEXT_PROMPT = Prompt(
+    DEFAULT_REFINE_TABLE_CONTEXT_TMPL, prompt_type=PromptType.TABLE_CONTEXT
 )
 
 
 ############################################
 # Knowledge-Graph Table
 ############################################
 
@@ -263,16 +265,16 @@
     "(Philz, is, coffee shop)\n"
     "(Philz, founded in, Berkeley)\n"
     "(Philz, founded in, 1982)\n"
     "---------------------\n"
     "Text: {text}\n"
     "Triplets:\n"
 )
-DEFAULT_KG_TRIPLET_EXTRACT_PROMPT = KnowledgeGraphPrompt(
-    DEFAULT_KG_TRIPLET_EXTRACT_TMPL
+DEFAULT_KG_TRIPLET_EXTRACT_PROMPT = Prompt(
+    DEFAULT_KG_TRIPLET_EXTRACT_TMPL, prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT
 )
 
 ############################################
 # HYDE
 ##############################################
 
 HYDE_TMPL = (
@@ -282,23 +284,25 @@
     "\n"
     "{context_str}\n"
     "\n"
     "\n"
     'Passage:"""\n'
 )
 
-DEFAULT_HYDE_PROMPT = SummaryPrompt(HYDE_TMPL)
+DEFAULT_HYDE_PROMPT = Prompt(HYDE_TMPL, prompt_type=PromptType.SUMMARY)
 
 
 ############################################
 # Simple Input
 ############################################
 
 DEFAULT_SIMPLE_INPUT_TMPL = "{query_str}"
-DEFAULT_SIMPLE_INPUT_PROMPT = SimpleInputPrompt(DEFAULT_SIMPLE_INPUT_TMPL)
+DEFAULT_SIMPLE_INPUT_PROMPT = Prompt(
+    DEFAULT_SIMPLE_INPUT_TMPL, prompt_type=PromptType.SIMPLE_INPUT
+)
 
 
 ############################################
 # Pandas
 ############################################
 
 DEFAULT_PANDAS_TMPL = (
@@ -309,8 +313,26 @@
     "Here is the input query: {query_str}.\n"
     "Given the df information and the input query, please follow "
     "these instructions:\n"
     "{instruction_str}"
     "Output:\n"
 )
 
-DEFAULT_PANDAS_PROMPT = PandasPrompt(DEFAULT_PANDAS_TMPL)
+DEFAULT_PANDAS_PROMPT = Prompt(DEFAULT_PANDAS_TMPL, prompt_type=PromptType.PANDAS)
+
+
+############################################
+# JSON Path
+############################################
+
+DEFAULT_JSON_PATH_TMPL = (
+    "We have provided a JSON schema below:\n"
+    "{schema}\n"
+    "Given a task, respond with a JSON Path query that "
+    "can retrieve data from a JSON value that matches the schema.\n"
+    "Task: {query_str}\n"
+    "JSONPath: "
+)
+
+DEFAULT_JSON_PATH_PROMPT = Prompt(
+    DEFAULT_JSON_PATH_TMPL, prompt_type=PromptType.JSON_PATH
+)
```

### Comparing `llama_index-0.6.9/llama_index/prompts/prompt_type.py` & `llama_index-0.7.0/llama_index/prompts/prompt_type.py`

 * *Files 18% similar despite different names*

```diff
@@ -37,20 +37,35 @@
 
     # Simple Input prompt
     SIMPLE_INPUT = "simple_input"
 
     # Pandas prompt
     PANDAS = "pandas"
 
+    # JSON path prompt
+    JSON_PATH = "json_path"
+
     # Single select prompt
     SINGLE_SELECT = "single_select"
 
     # Multiple select prompt
     MULTI_SELECT = "multi_select"
 
     VECTOR_STORE_QUERY = "vector_store_query"
 
     # Sub question prompt
     SUB_QUESTION = "sub_question"
 
+    # SQL response synthesis prompt
+    SQL_RESPONSE_SYNTHESIS = "sql_response_synthesis"
+
+    # Conversation
+    CONVERSATION = "conversation"
+
+    # Decompose query transform
+    DECOMPOSE = "decompose"
+
+    # Choice select
+    CHOICE_SELECT = "choice_select"
+
     # custom (by default)
     CUSTOM = "custom"
```

### Comparing `llama_index-0.6.9/llama_index/query_engine/__init__.py` & `llama_index-0.7.0/llama_index/query_engine/__init__.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,19 +1,39 @@
+from llama_index.query_engine.citation_query_engine import CitationQueryEngine
+from llama_index.query_engine.flare.base import FLAREInstructQueryEngine
 from llama_index.query_engine.graph_query_engine import ComposableGraphQueryEngine
 from llama_index.query_engine.multistep_query_engine import MultiStepQueryEngine
+from llama_index.query_engine.pandas_query_engine import PandasQueryEngine
 from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine
+from llama_index.query_engine.retry_query_engine import (
+    RetryGuidelineQueryEngine,
+    RetryQueryEngine,
+)
+from llama_index.query_engine.retry_source_query_engine import RetrySourceQueryEngine
 from llama_index.query_engine.router_query_engine import (
+    ToolRetrieverRouterQueryEngine,
     RetrieverRouterQueryEngine,
     RouterQueryEngine,
 )
+from llama_index.query_engine.sql_join_query_engine import SQLJoinQueryEngine
+from llama_index.query_engine.sql_vector_query_engine import SQLAutoVectorQueryEngine
 from llama_index.query_engine.sub_question_query_engine import SubQuestionQueryEngine
 from llama_index.query_engine.transform_query_engine import TransformQueryEngine
 
 __all__ = [
+    "CitationQueryEngine",
     "ComposableGraphQueryEngine",
     "RetrieverQueryEngine",
     "TransformQueryEngine",
     "MultiStepQueryEngine",
     "RouterQueryEngine",
     "RetrieverRouterQueryEngine",
+    "ToolRetrieverRouterQueryEngine",
     "SubQuestionQueryEngine",
+    "SQLJoinQueryEngine",
+    "SQLAutoVectorQueryEngine",
+    "RetryQueryEngine",
+    "RetrySourceQueryEngine",
+    "RetryGuidelineQueryEngine",
+    "FLAREInstructQueryEngine",
+    "PandasQueryEngine",
 ]
```

### Comparing `llama_index-0.6.9/llama_index/query_engine/graph_query_engine.py` & `llama_index-0.7.0/llama_index/query_engine/graph_query_engine.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 from typing import Dict, List, Optional, Tuple
 
-from llama_index.data_structs.node import IndexNode, Node, NodeWithScore
+from llama_index.callbacks.schema import CBEventType, EventPayload
 from llama_index.indices.composability.graph import ComposableGraph
 from llama_index.indices.query.base import BaseQueryEngine
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.response.schema import RESPONSE_TYPE
+from llama_index.schema import TextNode, IndexNode, NodeWithScore
 
 
 class ComposableGraphQueryEngine(BaseQueryEngine):
     """Composable graph query engine.
 
     This query engine can operate over a ComposableGraph.
     It can take in custom query engines for its sub-indices.
@@ -29,14 +30,16 @@
     ) -> None:
         """Init params."""
         self._graph = graph
         self._custom_query_engines = custom_query_engines or {}
 
         # additional configs
         self._recursive = recursive
+        callback_manager = self._graph.service_context.callback_manager
+        super().__init__(callback_manager)
 
     async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:
         return self._query_index(query_bundle, index_id=None, level=0)
 
     def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:
         return self._query_index(query_bundle, index_id=None, level=0)
 
@@ -44,21 +47,31 @@
         self,
         query_bundle: QueryBundle,
         index_id: Optional[str] = None,
         level: int = 0,
     ) -> RESPONSE_TYPE:
         """Query a single index."""
         index_id = index_id or self._graph.root_id
+        event_id = self.callback_manager.on_event_start(
+            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}
+        )
 
         # get query engine
         if index_id in self._custom_query_engines:
             query_engine = self._custom_query_engines[index_id]
         else:
             query_engine = self._graph.get_index(index_id).as_query_engine()
+
+        retrieve_event_id = self.callback_manager.on_event_start(CBEventType.RETRIEVE)
         nodes = query_engine.retrieve(query_bundle)
+        self.callback_manager.on_event_end(
+            CBEventType.RETRIEVE,
+            payload={EventPayload.NODES: nodes},
+            event_id=retrieve_event_id,
+        )
 
         if self._recursive:
             # do recursion here
             nodes_for_synthesis = []
             additional_source_nodes = []
             for node_with_score in nodes:
                 node_with_score, source_nodes = self._fetch_recursive_nodes(
@@ -68,14 +81,19 @@
                 additional_source_nodes.extend(source_nodes)
             response = query_engine.synthesize(
                 query_bundle, nodes_for_synthesis, additional_source_nodes
             )
         else:
             response = query_engine.synthesize(query_bundle, nodes)
 
+        self.callback_manager.on_event_end(
+            CBEventType.QUERY,
+            payload={EventPayload.RESPONSE: response},
+            event_id=event_id,
+        )
         return response
 
     def _fetch_recursive_nodes(
         self,
         node_with_score: NodeWithScore,
         query_bundle: QueryBundle,
         level: int,
@@ -87,14 +105,14 @@
 
         """
         if isinstance(node_with_score.node, IndexNode):
             index_node = node_with_score.node
             # recursive call
             response = self._query_index(query_bundle, index_node.index_id, level + 1)
 
-            new_node = Node(text=str(response))
+            new_node = TextNode(text=str(response))
             new_node_with_score = NodeWithScore(
                 node=new_node, score=node_with_score.score
             )
             return new_node_with_score, response.source_nodes
         else:
             return node_with_score, []
```

### Comparing `llama_index-0.6.9/llama_index/query_engine/multistep_query_engine.py` & `llama_index-0.7.0/llama_index/query_engine/multistep_query_engine.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 from typing import Any, Callable, Dict, List, Optional, Tuple, cast
-from llama_index.data_structs.node import Node, NodeWithScore
+
+from llama_index.callbacks.schema import CBEventType, EventPayload
 from llama_index.indices.query.base import BaseQueryEngine
 from llama_index.indices.query.query_transform.base import StepDecomposeQueryTransform
 from llama_index.indices.query.schema import QueryBundle
-from llama_index.indices.query.response_synthesis import ResponseSynthesizer
 from llama_index.response.schema import RESPONSE_TYPE
+from llama_index.response_synthesizers import BaseSynthesizer, get_response_synthesizer
+from llama_index.schema import NodeWithScore, TextNode
 
 
 def default_stop_fn(stop_dict: Dict) -> bool:
     """Stop function for multi-step query combiner."""
     query_bundle = cast(QueryBundle, stop_dict.get("query_bundle"))
     if query_bundle is None:
         raise ValueError("Response must be provided to stop function.")
@@ -25,93 +27,114 @@
     This query engine can operate over an existing base query engine,
     along with the multi-step query transform.
 
     Args:
         query_engine (BaseQueryEngine): A BaseQueryEngine object.
         query_transform (StepDecomposeQueryTransform): A StepDecomposeQueryTransform
             object.
-        response_synthesizer (Optional[ResponseSynthesizer]): A ResponseSynthesizer
+        response_synthesizer (Optional[BaseSynthesizer]): A BaseSynthesizer
             object.
         num_steps (Optional[int]): Number of steps to run the multi-step query.
         early_stopping (bool): Whether to stop early if the stop function returns True.
         index_summary (str): A string summary of the index.
         stop_fn (Optional[Callable[[Dict], bool]]): A stop function that takes in a
             dictionary of information and returns a boolean.
 
     """
 
     def __init__(
         self,
         query_engine: BaseQueryEngine,
         query_transform: StepDecomposeQueryTransform,
-        response_synthesizer: Optional[ResponseSynthesizer] = None,
+        response_synthesizer: Optional[BaseSynthesizer] = None,
         num_steps: Optional[int] = 3,
         early_stopping: bool = True,
         index_summary: str = "None",
         stop_fn: Optional[Callable[[Dict], bool]] = None,
     ) -> None:
         self._query_engine = query_engine
         self._query_transform = query_transform
-        self._response_synthesizer = (
-            response_synthesizer or ResponseSynthesizer.from_args()
+        self._response_synthesizer = response_synthesizer or get_response_synthesizer(
+            callback_manager=self._query_engine.callback_manager
         )
 
         self._index_summary = index_summary
         self._num_steps = num_steps
         self._early_stopping = early_stopping
         # TODO: make interface to stop function better
         self._stop_fn = stop_fn or default_stop_fn
         # num_steps must be provided if early_stopping is False
         if not self._early_stopping and self._num_steps is None:
             raise ValueError("Must specify num_steps if early_stopping is False.")
 
+        callback_manager = self._query_engine.callback_manager
+        super().__init__(callback_manager)
+
     def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:
-        nodes, source_nodes, extra_info = self._query_multistep(query_bundle)
+        query_event_id = self.callback_manager.on_event_start(
+            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}
+        )
+        nodes, source_nodes, metadata = self._query_multistep(query_bundle)
+
         final_response = self._response_synthesizer.synthesize(
-            query_bundle=query_bundle,
+            query=query_bundle,
             nodes=nodes,
             additional_source_nodes=source_nodes,
         )
-        final_response.extra_info = extra_info
+        final_response.metadata = metadata
+
+        self.callback_manager.on_event_end(
+            CBEventType.QUERY,
+            payload={EventPayload.RESPONSE: final_response},
+            event_id=query_event_id,
+        )
         return final_response
 
     async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:
-        nodes, source_nodes, extra_info = self._query_multistep(query_bundle)
+        event_id = self.callback_manager.on_event_start(
+            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}
+        )
+        nodes, source_nodes, metadata = self._query_multistep(query_bundle)
+
         final_response = await self._response_synthesizer.asynthesize(
-            query_bundle=query_bundle,
+            query=query_bundle,
             nodes=nodes,
             additional_source_nodes=source_nodes,
         )
-        final_response.extra_info = extra_info
+        final_response.metadata = metadata
+
+        self.callback_manager.on_event_end(
+            CBEventType.QUERY,
+            payload={EventPayload.RESPONSE: final_response},
+            event_id=event_id,
+        )
         return final_response
 
     def _combine_queries(
         self, query_bundle: QueryBundle, prev_reasoning: str
     ) -> QueryBundle:
         """Combine queries."""
-        transform_extra_info = {
+        transform_metadata = {
             "prev_reasoning": prev_reasoning,
             "index_summary": self._index_summary,
         }
-        query_bundle = self._query_transform(
-            query_bundle, extra_info=transform_extra_info
-        )
+        query_bundle = self._query_transform(query_bundle, metadata=transform_metadata)
         return query_bundle
 
     def _query_multistep(
         self, query_bundle: QueryBundle
     ) -> Tuple[List[NodeWithScore], List[NodeWithScore], Dict[str, Any]]:
         """Run query combiner."""
         prev_reasoning = ""
         cur_response = None
         should_stop = False
         cur_steps = 0
 
         # use response
-        final_response_extra_info: Dict[str, Any] = {"sub_qa": []}
+        final_response_metadata: Dict[str, Any] = {"sub_qa": []}
 
         text_chunks = []
         source_nodes = []
         while not should_stop:
             if self._num_steps is not None and cur_steps >= self._num_steps:
                 should_stop = True
                 break
@@ -132,19 +155,21 @@
             cur_qa_text = (
                 f"\nQuestion: {updated_query_bundle.query_str}\n"
                 f"Answer: {str(cur_response)}"
             )
             text_chunks.append(cur_qa_text)
             for source_node in cur_response.source_nodes:
                 source_nodes.append(source_node)
-            # update extra info
-            final_response_extra_info["sub_qa"].append(
+            # update metadata
+            final_response_metadata["sub_qa"].append(
                 (updated_query_bundle.query_str, cur_response)
             )
 
             prev_reasoning += (
                 f"- {updated_query_bundle.query_str}\n" f"- {str(cur_response)}\n"
             )
             cur_steps += 1
 
-        nodes = [NodeWithScore(Node(text_chunk)) for text_chunk in text_chunks]
-        return nodes, source_nodes, final_response_extra_info
+        nodes = [
+            NodeWithScore(node=TextNode(text=text_chunk)) for text_chunk in text_chunks
+        ]
+        return nodes, source_nodes, final_response_metadata
```

### Comparing `llama_index-0.6.9/llama_index/query_engine/retriever_query_engine.py` & `llama_index-0.7.0/llama_index/query_engine/retriever_query_engine.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,66 +1,68 @@
-from typing import Any, Dict, List, Optional, Sequence
+from typing import Any, List, Optional, Sequence
 
 from llama_index.callbacks.base import CallbackManager
-from llama_index.callbacks.schema import CBEventType
-from llama_index.data_structs.node import NodeWithScore
+from llama_index.callbacks.schema import CBEventType, EventPayload
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.postprocessor.types import BaseNodePostprocessor
 from llama_index.indices.query.base import BaseQueryEngine
-from llama_index.indices.query.response_synthesis import ResponseSynthesizer
 from llama_index.indices.query.schema import QueryBundle
-from llama_index.indices.response.type import ResponseMode
 from llama_index.indices.service_context import ServiceContext
-from llama_index.optimization.optimizer import BaseTokenUsageOptimizer
+from llama_index.response_synthesizers import (
+    BaseSynthesizer,
+    ResponseMode,
+    get_response_synthesizer,
+)
 from llama_index.prompts.prompts import (
     QuestionAnswerPrompt,
     RefinePrompt,
     SimpleInputPrompt,
 )
 from llama_index.response.schema import RESPONSE_TYPE
+from llama_index.schema import NodeWithScore
 
 
 class RetrieverQueryEngine(BaseQueryEngine):
     """Retriever query engine.
 
     Args:
         retriever (BaseRetriever): A retriever object.
-        response_synthesizer (Optional[ResponseSynthesizer]): A ResponseSynthesizer
+        response_synthesizer (Optional[BaseSynthesizer]): A BaseSynthesizer
             object.
-
+        callback_manager (Optional[CallbackManager]): A callback manager.
     """
 
     def __init__(
         self,
         retriever: BaseRetriever,
-        response_synthesizer: Optional[ResponseSynthesizer] = None,
+        response_synthesizer: Optional[BaseSynthesizer] = None,
+        node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,
         callback_manager: Optional[CallbackManager] = None,
     ) -> None:
         self._retriever = retriever
-        self._response_synthesizer = (
-            response_synthesizer or ResponseSynthesizer.from_args()
+        self._response_synthesizer = response_synthesizer or get_response_synthesizer(
+            callback_manager=callback_manager
         )
-        self.callback_manager = callback_manager or CallbackManager([])
+        self._node_postprocessors = node_postprocessors or []
+        super().__init__(callback_manager)
 
     @classmethod
     def from_args(
         cls,
         retriever: BaseRetriever,
+        response_synthesizer: Optional[BaseSynthesizer] = None,
         service_context: Optional[ServiceContext] = None,
         node_postprocessors: Optional[List[BaseNodePostprocessor]] = None,
-        verbose: bool = False,
         # response synthesizer args
         response_mode: ResponseMode = ResponseMode.COMPACT,
         text_qa_template: Optional[QuestionAnswerPrompt] = None,
         refine_template: Optional[RefinePrompt] = None,
         simple_template: Optional[SimpleInputPrompt] = None,
-        response_kwargs: Optional[Dict] = None,
         use_async: bool = False,
         streaming: bool = False,
-        optimizer: Optional[BaseTokenUsageOptimizer] = None,
         # class-specific args
         **kwargs: Any,
     ) -> "RetrieverQueryEngine":
         """Initialize a RetrieverQueryEngine object."
 
         Args:
             retriever (BaseRetriever): A retriever object.
@@ -69,108 +71,123 @@
                 node postprocessors.
             verbose (bool): Whether to print out debug info.
             response_mode (ResponseMode): A ResponseMode object.
             text_qa_template (Optional[QuestionAnswerPrompt]): A QuestionAnswerPrompt
                 object.
             refine_template (Optional[RefinePrompt]): A RefinePrompt object.
             simple_template (Optional[SimpleInputPrompt]): A SimpleInputPrompt object.
-            response_kwargs (Optional[Dict]): A dict of response kwargs.
+
             use_async (bool): Whether to use async.
             streaming (bool): Whether to use streaming.
             optimizer (Optional[BaseTokenUsageOptimizer]): A BaseTokenUsageOptimizer
                 object.
 
         """
-        response_synthesizer = ResponseSynthesizer.from_args(
+        response_synthesizer = response_synthesizer or get_response_synthesizer(
             service_context=service_context,
             text_qa_template=text_qa_template,
             refine_template=refine_template,
             simple_template=simple_template,
             response_mode=response_mode,
-            response_kwargs=response_kwargs,
             use_async=use_async,
             streaming=streaming,
-            optimizer=optimizer,
-            node_postprocessors=node_postprocessors,
-            verbose=verbose,
         )
 
         callback_manager = (
             service_context.callback_manager if service_context else CallbackManager([])
         )
 
         return cls(
             retriever=retriever,
             response_synthesizer=response_synthesizer,
             callback_manager=callback_manager,
+            node_postprocessors=node_postprocessors,
         )
 
     def retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
-        return self._retriever.retrieve(query_bundle)
+        nodes = self._retriever.retrieve(query_bundle)
+
+        for node_postprocessor in self._node_postprocessors:
+            nodes = node_postprocessor.postprocess_nodes(nodes)
+
+        return nodes
 
     def synthesize(
         self,
         query_bundle: QueryBundle,
         nodes: List[NodeWithScore],
         additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,
     ) -> RESPONSE_TYPE:
         return self._response_synthesizer.synthesize(
-            query_bundle=query_bundle,
+            query=query_bundle,
             nodes=nodes,
             additional_source_nodes=additional_source_nodes,
         )
 
     async def asynthesize(
         self,
         query_bundle: QueryBundle,
         nodes: List[NodeWithScore],
         additional_source_nodes: Optional[Sequence[NodeWithScore]] = None,
     ) -> RESPONSE_TYPE:
         return await self._response_synthesizer.asynthesize(
-            query_bundle=query_bundle,
+            query=query_bundle,
             nodes=nodes,
             additional_source_nodes=additional_source_nodes,
         )
 
     def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:
         """Answer a query."""
-        query_id = self.callback_manager.on_event_start(CBEventType.QUERY)
+        query_id = self.callback_manager.on_event_start(
+            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}
+        )
 
         retrieve_id = self.callback_manager.on_event_start(CBEventType.RETRIEVE)
         nodes = self._retriever.retrieve(query_bundle)
         self.callback_manager.on_event_end(
-            CBEventType.RETRIEVE, payload={"nodes": nodes}, event_id=retrieve_id
+            CBEventType.RETRIEVE,
+            payload={EventPayload.NODES: nodes},
+            event_id=retrieve_id,
         )
 
-        synth_id = self.callback_manager.on_event_start(CBEventType.SYNTHESIZE)
         response = self._response_synthesizer.synthesize(
-            query_bundle=query_bundle,
+            query=query_bundle,
             nodes=nodes,
         )
+
         self.callback_manager.on_event_end(
-            CBEventType.SYNTHESIZE, payload={"response": response}, event_id=synth_id
+            CBEventType.QUERY,
+            payload={EventPayload.RESPONSE: response},
+            event_id=query_id,
         )
-
-        self.callback_manager.on_event_end(CBEventType.QUERY, event_id=query_id)
         return response
 
     async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:
         """Answer a query."""
-        query_id = self.callback_manager.on_event_start(CBEventType.QUERY)
+        query_id = self.callback_manager.on_event_start(
+            CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}
+        )
 
         retrieve_id = self.callback_manager.on_event_start(CBEventType.RETRIEVE)
         nodes = self._retriever.retrieve(query_bundle)
         self.callback_manager.on_event_end(
-            CBEventType.RETRIEVE, payload={"nodes": nodes}, event_id=retrieve_id
+            CBEventType.RETRIEVE,
+            payload={EventPayload.NODES: nodes},
+            event_id=retrieve_id,
         )
 
-        synth_id = self.callback_manager.on_event_start(CBEventType.SYNTHESIZE)
         response = await self._response_synthesizer.asynthesize(
-            query_bundle=query_bundle,
+            query=query_bundle,
             nodes=nodes,
         )
+
         self.callback_manager.on_event_end(
-            CBEventType.SYNTHESIZE, payload={"response": response}, event_id=synth_id
+            CBEventType.QUERY,
+            payload={EventPayload.RESPONSE: response},
+            event_id=query_id,
         )
-
-        self.callback_manager.on_event_end(CBEventType.QUERY, event_id=query_id)
         return response
+
+    @property
+    def retriever(self) -> BaseRetriever:
+        """Get the retriever object."""
+        return self._retriever
```

### Comparing `llama_index-0.6.9/llama_index/query_engine/sub_question_query_engine.py` & `llama_index-0.7.0/llama_index/query_engine/sub_question_query_engine.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 import asyncio
 import logging
 from typing import List, Optional, Sequence, cast
 
-from langchain.input import get_color_mapping, print_text
+from llama_index.bridge.langchain import get_color_mapping, print_text
 
 from llama_index.async_utils import run_async_tasks
-from llama_index.data_structs.node import Node, NodeWithScore
+from llama_index.callbacks.base import CallbackManager
 from llama_index.indices.query.base import BaseQueryEngine
-from llama_index.indices.query.response_synthesis import ResponseSynthesizer
 from llama_index.indices.query.schema import QueryBundle
+from llama_index.indices.service_context import ServiceContext
 from llama_index.question_gen.llm_generators import LLMQuestionGenerator
 from llama_index.question_gen.types import BaseQuestionGenerator, SubQuestion
 from llama_index.response.schema import RESPONSE_TYPE
+from llama_index.response_synthesizers import BaseSynthesizer, get_response_synthesizer
+from llama_index.schema import NodeWithScore, TextNode
 from llama_index.tools.query_engine import QueryEngineTool
 
 logger = logging.getLogger(__name__)
 
 
 class SubQuestionQueryEngine(BaseQueryEngine):
     """Sub question query engine.
@@ -24,103 +26,131 @@
         many sub questions and their target query engine for execution.
         After executing all sub questions, all responses are gathered and sent to
         response synthesizer to produce the final response.
 
     Args:
         question_gen (BaseQuestionGenerator): A module for generating sub questions
             given a complex question and tools.
-        response_synthesizer (ResponseSynthesizer): A response synthesizer for
+        response_synthesizer (BaseSynthesizer): A response synthesizer for
             generating the final response
         query_engine_tools (Sequence[QueryEngineTool]): Tools to answer the
             sub questions.
         verbose (bool): whether to print intermediate questions and answers.
             Defaults to True
         use_async (bool): whether to execute the sub questions with asyncio.
             Defaults to True
     """
 
     def __init__(
         self,
         question_gen: BaseQuestionGenerator,
-        response_synthesizer: ResponseSynthesizer,
+        response_synthesizer: BaseSynthesizer,
         query_engine_tools: Sequence[QueryEngineTool],
+        callback_manager: Optional[CallbackManager] = None,
         verbose: bool = True,
-        use_async: bool = True,
+        use_async: bool = False,
     ) -> None:
         self._question_gen = question_gen
         self._response_synthesizer = response_synthesizer
         self._metadatas = [x.metadata for x in query_engine_tools]
         self._query_engines = {
             tool.metadata.name: tool.query_engine for tool in query_engine_tools
         }
         self._verbose = verbose
         self._use_async = use_async
+        super().__init__(callback_manager)
 
     @classmethod
     def from_defaults(
         cls,
         query_engine_tools: Sequence[QueryEngineTool],
         question_gen: Optional[BaseQuestionGenerator] = None,
-        response_synthesizer: Optional[ResponseSynthesizer] = None,
+        response_synthesizer: Optional[BaseSynthesizer] = None,
+        service_context: Optional[ServiceContext] = None,
         verbose: bool = True,
         use_async: bool = True,
     ) -> "SubQuestionQueryEngine":
-        question_gen = question_gen or LLMQuestionGenerator.from_defaults()
-        synth = response_synthesizer or ResponseSynthesizer.from_args()
+        callback_manager = None
+        if len(query_engine_tools) > 0:
+            callback_manager = query_engine_tools[0].query_engine.callback_manager
+
+        question_gen = question_gen or LLMQuestionGenerator.from_defaults(
+            service_context=service_context
+        )
+        synth = response_synthesizer or get_response_synthesizer(
+            callback_manager=callback_manager,
+            service_context=service_context,
+            use_async=use_async,
+        )
 
-        return cls(question_gen, synth, query_engine_tools, verbose, use_async)
+        return cls(
+            question_gen,
+            synth,
+            query_engine_tools,
+            callback_manager=callback_manager,
+            verbose=verbose,
+            use_async=use_async,
+        )
 
     def _query(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:
         sub_questions = self._question_gen.generate(self._metadatas, query_bundle)
 
+        colors = get_color_mapping([str(i) for i in range(len(sub_questions))])
+
         if self._verbose:
             print_text(f"Generated {len(sub_questions)} sub questions.\n")
-            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])
-
-        tasks = [
-            self.aquery_subq(sub_q, color=colors[str(ind)])
-            for ind, sub_q in enumerate(sub_questions)
-        ]
 
-        nodes_all = run_async_tasks(tasks)
-        nodes_all = cast(List[Optional[NodeWithScore]], nodes_all)
+        if self._use_async:
+            tasks = [
+                self._aquery_subq(sub_q, color=colors[str(ind)])
+                for ind, sub_q in enumerate(sub_questions)
+            ]
+
+            nodes_all = run_async_tasks(tasks)
+            nodes_all = cast(List[Optional[NodeWithScore]], nodes_all)
+        else:
+            nodes_all = [
+                self._query_subq(sub_q, color=colors[str(ind)])
+                for ind, sub_q in enumerate(sub_questions)
+            ]
 
         # filter out sub questions that failed
         nodes: List[NodeWithScore] = list(filter(None, nodes_all))
 
         return self._response_synthesizer.synthesize(
-            query_bundle=query_bundle,
+            query=query_bundle,
             nodes=nodes,
         )
 
     async def _aquery(self, query_bundle: QueryBundle) -> RESPONSE_TYPE:
         sub_questions = await self._question_gen.agenerate(
             self._metadatas, query_bundle
         )
 
+        colors = get_color_mapping([str(i) for i in range(len(sub_questions))])
+
         if self._verbose:
             print_text(f"Generated {len(sub_questions)} sub questions.\n")
-            colors = get_color_mapping([str(i) for i in range(len(sub_questions))])
 
         tasks = [
-            self.aquery_subq(sub_q, color=colors[str(ind)])
+            self._aquery_subq(sub_q, color=colors[str(ind)])
             for ind, sub_q in enumerate(sub_questions)
         ]
         nodes_all = await asyncio.gather(*tasks)
         nodes_all = cast(List[Optional[NodeWithScore]], nodes_all)
 
         # filter out sub questions that failed
-        nodes = list(filter(None, nodes_all))
+        nodes: List[NodeWithScore] = list(filter(None, nodes_all))
 
         return await self._response_synthesizer.asynthesize(
-            query_bundle=query_bundle,
+            query=query_bundle,
             nodes=nodes,
         )
 
-    async def aquery_subq(
+    async def _aquery_subq(
         self, sub_q: SubQuestion, color: Optional[str] = None
     ) -> Optional[NodeWithScore]:
         try:
             question = sub_q.sub_question
             query_engine = self._query_engines[sub_q.tool_name]
 
             if self._verbose:
@@ -129,11 +159,33 @@
             response = await query_engine.aquery(question)
             response_text = str(response)
             node_text = f"Sub question: {question}\nResponse: {response_text}"
 
             if self._verbose:
                 print_text(f"[{sub_q.tool_name}] A: {response_text}\n", color=color)
 
-            return NodeWithScore(Node(text=node_text))
+            return NodeWithScore(node=TextNode(text=node_text))
+        except ValueError:
+            logger.warn(f"[{sub_q.tool_name}] Failed to run {question}")
+            return None
+
+    def _query_subq(
+        self, sub_q: SubQuestion, color: Optional[str] = None
+    ) -> Optional[NodeWithScore]:
+        try:
+            question = sub_q.sub_question
+            query_engine = self._query_engines[sub_q.tool_name]
+
+            if self._verbose:
+                print_text(f"[{sub_q.tool_name}] Q: {question}\n", color=color)
+
+            response = query_engine.query(question)
+            response_text = str(response)
+            node_text = f"Sub question: {question}\nResponse: {response_text}"
+
+            if self._verbose:
+                print_text(f"[{sub_q.tool_name}] A: {response_text}\n", color=color)
+
+            return NodeWithScore(node=TextNode(text=node_text))
         except ValueError:
             logger.warn(f"[{sub_q.tool_name}] Failed to run {question}")
             return None
```

### Comparing `llama_index-0.6.9/llama_index/question_gen/llm_generators.py` & `llama_index-0.7.0/llama_index/question_gen/llm_generators.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,28 +1,30 @@
 from typing import List, Optional, Sequence, cast
 
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
-from llama_index.llm_predictor.base import LLMPredictor
-from llama_index.output_parsers.base import BaseOutputParser, StructuredOutput
+from llama_index.llm_predictor.base import BaseLLMPredictor
+from llama_index.output_parsers.base import StructuredOutput
+from llama_index.types import BaseOutputParser
+from llama_index.prompts.base import Prompt
+from llama_index.prompts.prompt_type import PromptType
 from llama_index.question_gen.output_parser import SubQuestionOutputParser
 from llama_index.question_gen.prompts import (
     DEFAULT_SUB_QUESTION_PROMPT_TMPL,
-    SubQuestionPrompt,
     build_tools_text,
 )
 from llama_index.question_gen.types import BaseQuestionGenerator, SubQuestion
 from llama_index.tools.types import ToolMetadata
 
 
 class LLMQuestionGenerator(BaseQuestionGenerator):
     def __init__(
         self,
-        llm_predictor: LLMPredictor,
-        prompt: SubQuestionPrompt,
+        llm_predictor: BaseLLMPredictor,
+        prompt: Prompt,
     ) -> None:
         self._llm_predictor = llm_predictor
         self._prompt = prompt
 
         if self._prompt.output_parser is None:
             raise ValueError("Prompt should have output parser.")
 
@@ -35,25 +37,27 @@
     ) -> "LLMQuestionGenerator":
         # optionally initialize defaults
         service_context = service_context or ServiceContext.from_defaults()
         prompt_template_str = prompt_template_str or DEFAULT_SUB_QUESTION_PROMPT_TMPL
         output_parser = output_parser or SubQuestionOutputParser()
 
         # construct prompt
-        prompt = SubQuestionPrompt(
-            template=prompt_template_str, output_parser=output_parser
+        prompt = Prompt(
+            template=prompt_template_str,
+            output_parser=output_parser,
+            prompt_type=PromptType.SUB_QUESTION,
         )
         return cls(service_context.llm_predictor, prompt)
 
     def generate(
         self, tools: Sequence[ToolMetadata], query: QueryBundle
     ) -> List[SubQuestion]:
         tools_str = build_tools_text(tools)
         query_str = query.query_str
-        prediction, _ = self._llm_predictor.predict(
+        prediction = self._llm_predictor.predict(
             prompt=self._prompt,
             tools_str=tools_str,
             query_str=query_str,
         )
 
         assert self._prompt.output_parser is not None
         parse = self._prompt.output_parser.parse(prediction)
@@ -61,15 +65,15 @@
         return parse.parsed_output
 
     async def agenerate(
         self, tools: Sequence[ToolMetadata], query: QueryBundle
     ) -> List[SubQuestion]:
         tools_str = build_tools_text(tools)
         query_str = query.query_str
-        prediction, _ = await self._llm_predictor.apredict(
+        prediction = await self._llm_predictor.apredict(
             prompt=self._prompt,
             tools_str=tools_str,
             query_str=query_str,
         )
 
         assert self._prompt.output_parser is not None
         parse = self._prompt.output_parser.parse(prediction)
```

### Comparing `llama_index-0.6.9/llama_index/question_gen/output_parser.py` & `llama_index-0.7.0/llama_index/question_gen/output_parser.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from typing import Any
 
-from llama_index.output_parsers.base import BaseOutputParser, StructuredOutput
+from llama_index.output_parsers.base import StructuredOutput
+from llama_index.types import BaseOutputParser
 from llama_index.output_parsers.utils import parse_json_markdown
 from llama_index.question_gen.types import SubQuestion
 
 
 class SubQuestionOutputParser(BaseOutputParser):
     def parse(self, output: str) -> Any:
         json_dict = parse_json_markdown(output)
```

### Comparing `llama_index-0.6.9/llama_index/question_gen/prompts.py` & `llama_index-0.7.0/llama_index/question_gen/prompts.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,15 +1,17 @@
 import json
-from typing import List, Sequence
+from typing import Sequence
 
 from llama_index.prompts.base import Prompt
-from llama_index.prompts.prompt_type import PromptType
 from llama_index.question_gen.types import SubQuestion
 from llama_index.tools.types import ToolMetadata
 
+# deprecated, kept for backward compatibility
+SubQuestionPrompt = Prompt
+
 
 def build_tools_text(tools: Sequence[ToolMetadata]) -> str:
     tools_dict = {}
     for tool in tools:
         tools_dict[tool.name] = tool.description
     tools_str = json.dumps(tools_dict, indent=4)
     return tools_str
@@ -84,12 +86,7 @@
 <User Question>
 {query_str}
 
 <Output>
 """
 
 DEFAULT_SUB_QUESTION_PROMPT_TMPL = PREFIX + EXAMPLES + SUFFIX
-
-
-class SubQuestionPrompt(Prompt):
-    prompt_type: PromptType = PromptType.SUB_QUESTION
-    input_variables: List[str] = ["tools_str", "query_str"]
```

### Comparing `llama_index-0.6.9/llama_index/question_gen/types.py` & `llama_index-0.7.0/llama_index/question_gen/types.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/readers/__init__.py` & `llama_index-0.7.0/llama_index/readers/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,16 +29,17 @@
 from llama_index.readers.milvus import MilvusReader
 from llama_index.readers.mongo import SimpleMongoReader
 from llama_index.readers.metal import MetalReader
 from llama_index.readers.myscale import MyScaleReader
 from llama_index.readers.notion import NotionPageReader
 from llama_index.readers.obsidian import ObsidianReader
 from llama_index.readers.pinecone import PineconeReader
+from llama_index.readers.psychic import PsychicReader
 from llama_index.readers.qdrant import QdrantReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 from llama_index.readers.slack import SlackReader
 from llama_index.readers.steamship.file_reader import SteamshipFileReader
 from llama_index.readers.string_iterable import StringIterableReader
 from llama_index.readers.twitter import TwitterTweetReader
 from llama_index.readers.weaviate.reader import WeaviateReader
 from llama_index.readers.web import (
     BeautifulSoupWebReader,
@@ -58,14 +59,15 @@
     "NotionPageReader",
     "GoogleDocsReader",
     "MetalReader",
     "DiscordReader",
     "SlackReader",
     "WeaviateReader",
     "PineconeReader",
+    "PsychicReader",
     "QdrantReader",
     "MilvusReader",
     "ChromaReader",
     "DeepLakeReader",
     "FaissReader",
     "MyScaleReader",
     "Document",
```

### Comparing `llama_index-0.6.9/llama_index/readers/base.py` & `llama_index-0.7.0/llama_index/readers/base.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """Base reader class."""
 from abc import abstractmethod
 from typing import Any, List
 
-from langchain.docstore.document import Document as LCDocument
+from llama_index.bridge.langchain import Document as LCDocument
 
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class BaseReader:
     """Utilities for loading data from a directory."""
 
     @abstractmethod
     def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:
```

### Comparing `llama_index-0.6.9/llama_index/readers/chatgpt_plugin/base.py` & `llama_index-0.7.0/llama_index/readers/chatgpt_plugin/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 import os
 from typing import Any, List, Optional
 
 import requests
 from requests.adapters import HTTPAdapter, Retry
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class ChatGPTRetrievalPluginReader(BaseReader):
     """ChatGPT Retrieval Plugin reader."""
 
     def __init__(
         self,
@@ -46,21 +46,21 @@
         for query_result in res.json()["results"]:
             for result in query_result["results"]:
                 result_id = result["id"]
                 result_txt = result["text"]
                 result_embedding = result["embedding"]
                 document = Document(
                     text=result_txt,
-                    doc_id=result_id,
+                    id_=result_id,
                     embedding=result_embedding,
                 )
                 documents.append(document)
 
             # NOTE: there should only be one query
             break
 
         if not separate_documents:
-            text_list = [doc.get_text() for doc in documents]
+            text_list = [doc.get_content() for doc in documents]
             text = "\n\n".join(text_list)
             documents = [Document(text=text)]
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/chroma.py` & `llama_index-0.7.0/llama_index/readers/chroma.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Chroma Reader."""
 
 from typing import Any, List, Optional, Union
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class ChromaReader(BaseReader):
     """Chroma reader.
 
     Retrieve documents from existing persisted Chroma collections.
 
@@ -17,14 +17,16 @@
 
     """
 
     def __init__(
         self,
         collection_name: str,
         persist_directory: Optional[str] = None,
+        chroma_api_impl: str = "rest",
+        chroma_db_impl: Optional[str] = None,
         host: str = "localhost",
         port: int = 8000,
     ) -> None:
         """Initialize with parameters."""
         import_err_msg = (
             "`chromadb` package not found, please run `pip install chromadb`"
         )
@@ -35,15 +37,16 @@
 
         if collection_name is None:
             raise ValueError("Please provide a collection name.")
         from chromadb.config import Settings
 
         self._client = chromadb.Client(
             Settings(
-                chroma_api_impl="rest",
+                chroma_api_impl=chroma_api_impl,
+                chroma_db_impl=chroma_db_impl or "chromadb.db.duckdb.DuckDB",
                 chroma_server_host=host,
                 chroma_server_http_port=port,
                 persist_directory=persist_directory
                 if persist_directory
                 else "./chroma",
             )
         )
@@ -63,18 +66,18 @@
         for result in zip(
             results["ids"],
             results["documents"],
             results["embeddings"],
             results["metadatas"],
         ):
             document = Document(
-                doc_id=result[0][0],
+                id_=result[0][0],
                 text=result[1][0],
                 embedding=result[2][0],
-                extra_info=result[3][0],
+                metadata=result[3][0],
             )
             documents.append(document)
 
         return documents
 
     def load_data(
         self,
```

### Comparing `llama_index-0.6.9/llama_index/readers/database.py` & `llama_index-0.7.0/llama_index/readers/database.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 from typing import Any, List, Optional
 
 from sqlalchemy import text
 from sqlalchemy.engine import Engine
 
 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class DatabaseReader(BaseReader):
     """Simple Database reader.
 
     Concatenates each row into Document used by LlamaIndex.
 
@@ -89,9 +89,9 @@
                 raise ValueError("A query parameter is necessary to filter the data")
             else:
                 result = connection.execute(text(query))
 
             for item in result.fetchall():
                 # fetch each item
                 doc_str = ", ".join([str(entry) for entry in item])
-                documents.append(Document(doc_str))
+                documents.append(Document(text=doc_str))
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/deeplake.py` & `llama_index-0.7.0/llama_index/readers/deeplake.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """DeepLake reader."""
 from typing import List, Optional, Union
 
 import numpy as np
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 distance_metric_map = {
     "l2": lambda a, b: np.linalg.norm(a - b, axis=1, ord=2),
     "l1": lambda a, b: np.linalg.norm(a - b, axis=1, ord=1),
     "max": lambda a, b: np.linalg.norm(a - b, axis=1, ord=np.inf),
     "cos": lambda a, b: np.dot(a, b.T)
     / (np.linalg.norm(a) * np.linalg.norm(b, axis=1)),
@@ -101,14 +101,14 @@
         indices = vector_search(
             query_vector, embeddings, distance_metric=distance_metric, limit=limit
         )
 
         documents = []
         for idx in indices:
             document = Document(
-                doc_id=dataset[idx].ids.numpy().tolist()[0],
                 text=str(dataset[idx].text.numpy().tolist()[0]),
+                id_=dataset[idx].ids.numpy().tolist()[0],
             )
 
             documents.append(document)
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/discord_reader.py` & `llama_index-0.7.0/llama_index/readers/discord_reader.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 import asyncio
 import logging
 import os
 from typing import List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 logger = logging.getLogger(__name__)
 
 
 async def read_channel(
     discord_token: str, channel_id: int, limit: Optional[int], oldest_first: bool
 ) -> str:
@@ -136,15 +136,15 @@
                     f"Channel id {channel_id} must be an integer, "
                     f"not {type(channel_id)}."
                 )
             channel_content = self._read_channel(
                 channel_id, limit=limit, oldest_first=oldest_first
             )
             results.append(
-                Document(channel_content, extra_info={"channel": channel_id})
+                Document(text=channel_content, metadata={"channel": channel_id})
             )
         return results
 
 
 if __name__ == "__main__":
     reader = DiscordReader()
     logger.info("initialized reader")
```

### Comparing `llama_index-0.6.9/llama_index/readers/download.py` & `llama_index-0.7.0/llama_index/readers/download.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,13 @@
-"""Download loader from the Loader Hub."""
+"""Download loader from the Loader Hub.
+
+NOTE: using `download_loader` is now deprecated.
+Please do `pip install llama-hub` instead.
+
+"""
 
 import json
 import os
 import subprocess
 import sys
 from importlib import util
 from pathlib import Path
@@ -10,16 +15,16 @@
 
 import pkg_resources
 import requests
 from pkg_resources import DistributionNotFound
 
 from llama_index.readers.base import BaseReader
 
-LLAMA_HUB_CONTENTS_URL = "https://raw.githubusercontent.com/emptycrown/loader-hub/main"
-LOADER_HUB_PATH = "/loader_hub"
+LLAMA_HUB_CONTENTS_URL = "https://raw.githubusercontent.com/emptycrown/llama-hub/main"
+LOADER_HUB_PATH = "/llama_hub"
 LOADER_HUB_URL = LLAMA_HUB_CONTENTS_URL + LOADER_HUB_PATH
 
 
 def _get_file_content(loader_hub_url: str, path: str) -> Tuple[str, int]:
     """Get the content of a file from the GitHub REST API."""
     resp = requests.get(loader_hub_url + path)
     return resp.text, resp.status_code
```

### Comparing `llama_index-0.6.9/llama_index/readers/elasticsearch.py` & `llama_index-0.7.0/llama_index/readers/elasticsearch.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 
 """
 
 
 from typing import List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class ElasticsearchReader(BaseReader):
     """
     Read documents from an Elasticsearch/Opensearch index.
 
     These documents can then be used in a downstream Llama Index data structure.
@@ -60,10 +60,10 @@
         """
         res = self._client.post(f"{self._index}/_search", json=query).json()
         documents = []
         for hit in res["hits"]["hits"]:
             value = hit["_source"][field]
             embedding = hit["_source"].get(embedding_field or "", None)
             documents.append(
-                Document(text=value, extra_info=hit["_source"], embedding=embedding)
+                Document(text=value, metadata=hit["_source"], embedding=embedding)
             )
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/faiss.py` & `llama_index-0.7.0/llama_index/readers/faiss.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 """Faiss reader."""
 
 from typing import Any, Dict, List
 
 import numpy as np
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class FaissReader(BaseReader):
     """Faiss reader.
 
     Retrieves documents through an existing in-memory Faiss index.
     These documents can then be used in a downstream LlamaIndex data structure.
     If you wish use Faiss itself as an index to to organize documents,
-    insert documents, and perform queries on them, please use GPTVectorStoreIndex
+    insert documents, and perform queries on them, please use VectorStoreIndex
     with FaissVectorStore.
 
     Args:
         faiss_index (faiss.Index): A Faiss Index object (required)
 
     """
 
@@ -65,12 +65,12 @@
                         f"Document ID {doc_id} not found in id_to_text_map."
                     )
                 text = id_to_text_map[doc_id]
                 documents.append(Document(text=text))
 
         if not separate_documents:
             # join all documents into one
-            text_list = [doc.get_text() for doc in documents]
+            text_list = [doc.get_content() for doc in documents]
             text = "\n\n".join(text_list)
             documents = [Document(text=text)]
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/base.py` & `llama_index-0.7.0/llama_index/readers/file/base.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 from llama_index.readers.file.image_reader import ImageReader
 from llama_index.readers.file.ipynb_reader import IPYNBReader
 from llama_index.readers.file.markdown_reader import MarkdownReader
 from llama_index.readers.file.mbox_reader import MboxReader
 from llama_index.readers.file.slides_reader import PptxReader
 from llama_index.readers.file.tabular_reader import PandasCSVReader
 from llama_index.readers.file.video_audio_reader import VideoAudioReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 DEFAULT_FILE_READER_CLS: Dict[str, Type[BaseReader]] = {
     ".pdf": PDFReader,
     ".docx": DocxReader,
     ".pptx": PptxReader,
     ".jpg": ImageReader,
     ".png": ImageReader,
@@ -66,14 +66,15 @@
         self,
         input_dir: Optional[str] = None,
         input_files: Optional[List] = None,
         exclude: Optional[List] = None,
         exclude_hidden: bool = True,
         errors: str = "ignore",
         recursive: bool = False,
+        filename_as_id: bool = False,
         required_exts: Optional[List[str]] = None,
         file_extractor: Optional[Dict[str, BaseReader]] = None,
         num_files_limit: Optional[int] = None,
         file_metadata: Optional[Callable[[str], Dict]] = None,
     ) -> None:
         """Initialize with parameters."""
         super().__init__()
@@ -102,14 +103,15 @@
         if file_extractor is not None:
             self.file_extractor = file_extractor
         else:
             self.file_extractor = {}
 
         self.supported_suffix = list(DEFAULT_FILE_READER_CLS.keys())
         self.file_metadata = file_metadata
+        self.filename_as_id = filename_as_id
 
     def _add_files(self, input_dir: Path) -> List[Path]:
         """Add files."""
         all_files = set()
         rejected_files = set()
 
         if self.exclude is not None:
@@ -175,25 +177,38 @@
         """
         documents = []
         for input_file in self.input_files:
             metadata: Optional[dict] = None
             if self.file_metadata is not None:
                 metadata = self.file_metadata(str(input_file))
 
-            if input_file.suffix.lower() in self.supported_suffix:
+            file_suffix = input_file.suffix.lower()
+            if (
+                file_suffix in self.supported_suffix
+                or file_suffix in self.file_extractor
+            ):
                 # use file readers
-                if input_file.suffix not in self.file_extractor:
+                if file_suffix not in self.file_extractor:
                     # instantiate file reader if not already
-                    reader_cls = DEFAULT_FILE_READER_CLS[input_file.suffix]
-                    self.file_extractor[input_file.suffix] = reader_cls()
-                reader = self.file_extractor[input_file.suffix]
+                    reader_cls = DEFAULT_FILE_READER_CLS[file_suffix]
+                    self.file_extractor[file_suffix] = reader_cls()
+                reader = self.file_extractor[file_suffix]
                 docs = reader.load_data(input_file, extra_info=metadata)
+
+                # iterate over docs if needed
+                if self.filename_as_id:
+                    for i, doc in enumerate(docs):
+                        doc.id_ = f"{str(input_file)}_part_{i}"
+
                 documents.extend(docs)
             else:
                 # do standard read
                 with open(input_file, "r", errors=self.errors, encoding="utf8") as f:
                     data = f.read()
 
-                doc = Document(data, extra_info=metadata)
+                doc = Document(text=data, metadata=metadata or {})
+                if self.filename_as_id:
+                    doc.id_ = str(input_file)
+
                 documents.append(doc)
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/docs_reader.py` & `llama_index-0.7.0/llama_index/readers/file/docs_reader.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 Contains parsers for docx, pdf files.
 
 """
 from pathlib import Path
 from typing import Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class PDFReader(BaseReader):
     """PDF parser."""
 
     def load_data(
         self, file: Path, extra_info: Optional[Dict] = None
@@ -33,19 +33,19 @@
             # Iterate over every page
             docs = []
             for page in range(num_pages):
                 # Extract the text from the page
                 page_text = pdf.pages[page].extract_text()
                 page_label = pdf.page_labels[page]
 
-                metadata = {"page_label": page_label}
+                metadata = {"page_label": page_label, "file_name": file.name}
                 if extra_info is not None:
                     metadata.update(extra_info)
 
-                docs.append(Document(page_text, extra_info=metadata))
+                docs.append(Document(text=page_text, metadata=metadata))
             return docs
 
 
 class DocxReader(BaseReader):
     """Docx parser."""
 
     def load_data(
@@ -57,9 +57,12 @@
         except ImportError:
             raise ImportError(
                 "docx2txt is required to read Microsoft Word files: "
                 "`pip install docx2txt`"
             )
 
         text = docx2txt.process(file)
+        metadata = {"file_name": file.name}
+        if extra_info is not None:
+            metadata.update(extra_info)
 
-        return [Document(text, extra_info=extra_info)]
+        return [Document(text=text, metadata=metadata or {})]
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/epub_reader.py` & `llama_index-0.7.0/llama_index/readers/file/epub_reader.py`

 * *Files 12% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 Contains parsers for epub files.
 """
 
 from pathlib import Path
 from typing import Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class EpubReader(BaseReader):
     """Epub Parser."""
 
     def load_data(
         self, file: Path, extra_info: Optional[Dict] = None
@@ -36,8 +36,8 @@
             # Chapters are typically located in epub documents items.
             if item.get_type() == ebooklib.ITEM_DOCUMENT:
                 text_list.append(
                     html2text.html2text(item.get_content().decode("utf-8"))
                 )
 
         text = "\n".join(text_list)
-        return [Document(text, extra_info=extra_info)]
+        return [Document(text=text, metadata=extra_info or {})]
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/image_caption_reader.py` & `llama_index-0.7.0/llama_index/readers/file/image_caption_reader.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from pathlib import Path
 from typing import Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document, ImageDocument
+from llama_index.schema import Document, ImageDocument
 
 
 class ImageCaptionReader(BaseReader):
     """Image parser.
 
     Caption image using Blip.
 
@@ -87,10 +87,10 @@
         out = model.generate(**inputs)
         text_str = processor.decode(out[0], skip_special_tokens=True)
 
         return [
             ImageDocument(
                 text=text_str,
                 image=image_str,
-                extra_info=extra_info,
+                metadata=extra_info or {},
             )
         ]
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/image_reader.py` & `llama_index-0.7.0/llama_index/readers/file/image_reader.py`

 * *Files 4% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 """
 
 import re
 from pathlib import Path
 from typing import Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document, ImageDocument
+from llama_index.schema import Document, ImageDocument
 
 
 class ImageReader(BaseReader):
     """Image parser.
 
     Extract text from images using DONUT.
 
@@ -105,8 +105,10 @@
             sequence = processor.batch_decode(outputs.sequences)[0]
             sequence = sequence.replace(processor.tokenizer.eos_token, "").replace(
                 processor.tokenizer.pad_token, ""
             )
             # remove first task start token
             text_str = re.sub(r"<.*?>", "", sequence, count=1).strip()
 
-        return [ImageDocument(text=text_str, image=image_str, extra_info=extra_info)]
+        return [
+            ImageDocument(text=text_str, image=image_str, metadata=extra_info or {})
+        ]
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/image_vision_llm_reader.py` & `llama_index-0.7.0/llama_index/readers/file/image_vision_llm_reader.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from pathlib import Path
 from typing import Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document, ImageDocument
+from llama_index.schema import Document, ImageDocument
 
 
 class ImageVisionLLMReader(BaseReader):
     """Image parser.
 
     Caption image using Blip2 (a multimodal VisionLLM similar to GPT4).
 
@@ -49,15 +49,14 @@
         self._parser_config = parser_config
         self._keep_image = keep_image
         self._prompt = prompt
 
     def load_data(
         self, file: Path, extra_info: Optional[Dict] = None
     ) -> List[Document]:
-
         """Parse file."""
         from PIL import Image
 
         from llama_index.img_utils import img_2_b64
 
         # load document image
         image = Image.open(file)
@@ -84,10 +83,10 @@
         out = model.generate(**inputs)
         text_str = processor.decode(out[0], skip_special_tokens=True)
 
         return [
             ImageDocument(
                 text=text_str,
                 image=image_str,
-                extra_info=extra_info,
+                metadata=extra_info or {},
             )
         ]
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/ipynb_reader.py` & `llama_index-0.7.0/llama_index/readers/file/ipynb_reader.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import re
 from pathlib import Path
 from typing import Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class IPYNBReader(BaseReader):
     """Image parser."""
 
     def __init__(
         self,
@@ -31,11 +31,11 @@
         string = nbconvert.exporters.ScriptExporter().from_file(file)[0]
         # split each In[] cell into a separate string
         splits = re.split(r"In\[\d+\]:", string)
         # remove the first element, which is empty
         splits.pop(0)
 
         if self._concatenate:
-            docs = [Document(text="\n\n".join(splits), extra_info=extra_info)]
+            docs = [Document(text="\n\n".join(splits), metadata=extra_info or {})]
         else:
-            docs = [Document(text=s, extra_info=extra_info) for s in splits]
+            docs = [Document(text=s, metadata=extra_info or {}) for s in splits]
         return docs
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/markdown_reader.py` & `llama_index-0.7.0/llama_index/readers/file/markdown_reader.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 
 """
 import re
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple, cast
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class MarkdownReader(BaseReader):
     """Markdown parser.
 
     Extract text from markdown files.
     Returns dictionary with keys as headers and values as the text between headers.
@@ -61,15 +61,15 @@
             # pass linting, assert keys are defined
             markdown_tups = [
                 (re.sub(r"#", "", cast(str, key)).strip(), re.sub(r"<.*?>", "", value))
                 for key, value in markdown_tups
             ]
         else:
             markdown_tups = [
-                (key, re.sub("\n", "", value)) for key, value in markdown_tups
+                (key, re.sub("<.*?>", "", value)) for key, value in markdown_tups
             ]
 
         return markdown_tups
 
     def remove_images(self, content: str) -> str:
         """Get a dictionary of a markdown file from its path."""
         pattern = r"!{1}\[\[(.*)\]\]"
@@ -104,13 +104,13 @@
     ) -> List[Document]:
         """Parse file into string."""
         tups = self.parse_tups(file)
         results = []
         # TODO: don't include headers right now
         for header, value in tups:
             if header is None:
-                results.append(Document(value, extra_info=extra_info))
+                results.append(Document(text=value, metadata=extra_info or {}))
             else:
                 results.append(
-                    Document(f"\n\n{header}\n{value}", extra_info=extra_info)
+                    Document(text=f"\n\n{header}\n{value}", metadata=extra_info or {})
                 )
         return results
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/mbox_reader.py` & `llama_index-0.7.0/llama_index/readers/file/mbox_reader.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,17 +1,21 @@
 """Mbox parser.
 
 Contains simple parser for mbox files.
 
 """
+import logging
+
 from pathlib import Path
 from typing import Any, Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
+
+logger = logging.getLogger(__name__)
 
 
 class MboxReader(BaseReader):
     """Mbox parser.
 
     Extract messages from mailbox files.
     Returns string including date, subject, sender, receiver and
@@ -28,15 +32,15 @@
     )
 
     def __init__(
         self,
         *args: Any,
         max_count: int = 0,
         message_format: str = DEFAULT_MESSAGE_FORMAT,
-        **kwargs: Any
+        **kwargs: Any,
     ) -> None:
         """Init params."""
         try:
             from bs4 import BeautifulSoup  # noqa: F401
         except ImportError:
             raise ImportError(
                 "`beautifulsoup4` package not found: `pip install beautifulsoup4`"
@@ -61,39 +65,43 @@
         results: List[str] = []
         # Load file using mailbox
         bytes_parser = BytesParser(policy=default).parse
         mbox = mailbox.mbox(file, factory=bytes_parser)  # type: ignore
 
         # Iterate through all messages
         for _, _msg in enumerate(mbox):
-            msg: mailbox.mboxMessage = _msg
-            # Parse multipart messages
-            if msg.is_multipart():
-                for part in msg.walk():
-                    ctype = part.get_content_type()
-                    cdispo = str(part.get("Content-Disposition"))
-                    if ctype == "text/plain" and "attachment" not in cdispo:
-                        content = part.get_payload(decode=True)  # decode
-                        break
-            # Get plain message payload for non-multipart messages
-            else:
-                content = msg.get_payload(decode=True)
-
-            # Parse message HTML content and remove unneeded whitespace
-            soup = BeautifulSoup(content)
-            stripped_content = " ".join(soup.get_text().split())
-            # Format message to include date, sender, receiver and subject
-            msg_string = self.message_format.format(
-                _date=msg["date"],
-                _from=msg["from"],
-                _to=msg["to"],
-                _subject=msg["subject"],
-                _content=stripped_content,
-            )
-            # Add message string to results
-            results.append(msg_string)
+            try:
+                msg: mailbox.mboxMessage = _msg
+                # Parse multipart messages
+                if msg.is_multipart():
+                    for part in msg.walk():
+                        ctype = part.get_content_type()
+                        cdispo = str(part.get("Content-Disposition"))
+                        if ctype == "text/plain" and "attachment" not in cdispo:
+                            content = part.get_payload(decode=True)  # decode
+                            break
+                # Get plain message payload for non-multipart messages
+                else:
+                    content = msg.get_payload(decode=True)
+
+                # Parse message HTML content and remove unneeded whitespace
+                soup = BeautifulSoup(content)
+                stripped_content = " ".join(soup.get_content().split())
+                # Format message to include date, sender, receiver and subject
+                msg_string = self.message_format.format(
+                    _date=msg["date"],
+                    _from=msg["from"],
+                    _to=msg["to"],
+                    _subject=msg["subject"],
+                    _content=stripped_content,
+                )
+                # Add message string to results
+                results.append(msg_string)
+            except Exception as e:
+                logger.warning(f"Failed to parse message:\n{_msg}\n with exception {e}")
+
             # Increment counter and return if max count is met
             i += 1
             if self.max_count > 0 and i >= self.max_count:
                 break
 
-        return [Document(result, extra_info=extra_info) for result in results]
+        return [Document(text=result, metadata=extra_info or {}) for result in results]
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/slides_reader.py` & `llama_index-0.7.0/llama_index/readers/file/slides_reader.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 """
 
 import os
 from pathlib import Path
 from typing import Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class PptxReader(BaseReader):
     """Powerpoint parser.
 
     Extract text, caption images, and specify slides.
 
@@ -106,8 +106,8 @@
                         f.write(image_bytes)
                     result += f"\n Image: {self.caption_image(image_filename)}\n\n"
 
                     os.remove(image_filename)
                 if hasattr(shape, "text"):
                     result += f"{shape.text}\n"
 
-        return [Document(result, extra_info=extra_info)]
+        return [Document(text=result, metadata=extra_info or {})]
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/tabular_reader.py` & `llama_index-0.7.0/llama_index/readers/file/tabular_reader.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 """
 from pathlib import Path
 from typing import Any, Dict, List, Optional
 
 import pandas as pd
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class CSVReader(BaseReader):
     """CSV parser.
 
     Args:
         concat_rows (bool): whether to concatenate all rows into one document.
@@ -42,17 +42,17 @@
             raise ImportError("csv module is required to read CSV files.")
         text_list = []
         with open(file, "r") as fp:
             csv_reader = csv.reader(fp)
             for row in csv_reader:
                 text_list.append(", ".join(row))
         if self._concat_rows:
-            return [Document("\n".join(text_list), extra_info=extra_info)]
+            return [Document(text="\n".join(text_list), metadata=extra_info)]
         else:
-            return [Document(text, extra_info=extra_info) for text in text_list]
+            return [Document(text=text, metadata=extra_info) for text in text_list]
 
 
 class PandasCSVReader(BaseReader):
     r"""Pandas-based CSV parser.
 
     Parses CSVs using the separator detection from Pandas `read_csv`function.
     If special parameters are required, use the `pandas_config` dict.
@@ -100,10 +100,16 @@
         df = pd.read_csv(file, **self._pandas_config)
 
         text_list = df.apply(
             lambda row: (self._col_joiner).join(row.astype(str).tolist()), axis=1
         ).tolist()
 
         if self._concat_rows:
-            return [Document((self._row_joiner).join(text_list), extra_info=extra_info)]
+            return [
+                Document(
+                    text=(self._row_joiner).join(text_list), metadata=extra_info or {}
+                )
+            ]
         else:
-            return [Document(text, extra_info=extra_info) for text in text_list]
+            return [
+                Document(text=text, metadata=extra_info or {}) for text in text_list
+            ]
```

### Comparing `llama_index-0.6.9/llama_index/readers/file/video_audio_reader.py` & `llama_index-0.7.0/llama_index/readers/file/video_audio_reader.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 Contains parsers for mp3, mp4 files.
 
 """
 from pathlib import Path
 from typing import Any, Dict, List, Optional, cast
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class VideoAudioReader(BaseReader):
     """Video audio parser.
 
     Extract text from transcript of video/audio files.
 
@@ -57,8 +57,8 @@
             audio.export(file_str, format="mp3")
 
         model = cast(whisper.Whisper, self.parser_config["model"])
         result = model.transcribe(str(file))
 
         transcript = result["text"]
 
-        return [Document(transcript, extra_info=extra_info)]
+        return [Document(text=transcript, metadata=extra_info or {})]
```

### Comparing `llama_index-0.6.9/llama_index/readers/github_readers/github_api_client.py` & `llama_index-0.7.0/llama_index/readers/github_readers/github_api_client.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 """
-Github API client for the GPT-Index library.
+Github API client for the LlamaIndex library.
 
-This module contains the Github API client for the GPT-Index library.
+This module contains the Github API client for the LlamaIndex library.
 It is used by the Github readers to retrieve the data from Github.
 """
 
 import os
 from dataclasses import dataclass
 from typing import Any, Dict, List, Optional
```

### Comparing `llama_index-0.6.9/llama_index/readers/github_readers/github_repository_reader.py` & `llama_index-0.7.0/llama_index/readers/github_readers/github_repository_reader.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,15 +24,15 @@
     GitTreeResponseModel,
 )
 from llama_index.readers.github_readers.utils import (
     BufferedGitBlobDataIterator,
     get_file_extension,
     print_if_verbose,
 )
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 logger = logging.getLogger(__name__)
 
 
 class GithubRepositoryReader(BaseReader):
     """
     Github repository reader.
@@ -218,15 +218,15 @@
             file_path = os.path.join(current_path, tree_obj.path)
             if tree_obj.type == "tree":
                 print_if_verbose(
                     self._verbose,
                     "\t" * current_depth + f"recursing into {tree_obj.path}",
                 )
                 if self._ignore_directories is not None:
-                    if file_path in self._ignore_directories:
+                    if tree_obj.path in self._ignore_directories:
                         print_if_verbose(
                             self._verbose,
                             "\t" * current_depth
                             + f"ignoring tree {tree_obj.path} due to directory",
                         )
                         continue
 
@@ -289,15 +289,14 @@
                     file_path=full_path,
                     file_content=decoded_bytes,
                     tree_sha=blob_data.sha,
                     tree_path=full_path,
                 )
                 if document is not None:
                     documents.append(document)
-                else:
                     continue
 
             try:
                 if decoded_bytes is None:
                     raise ValueError("decoded_bytes is None")
                 decoded_text = decoded_bytes.decode("utf-8")
             except UnicodeDecodeError:
@@ -308,16 +307,16 @@
             print_if_verbose(
                 self._verbose,
                 f"got {len(decoded_text)} characters"
                 + f"- adding to documents - {full_path}",
             )
             document = Document(
                 text=decoded_text,
-                doc_id=blob_data.sha,
-                extra_info={
+                id_=blob_data.sha,
+                metadata={
                     "file_path": full_path,
                     "file_name": full_path.split("/")[-1],
                 },
             )
             documents.append(document)
         return documents
 
@@ -362,31 +361,31 @@
                     + f"{tmpfile.name} for parsing {file_path}",
                 )
                 tmpfile.write(file_content)
                 tmpfile.flush()
                 tmpfile.close()
                 try:
                     docs = reader.load_data(pathlib.Path(tmpfile.name))
-                    parsed_file = "\n\n".join([doc.get_text() for doc in docs])
+                    parsed_file = "\n\n".join([doc.get_content() for doc in docs])
                 except Exception as e:
                     print_if_verbose(self._verbose, f"error while parsing {file_path}")
                     logger.error(
                         "Error while parsing "
                         + f"{file_path} with "
                         + f"{reader.__class__.__name__}:\n{e}"
                     )
                     parsed_file = None
                 finally:
                     os.remove(tmpfile.name)
                 if parsed_file is None:
                     return None
                 return Document(
                     text=parsed_file,
-                    doc_id=tree_sha,
-                    extra_info={
+                    id_=tree_sha,
+                    metadata={
                         "file_path": file_path,
                         "file_name": tree_path,
                     },
                 )
 
 
 if __name__ == "__main__":
@@ -416,22 +415,22 @@
     @timeit
     def load_data_from_commit() -> None:
         """Load data from a commit."""
         documents = reader1.load_data(
             commit_sha="22e198b3b166b5facd2843d6a62ac0db07894a13"
         )
         for document in documents:
-            print(document.extra_info)
+            print(document.metadata)
 
     @timeit
     def load_data_from_branch() -> None:
         """Load data from a branch."""
         documents = reader1.load_data(branch="main")
         for document in documents:
-            print(document.extra_info)
+            print(document.metadata)
 
     input("Press enter to load github repository from branch name...")
 
     load_data_from_branch()
 
     input("Press enter to load github repository from commit sha...")
```

### Comparing `llama_index-0.6.9/llama_index/readers/github_readers/utils.py` & `llama_index-0.7.0/llama_index/readers/github_readers/utils.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/readers/google_readers/gdocs.py` & `llama_index-0.7.0/llama_index/readers/google_readers/gdocs.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Google docs reader."""
 
 import logging
 import os
 from typing import Any, List
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 SCOPES = ["https://www.googleapis.com/auth/documents.readonly"]
 
 logger = logging.getLogger(__name__)
 
 # Copyright 2019 Google LLC
 #
@@ -55,15 +55,15 @@
         """
         if document_ids is None:
             raise ValueError('Must specify a "document_ids" in `load_kwargs`.')
 
         results = []
         for document_id in document_ids:
             doc = self._load_doc(document_id)
-            results.append(Document(doc, extra_info={"document_id": document_id}))
+            results.append(Document(text=doc, metadata={"document_id": document_id}))
         return results
 
     def _load_doc(self, document_id: str) -> str:
         """Load a document from Google Docs.
 
         Args:
             document_id: the document id.
```

### Comparing `llama_index-0.6.9/llama_index/readers/google_readers/gsheets.py` & `llama_index-0.7.0/llama_index/readers/google_readers/gsheets.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Google sheets reader."""
 
 import logging
 import os
 from typing import Any, List
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 SCOPES = ["https://www.googleapis.com/auth/spreadsheets.readonly"]
 
 logger = logging.getLogger(__name__)
 
 # Copyright 2019 Google LLC
 #
@@ -56,15 +56,15 @@
         if spreadsheet_ids is None:
             raise ValueError('Must specify a "spreadsheet_ids" in `load_kwargs`.')
 
         results = []
         for spreadsheet_id in spreadsheet_ids:
             sheet = self._load_sheet(spreadsheet_id)
             results.append(
-                Document(sheet, extra_info={"spreadsheet_id": spreadsheet_id})
+                Document(text=sheet, metadata={"spreadsheet_id": spreadsheet_id})
             )
         return results
 
     def _load_sheet(self, spreadsheet_id: str) -> str:
         """Load a sheet from Google Sheets.
 
         Args:
```

### Comparing `llama_index-0.6.9/llama_index/readers/json.py` & `llama_index-0.7.0/llama_index/readers/json.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """JSON Reader."""
 
 import json
 import re
 from typing import Any, Generator, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 def _depth_first_yield(
     json_data: Any,
     levels_back: int,
     collapse_length: Optional[int],
     path: List[str],
@@ -50,23 +50,23 @@
 class JSONReader(BaseReader):
     """JSON reader.
 
     Reads JSON documents with options to help suss out relationships between nodes.
 
     Args:
         levels_back (int): the number of levels to go back in the JSON tree, 0
-        if you want all levels. If levels_back is None, then we just format the
-        JSON and make each line an embedding
+          if you want all levels. If levels_back is None, then we just format the
+          JSON and make each line an embedding
 
         collapse_length (int): the maximum number of characters a JSON fragment
-        would be collapsed in the output (levels_back needs to be not None)
-        ex: if collapse_length = 10, and
-        input is {a: [1, 2, 3], b: {"hello": "world", "foo": "bar"}}
-        then a would be collapsed into one line, while b would not.
-        Recommend starting around 100 and then adjusting from there.
+          would be collapsed in the output (levels_back needs to be not None)
+          ex: if collapse_length = 10, and
+          input is {a: [1, 2, 3], b: {"hello": "world", "foo": "bar"}}
+          then a would be collapsed into one line, while b would not.
+          Recommend starting around 100 and then adjusting from there.
 
     """
 
     def __init__(
         self, levels_back: Optional[int] = None, collapse_length: Optional[int] = None
     ) -> None:
         """Initialize with arguments."""
@@ -82,17 +82,17 @@
                 # If levels_back isn't set, we just format and make each
                 # line an embedding
                 json_output = json.dumps(data, indent=0)
                 lines = json_output.split("\n")
                 useful_lines = [
                     line for line in lines if not re.match(r"^[{}\[\],]*$", line)
                 ]
-                return [Document("\n".join(useful_lines))]
+                return [Document(text="\n".join(useful_lines))]
             elif self.levels_back is not None:
                 # If levels_back is set, we make the embeddings contain the labels
                 # from further up the JSON tree
                 lines = [
                     *_depth_first_yield(
                         data, self.levels_back, self.collapse_length, []
                     )
                 ]
-                return [Document("\n".join(lines))]
+                return [Document(text="\n".join(lines))]
```

### Comparing `llama_index-0.6.9/llama_index/readers/make_com/wrapper.py` & `llama_index-0.7.0/llama_index/readers/make_com/wrapper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,21 +1,20 @@
 """Make.com API wrapper.
 
 Currently cannot load documents.
 
 """
 
-from typing import Any, List, Optional
-
 import requests
-from llama_index.data_structs.node import Node, NodeWithScore
+from typing import Any, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 from llama_index.response.schema import Response
+from llama_index.schema import NodeWithScore, TextNode
 
 
 class MakeWrapper(BaseReader):
     """Make reader."""
 
     def load_data(self, *args: Any, **load_kwargs: Any) -> List[Document]:
         """Load data from the input directory.
@@ -33,28 +32,28 @@
         Args:
             webhook_url (str): Webhook URL.
             response (Response): Response object.
             query (Optional[str]): Query. Defaults to None.
 
         """
         response_text = response.response
-        source_nodes = [n.to_dict() for n in response.source_nodes]
+        source_nodes = [n.dict() for n in response.source_nodes]
         json_dict = {
             "response": response_text,
             "source_nodes": source_nodes,
             "query": query,
         }
         r = requests.post(webhook_url, json=json_dict)
         r.raise_for_status()
 
 
 if __name__ == "__main__":
     wrapper = MakeWrapper()
     test_response = Response(
         response="test response",
-        source_nodes=[NodeWithScore(node=Node(text="test source", doc_id="test id"))],
+        source_nodes=[NodeWithScore(node=TextNode(text="test source", id_="test id"))],
     )
     wrapper.pass_response_to_webhook(
         "https://hook.us1.make.com/asdfadsfasdfasdfd",
         test_response,
         "Test query",
     )
```

### Comparing `llama_index-0.6.9/llama_index/readers/mbox.py` & `llama_index-0.7.0/llama_index/readers/mbox.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Simple reader for mbox (mailbox) files."""
 import os
 from pathlib import Path
 from typing import Any, List
 
 from llama_index.readers.base import BaseReader
 from llama_index.readers.file.mbox_reader import MboxReader as MboxFileReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class MboxReader(BaseReader):
     """Mbox e-mail reader.
 
     Reads a set of e-mails saved in the mbox format.
     """
```

### Comparing `llama_index-0.6.9/llama_index/readers/metal.py` & `llama_index-0.7.0/llama_index/readers/metal.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Any, Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class MetalReader(BaseReader):
     """Metal reader.
 
     Args:
         api_key (str): Metal API key.
@@ -59,12 +59,12 @@
 
         documents = []
         for item in response["data"]:
             text = item["text"] or (item["metadata"] and item["metadata"]["text"])
             documents.append(Document(text=text))
 
         if not separate_documents:
-            text_list = [doc.get_text() for doc in documents]
+            text_list = [doc.get_content() for doc in documents]
             text = "\n\n".join(text_list)
             documents = [Document(text=text)]
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/milvus.py` & `llama_index-0.7.0/llama_index/readers/milvus.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 """Milvus reader."""
 
 from typing import Any, Dict, List, Optional
 from uuid import uuid4
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class MilvusReader(BaseReader):
     """Milvus reader."""
 
     def __init__(
         self,
@@ -95,15 +95,15 @@
             limit=limit,
         )
 
         documents = []
         # TODO: In future append embedding when more efficient
         for hit in res[0]:
             document = Document(
-                doc_id=hit.entity.get("doc_id"),
+                id_=hit.entity.get("doc_id"),
                 text=hit.entity.get("text"),
             )
 
             documents.append(document)
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/mongo.py` & `llama_index-0.7.0/llama_index/readers/mongo.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Mongo client."""
 
 from typing import Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class SimpleMongoReader(BaseReader):
     """Simple mongo reader.
 
     Concatenates each Mongo doc into Document used by LlamaIndex.
 
@@ -76,10 +76,10 @@
             for field_name in field_names:
                 if field_name not in item:
                     raise ValueError(
                         f"`{field_name}` field not found in Mongo document."
                     )
                 text += item[field_name]
 
-            documents.append(Document(text))
+            documents.append(Document(text=text))
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/myscale.py` & `llama_index-0.7.0/llama_index/readers/myscale.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """MyScale reader."""
 import logging
 from typing import Any, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 logger = logging.getLogger(__name__)
 
 
 def escape_str(value: str) -> str:
     BS = "\\"
     must_escape = (BS, "'")
@@ -69,15 +69,15 @@
                 + ")"
             )
             if self.search_params
             else ""
         )
 
         query_statement = f"""
-            SELECT id, doc_id, text, node_info, extra_info, 
+            SELECT id, doc_id, text, node_info, metadata, 
             distance{search_params_str}(vector, {query_embed_str}) AS dist
             FROM {self.database}.{self.table} {where_str}
             ORDER BY dist {order}
             LIMIT {limit}
             """
         return query_statement
 
@@ -167,10 +167,10 @@
         query_statement = self.config.build_query_statement(
             query_embed=query_vector,
             where_str=where_str,
             limit=limit,
         )
 
         return [
-            Document(doc_id=r["doc_id"], text=r["text"], extra_info=r["extra_info"])
+            Document(id_=r["doc_id"], text=r["text"], metadata=r["metadata"])
             for r in self.client.query(query_statement).named_results()
         ]
```

### Comparing `llama_index-0.6.9/llama_index/readers/notion.py` & `llama_index-0.7.0/llama_index/readers/notion.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import logging
 import os
 from typing import Any, Dict, List, Optional
 
 import requests  # type: ignore
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 INTEGRATION_TOKEN_NAME = "NOTION_INTEGRATION_TOKEN"
 BLOCK_CHILD_URL_TMPL = "https://api.notion.com/v1/blocks/{block_id}/children"
 DATABASE_URL_TMPL = "https://api.notion.com/v1/databases/{database_id}/query"
 SEARCH_URL = "https://api.notion.com/v1/search"
 
 logger = logging.getLogger(__name__)
@@ -151,19 +151,19 @@
             raise ValueError("Must specify either `page_ids` or `database_id`.")
         docs = []
         if database_id is not None:
             # get all the pages in the database
             page_ids = self.query_database(database_id)
             for page_id in page_ids:
                 page_text = self.read_page(page_id)
-                docs.append(Document(page_text, extra_info={"page_id": page_id}))
+                docs.append(Document(text=page_text, metadata={"page_id": page_id}))
         else:
             for page_id in page_ids:
                 page_text = self.read_page(page_id)
-                docs.append(Document(page_text, extra_info={"page_id": page_id}))
+                docs.append(Document(text=page_text, metadata={"page_id": page_id}))
 
         return docs
 
 
 if __name__ == "__main__":
     reader = NotionPageReader()
     logger.info(reader.search("What I"))
```

### Comparing `llama_index-0.6.9/llama_index/readers/obsidian.py` & `llama_index-0.7.0/llama_index/readers/obsidian.py`

 * *Files 9% similar despite different names*

```diff
@@ -5,19 +5,19 @@
 with each Document containing text from under an Obsidian header.
 
 """
 import os
 from pathlib import Path
 from typing import Any, List
 
-from langchain.docstore.document import Document as LCDocument
+from llama_index.bridge.langchain import Document as LCDocument
 
 from llama_index.readers.base import BaseReader
 from llama_index.readers.file.markdown_reader import MarkdownReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class ObsidianReader(BaseReader):
     """Utilities for loading data from an Obsidian Vault.
 
     Args:
         input_dir (str): Path to the vault.
```

### Comparing `llama_index-0.6.9/llama_index/readers/pinecone.py` & `llama_index-0.7.0/llama_index/readers/pinecone.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Pinecone reader."""
 
 from typing import Any, Dict, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class PineconeReader(BaseReader):
     """Pinecone reader.
 
     Args:
         api_key (str): Pinecone API key.
@@ -70,12 +70,12 @@
             text = id_to_text_map[match.id]
             embedding = match.values
             if len(embedding) == 0:
                 embedding = None
             documents.append(Document(text=text, embedding=embedding))
 
         if not separate_documents:
-            text_list = [doc.get_text() for doc in documents]
+            text_list = [doc.get_content() for doc in documents]
             text = "\n\n".join(text_list)
             documents = [Document(text=text)]
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/qdrant.py` & `llama_index-0.7.0/llama_index/readers/qdrant.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Qdrant reader."""
 
 from typing import List, Optional, cast, Dict
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class QdrantReader(BaseReader):
     """Qdrant reader.
 
     Retrieve documents from existing Qdrant collections.
 
@@ -173,15 +173,15 @@
         for point in response:
             payload = cast(Payload, point.payload)
             try:
                 vector = cast(List[float], point.vector)
             except ValueError as e:
                 raise ValueError("Could not cast vector to List[float].") from e
             document = Document(
-                doc_id=payload.get("doc_id"),
+                id_=payload.get("doc_id"),
                 text=payload.get("text"),
-                extra_info=payload.get("extra_info"),
+                metadata=payload.get("metadata"),
                 embedding=vector,
             )
             documents.append(document)
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/redis/utils.py` & `llama_index-0.7.0/llama_index/readers/redis/utils.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/readers/slack.py` & `llama_index-0.7.0/llama_index/readers/slack.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,15 +3,15 @@
 import os
 import time
 from datetime import datetime
 from ssl import SSLContext
 from typing import List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 logger = logging.getLogger(__name__)
 
 
 class SlackReader(BaseReader):
     """Slack reader.
 
@@ -185,15 +185,15 @@
         """
         results = []
         for channel_id in channel_ids:
             channel_content = self._read_channel(
                 channel_id, reverse_chronological=reverse_chronological
             )
             results.append(
-                Document(channel_content, extra_info={"channel": channel_id})
+                Document(text=channel_content, metadata={"channel": channel_id})
             )
         return results
 
 
 if __name__ == "__main__":
     reader = SlackReader()
     logger.info(reader.load_data(channel_ids=["C04DC2VUY3F"]))
```

### Comparing `llama_index-0.6.9/llama_index/readers/steamship/file_reader.py` & `llama_index-0.7.0/llama_index/readers/steamship/file_reader.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Load Documents from a set of persistent Steamship Files."""
 from typing import List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class SteamshipFileReader(BaseReader):
     """Reads persistent Steamship Files and converts them to Documents.
 
     Args:
         api_key: Steamship API key. Defaults to STEAMSHIP_API_KEY value if not provided.
@@ -67,26 +67,24 @@
             files.extend(files_from_query)
 
         if file_handles:
             files.extend([File.get(client=client, handle=h) for h in file_handles])
 
         docs = []
         for file in files:
-            extra_info = {"source": file.handle}
+            metadata = {"source": file.handle}
 
             for tag in file.tags:
-                extra_info[tag.kind] = tag.value
+                metadata[tag.kind] = tag.value
 
             if collapse_blocks:
                 text = join_str.join([b.text for b in file.blocks])
-                docs.append(
-                    Document(text=text, doc_id=file.handle, extra_info=extra_info)
-                )
+                docs.append(Document(text=text, id_=file.handle, metadata=metadata))
             else:
                 docs.extend(
                     [
-                        Document(text=b.text, doc_id=file.handle, extra_info=extra_info)
+                        Document(text=b.text, id_=file.handle, metadata=metadata)
                         for b in file.blocks
                     ]
                 )
 
         return docs
```

### Comparing `llama_index-0.6.9/llama_index/readers/string_iterable.py` & `llama_index-0.7.0/llama_index/readers/string_iterable.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,33 +1,33 @@
 """Simple reader that turns an iterable of strings into a list of Documents."""
 from typing import List
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class StringIterableReader(BaseReader):
     """String Iterable Reader.
 
     Gets a list of documents, given an iterable (e.g. list) of strings.
 
     Example:
         .. code-block:: python
 
-            from llama_index import StringIterableReader, GPTTreeIndex
+            from llama_index import StringIterableReader, TreeIndex
 
             documents = StringIterableReader().load_data(
                 texts=["I went to the store", "I bought an apple"])
-            index = GPTTreeIndex.from_documents(documents)
+            index = TreeIndex.from_documents(documents)
             query_engine = index.as_query_engine()
             query_engine.query("what did I buy?")
 
             # response should be something like "You bought an apple."
     """
 
     def load_data(self, texts: List[str]) -> List[Document]:
         """Load the data."""
         results = []
         for text in texts:
-            results.append(Document(text))
+            results.append(Document(text=text))
 
         return results
```

### Comparing `llama_index-0.6.9/llama_index/readers/twitter.py` & `llama_index-0.7.0/llama_index/readers/twitter.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Simple reader that reads tweets of a twitter handle."""
 from typing import Any, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class TwitterTweetReader(BaseReader):
     """Twitter tweets reader.
 
     Read tweets of user twitter handle.
 
@@ -50,9 +50,9 @@
         for username in twitterhandles:
             # tweets = api.user_timeline(screen_name=user, count=self.num_tweets)
             user = client.get_user(username=username)
             tweets = client.get_users_tweets(user.data.id, max_results=self.num_tweets)
             response = " "
             for tweet in tweets.data:
                 response = response + tweet.text + "\n"
-            results.append(Document(response))
+            results.append(Document(text=response))
         return results
```

### Comparing `llama_index-0.6.9/llama_index/readers/weaviate/reader.py` & `llama_index-0.7.0/llama_index/readers/weaviate/reader.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 """Weaviate reader."""
 
 from typing import Any, List, Optional
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class WeaviateReader(BaseReader):
     """Weaviate reader.
 
     Retrieves documents from Weaviate through vector lookup. Allows option
     to concatenate retrieved documents into one Document, or to return
@@ -89,15 +89,15 @@
 
         if class_name is None:
             # infer class_name if only graphql_query was provided
             class_name = list(data_response["Get"].keys())[0]
         entries = data_response["Get"][class_name]
         documents = []
         for entry in entries:
-            embedding = None
+            embedding: Optional[List[float]] = None
             # for each entry, join properties into <property>:<value>
             # separated by newlines
             text_list = []
             for k, v in entry.items():
                 if k == "_additional":
                     if "vector" in v:
                         embedding = v["vector"]
@@ -105,12 +105,12 @@
                 text_list.append(f"{k}: {v}")
 
             text = "\n".join(text_list)
             documents.append(Document(text=text, embedding=embedding))
 
         if not separate_documents:
             # join all documents into one
-            text_list = [doc.get_text() for doc in documents]
+            text_list = [doc.get_content() for doc in documents]
             text = "\n\n".join(text_list)
             documents = [Document(text=text)]
 
         return documents
```

### Comparing `llama_index-0.6.9/llama_index/readers/web.py` & `llama_index-0.7.0/llama_index/readers/web.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 """Web scraper."""
 import logging
 from typing import Any, Callable, Dict, List, Optional, Tuple
 
 import requests
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 logger = logging.getLogger(__name__)
 
 
 class SimpleWebPageReader(BaseReader):
     """Simple web page reader.
 
@@ -47,15 +47,15 @@
         for url in urls:
             response = requests.get(url, headers=None).text
             if self._html_to_text:
                 import html2text
 
                 response = html2text.html2text(response)
 
-            documents.append(Document(response))
+            documents.append(Document(text=response))
 
         return documents
 
 
 class TrafilaturaWebReader(BaseReader):
     """Trafilatura web page reader.
 
@@ -100,43 +100,43 @@
                     raise ValueError(f"Trafilatura fails to get string from url: {url}")
                 continue
             response = trafilatura.extract(downloaded)
             if not response:
                 if self.error_on_missing:
                     raise ValueError(f"Trafilatura fails to parse page: {url}")
                 continue
-            documents.append(Document(response))
+            documents.append(Document(text=response))
 
         return documents
 
 
 def _substack_reader(soup: Any) -> Tuple[str, Dict[str, Any]]:
     """Extract text from Substack blog post."""
-    extra_info = {
+    metadata = {
         "Title of this Substack post": soup.select_one("h1.post-title").getText(),
         "Subtitle": soup.select_one("h3.subtitle").getText(),
         "Author": soup.select_one("span.byline-names").getText(),
     }
     text = soup.select_one("div.available-content").getText()
-    return text, extra_info
+    return text, metadata
 
 
 DEFAULT_WEBSITE_EXTRACTOR: Dict[str, Callable[[Any], Tuple[str, Dict[str, Any]]]] = {
     "substack.com": _substack_reader,
 }
 
 
 class BeautifulSoupWebReader(BaseReader):
     """BeautifulSoup web page reader.
 
     Reads pages from the web.
     Requires the `bs4` and `urllib` packages.
 
     Args:
-        file_extractor (Optional[Dict[str, Callable]]): A mapping of website
+        website_extractor (Optional[Dict[str, Callable]]): A mapping of website
             hostname (e.g. google.com) to a function that specifies how to
             extract text from the BeautifulSoup obj. See DEFAULT_WEBSITE_EXTRACTOR.
     """
 
     def __init__(
         self,
         website_extractor: Optional[Dict[str, Callable]] = None,
@@ -182,22 +182,22 @@
                 raise ValueError(f"One of the inputs is not a valid url: {url}")
 
             hostname = custom_hostname or urlparse(url).hostname or ""
 
             soup = BeautifulSoup(page.content, "html.parser")
 
             data = ""
-            extra_info = {"URL": url}
+            metadata = {"URL": url}
             if hostname in self.website_extractor:
                 data, metadata = self.website_extractor[hostname](soup)
-                extra_info.update(metadata)
+                metadata.update(metadata)
             else:
                 data = soup.getText()
 
-            documents.append(Document(data, extra_info=extra_info))
+            documents.append(Document(text=data, metadata=metadata))
 
         return documents
 
 
 class RssReader(BaseReader):
     """RSS reader.
 
@@ -255,16 +255,16 @@
                     data = entry.description or entry.summary
 
                 if self._html_to_text:
                     import html2text
 
                     data = html2text.html2text(data)
 
-                extra_info = {"title": entry.title, "link": entry.link}
-                documents.append(Document(data, extra_info=extra_info))
+                metadata = {"title": entry.title, "link": entry.link}
+                documents.append(Document(text=data, metadata=metadata))
 
         return documents
 
 
 if __name__ == "__main__":
     reader = SimpleWebPageReader()
     logger.info(reader.load_data(["http://www.google.com"]))
```

### Comparing `llama_index-0.6.9/llama_index/readers/wikipedia.py` & `llama_index-0.7.0/llama_index/readers/wikipedia.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Simple reader that reads wikipedia."""
 from typing import Any, List
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class WikipediaReader(BaseReader):
     """Wikipedia reader.
 
     Reads a page.
 
@@ -29,9 +29,9 @@
 
         """
         import wikipedia
 
         results = []
         for page in pages:
             page_content = wikipedia.page(page, **load_kwargs).content
-            results.append(Document(page_content))
+            results.append(Document(text=page_content))
         return results
```

### Comparing `llama_index-0.6.9/llama_index/readers/youtube_transcript.py` & `llama_index-0.7.0/llama_index/readers/youtube_transcript.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Simple Reader that reads transcript of youtube video."""
 from typing import Any, List
 
 from llama_index.readers.base import BaseReader
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class YoutubeTranscriptReader(BaseReader):
     """Youtube Transcript reader."""
 
     def __init__(self) -> None:
         """Initialize with parameters."""
@@ -30,9 +30,9 @@
         results = []
         for link in ytlinks:
             video_id = link.split("?v=")[-1]
             srt = YouTubeTranscriptApi.get_transcript(video_id)
             transcript = ""
             for chunk in srt:
                 transcript = transcript + chunk["text"] + "\n"
-            results.append(Document(transcript))
+            results.append(Document(text=transcript))
         return results
```

### Comparing `llama_index-0.6.9/llama_index/response/notebook_utils.py` & `llama_index-0.7.0/llama_index/response/notebook_utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,53 +1,55 @@
 """Utils for jupyter notebook."""
 from typing import Any, Dict, Tuple
 
 from IPython.display import Markdown, display
 
-from llama_index.data_structs.node import ImageNode, NodeWithScore
 from llama_index.img_utils import b64_2_img
 from llama_index.response.schema import Response
+from llama_index.schema import ImageNode, NodeWithScore
 from llama_index.utils import truncate_text
 
 DEFAULT_THUMBNAIL_SIZE = (512, 512)
 
 
 def display_image(img_str: str, size: Tuple[int, int] = DEFAULT_THUMBNAIL_SIZE) -> None:
     """Display base64 encoded image str as image for jupyter notebook."""
     img = b64_2_img(img_str)
     img.thumbnail(size)
     display(img)
 
 
 def display_source_node(source_node: NodeWithScore, source_length: int = 100) -> None:
     """Display source node for jupyter notebook."""
-    source_text_fmt = truncate_text(source_node.node.get_text().strip(), source_length)
+    source_text_fmt = truncate_text(
+        source_node.node.get_content().strip(), source_length
+    )
     text_md = (
-        f"**Document ID:** {source_node.node.doc_id}<br>"
+        f"**Node ID:** {source_node.node.node_id}<br>"
         f"**Similarity:** {source_node.score}<br>"
         f"**Text:** {source_text_fmt}<br>"
     )
     if isinstance(source_node.node, ImageNode):
         text_md += "**Image:**"
 
     display(Markdown(text_md))
     if isinstance(source_node.node, ImageNode) and source_node.node.image is not None:
         display_image(source_node.node.image)
 
 
-def display_extra_info(extra_info: Dict[str, Any]) -> None:
-    """Display extra info for jupyter notebook."""
-    display(extra_info)
+def display_metadata(metadata: Dict[str, Any]) -> None:
+    """Display metadata for jupyter notebook."""
+    display(metadata)
 
 
 def display_response(
     response: Response,
     source_length: int = 100,
     show_source: bool = False,
-    show_extra_info: bool = False,
+    show_metadata: bool = False,
 ) -> None:
     """Display response for jupyter notebook."""
     if response.response is None:
         response_text = "None"
     else:
         response_text = response.response.strip()
 
@@ -55,10 +57,10 @@
     if show_source:
         for ind, source_node in enumerate(response.source_nodes):
             display(Markdown("---"))
             display(
                 Markdown(f"**`Source Node {ind + 1}/{len(response.source_nodes)}`**")
             )
             display_source_node(source_node, source_length=source_length)
-    if show_extra_info:
-        if response.extra_info is not None:
-            display_extra_info(response.extra_info)
+    if show_metadata:
+        if response.metadata is not None:
+            display_metadata(response.metadata)
```

### Comparing `llama_index-0.6.9/llama_index/response/pprint_utils.py` & `llama_index-0.7.0/llama_index/response/pprint_utils.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,28 +1,30 @@
 """Utils for pretty print."""
 import textwrap
 from pprint import pprint
 from typing import Any, Dict
 
-from llama_index.data_structs.node import NodeWithScore
 from llama_index.response.schema import Response
+from llama_index.schema import NodeWithScore
 from llama_index.utils import truncate_text
 
 
-def pprint_extra_info(extra_info: Dict[str, Any]) -> None:
-    """Display extra info for jupyter notebook."""
-    pprint(extra_info)
+def pprint_metadata(metadata: Dict[str, Any]) -> None:
+    """Display metadata for jupyter notebook."""
+    pprint(metadata)
 
 
 def pprint_source_node(
     source_node: NodeWithScore, source_length: int = 350, wrap_width: int = 70
 ) -> None:
     """Display source node for jupyter notebook."""
-    source_text_fmt = truncate_text(source_node.node.get_text().strip(), source_length)
-    print(f"Document ID: {source_node.node.doc_id}")
+    source_text_fmt = truncate_text(
+        source_node.node.get_content().strip(), source_length
+    )
+    print(f"Node ID: {source_node.node.node_id}")
     print(f"Similarity: {source_node.score}")
     print(textwrap.fill(f"Text: {source_text_fmt}\n", width=wrap_width))
 
 
 def pprint_response(
     response: Response,
     source_length: int = 350,
```

### Comparing `llama_index-0.6.9/llama_index/response/schema.py` & `llama_index-0.7.0/llama_index/response/schema.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 """Response schema."""
 
 from dataclasses import dataclass, field
-from typing import Any, Dict, Generator, List, Optional, Union
+from typing import Any, Dict, List, Optional, Union
 
-from llama_index.data_structs.node import NodeWithScore
+from llama_index.schema import NodeWithScore
+from llama_index.types import TokenGen
 from llama_index.utils import truncate_text
 
 
 @dataclass
 class Response:
     """Response object.
 
@@ -16,26 +17,26 @@
     Attributes:
         response: The response text.
 
     """
 
     response: Optional[str]
     source_nodes: List[NodeWithScore] = field(default_factory=list)
-    extra_info: Optional[Dict[str, Any]] = None
+    metadata: Optional[Dict[str, Any]] = None
 
     def __str__(self) -> str:
         """Convert to string representation."""
         return self.response or "None"
 
     def get_formatted_sources(self, length: int = 100) -> str:
         """Get formatted sources text."""
         texts = []
         for source_node in self.source_nodes:
-            fmt_text_chunk = truncate_text(source_node.node.get_text(), length)
-            doc_id = source_node.node.doc_id or "None"
+            fmt_text_chunk = truncate_text(source_node.node.get_content(), length)
+            doc_id = source_node.node.node_id or "None"
             source_text = f"> Source (Doc id: {doc_id}): {fmt_text_chunk}"
             texts.append(source_text)
         return "\n\n".join(texts)
 
 
 @dataclass
 class StreamingResponse:
@@ -44,17 +45,17 @@
     Returned if streaming=True.
 
     Attributes:
         response_gen: The response generator.
 
     """
 
-    response_gen: Optional[Generator]
+    response_gen: Optional[TokenGen]
     source_nodes: List[NodeWithScore] = field(default_factory=list)
-    extra_info: Optional[Dict[str, Any]] = None
+    metadata: Optional[Dict[str, Any]] = None
     response_txt: Optional[str] = None
 
     def __str__(self) -> str:
         """Convert to string representation."""
         if self.response_txt is None and self.response_gen is not None:
             response_txt = ""
             for text in self.response_gen:
@@ -65,31 +66,32 @@
     def get_response(self) -> Response:
         """Get a standard response object."""
         if self.response_txt is None and self.response_gen is not None:
             response_txt = ""
             for text in self.response_gen:
                 response_txt += text
             self.response_txt = response_txt
-        return Response(self.response_txt, self.source_nodes, self.extra_info)
+        return Response(self.response_txt, self.source_nodes, self.metadata)
 
     def print_response_stream(self) -> None:
         """Print the response stream."""
         if self.response_txt is None and self.response_gen is not None:
             response_txt = ""
             for text in self.response_gen:
                 print(text, end="", flush=True)
+                response_txt += text
             self.response_txt = response_txt
         else:
             print(self.response_txt)
 
     def get_formatted_sources(self, length: int = 100) -> str:
         """Get formatted sources text."""
         texts = []
         for source_node in self.source_nodes:
-            fmt_text_chunk = truncate_text(source_node.source_text, length)
-            doc_id = source_node.doc_id or "None"
-            source_text = f"> Source (Doc id: {doc_id}): {fmt_text_chunk}"
+            fmt_text_chunk = truncate_text(source_node.node.get_content(), length)
+            node_id = source_node.node.node_id or "None"
+            source_text = f"> Source (Node id: {node_id}): {fmt_text_chunk}"
             texts.append(source_text)
         return "\n\n".join(texts)
 
 
 RESPONSE_TYPE = Union[Response, StreamingResponse]
```

### Comparing `llama_index-0.6.9/llama_index/retrievers/__init__.py` & `llama_index-0.7.0/llama_index/retrievers/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.empty.retrievers import EmptyIndexRetriever
 from llama_index.indices.keyword_table.retrievers import KeywordTableSimpleRetriever
-from llama_index.indices.knowledge_graph.retrievers import KGTableRetriever
+from llama_index.indices.knowledge_graph.retriever import KGTableRetriever
 from llama_index.indices.list.retrievers import (
     ListIndexEmbeddingRetriever,
     ListIndexRetriever,
 )
 from llama_index.indices.tree.all_leaf_retriever import TreeAllLeafRetriever
 from llama_index.indices.tree.select_leaf_embedding_retriever import (
     TreeSelectLeafEmbeddingRetriever,
```

### Comparing `llama_index-0.6.9/llama_index/retrievers/transform_retriever.py` & `llama_index-0.7.0/llama_index/retrievers/transform_retriever.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,30 +1,31 @@
 from typing import List, Optional
-from llama_index.data_structs.node import NodeWithScore
+
 from llama_index.indices.base_retriever import BaseRetriever
 from llama_index.indices.query.query_transform.base import BaseQueryTransform
 from llama_index.indices.query.schema import QueryBundle
+from llama_index.schema import NodeWithScore
 
 
 class TransformRetriever(BaseRetriever):
     """Transform Retriever.
 
     Takes in an existing retriever and a query transform and runs the query transform
     before running the retriever.
 
     """
 
     def __init__(
         self,
         retriever: BaseRetriever,
         query_transform: BaseQueryTransform,
-        transform_extra_info: Optional[dict] = None,
+        transform_metadata: Optional[dict] = None,
     ) -> None:
         self._retriever = retriever
         self._query_transform = query_transform
-        self._transform_extra_info = transform_extra_info
+        self._transform_metadata = transform_metadata
 
     def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
         query_bundle = self._query_transform.run(
-            query_bundle, extra_info=self._transform_extra_info
+            query_bundle, metadata=self._transform_metadata
         )
         return self._retriever.retrieve(query_bundle)
```

### Comparing `llama_index-0.6.9/llama_index/schema.py` & `llama_index-0.7.0/llama_index/tools/tool_spec/notion/base.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,121 +1,100 @@
-"""Base schema for data structures."""
-from abc import abstractmethod
-from dataclasses import dataclass
-from hashlib import sha256
-from typing import Any, Dict, List, Optional
+"""Notion tool spec."""
 
-from dataclasses_json import DataClassJsonMixin
+from llama_index.tools.tool_spec.base import BaseToolSpec
+from llama_index.readers.notion import NotionPageReader
+from typing import Optional, List, Type, Dict, Any
+import requests
+from pydantic import BaseModel
 
-from llama_index.utils import get_new_id
+SEARCH_URL = "https://api.notion.com/v1/search"
 
 
-def _validate_is_flat_dict(metadata_dict: dict) -> None:
-    """
-    Validate that metadata dict is flat,
-    and key is str, and value is one of (str, int, float).
-    """
-    for key, val in metadata_dict.items():
-        if not isinstance(key, str):
-            raise ValueError("Metadata key must be str!")
-        if not isinstance(val, (str, int, float)):
-            raise ValueError("Value must be one of (str, int, float)")
+class NotionLoadDataSchema(BaseModel):
+    """Notion load data schema."""
 
+    page_ids: Optional[List[str]] = None
+    database_id: Optional[str] = None
 
-@dataclass
-class BaseDocument(DataClassJsonMixin):
-    """Base document.
 
-    Generic abstract interfaces that captures both index structs
-    as well as documents.
+class NotionSearchDataSchema(BaseModel):
+    """Notion search data schema."""
+
+    query: str
+    direction: Optional[str] = None
+    timestamp: Optional[str] = None
+    value: Optional[str] = None
+    property: Optional[str] = None
+    page_size: int = 100
 
-    """
 
-    # TODO: consolidate fields from Document/IndexStruct into base class
-    text: Optional[str] = None
-    doc_id: Optional[str] = None
-    embedding: Optional[List[float]] = None
-    doc_hash: Optional[str] = None
-
-    """"
-    metadata fields
-    - injected as part of the text shown to LLMs as context
-    - used by vector DBs for metadata filtering
+class NotionToolSpec(BaseToolSpec):
+    """Notion tool spec.
+
+    Currently a simple wrapper around the data loader.
+    TODO: add more methods to the Notion spec.
 
-    This must be a flat dictionary, 
-    and only uses str keys, and (str, int, float) values.
     """
-    extra_info: Optional[Dict[str, Any]] = None
 
-    def __post_init__(self) -> None:
-        """Post init."""
-        # assign doc_id if not set
-        if self.doc_id is None:
-            self.doc_id = get_new_id(set())
-        if self.doc_hash is None:
-            self.doc_hash = self._generate_doc_hash()
-
-        if self.extra_info is not None:
-            _validate_is_flat_dict(self.extra_info)
-
-    def _generate_doc_hash(self) -> str:
-        """Generate a hash to represent the document."""
-        doc_identity = str(self.text) + str(self.extra_info)
-        return sha256(doc_identity.encode("utf-8", "surrogatepass")).hexdigest()
-
-    @classmethod
-    @abstractmethod
-    def get_type(cls) -> str:
-        """Get Document type."""
-
-    @classmethod
-    def get_types(cls) -> List[str]:
-        """Get Document type."""
-        # TODO: remove this method
-        # a hack to preserve backwards compatibility for vector indices
-        return [cls.get_type()]
-
-    def get_text(self) -> str:
-        """Get text."""
-        if self.text is None:
-            raise ValueError("text field not set.")
-        return self.text
-
-    def get_doc_id(self) -> str:
-        """Get doc_id."""
-        if self.doc_id is None:
-            raise ValueError("doc_id not set.")
-        return self.doc_id
-
-    def get_doc_hash(self) -> str:
-        """Get doc_hash."""
-        if self.doc_hash is None:
-            raise ValueError("doc_hash is not set.")
-        return self.doc_hash
-
-    @property
-    def is_doc_id_none(self) -> bool:
-        """Check if doc_id is None."""
-        return self.doc_id is None
-
-    @property
-    def is_text_none(self) -> bool:
-        """Check if text is None."""
-        return self.text is None
+    spec_functions = ["load_data", "search_data"]
 
-    def get_embedding(self) -> List[float]:
-        """Get embedding.
+    def __init__(self, integration_token: Optional[str] = None) -> None:
+        """Initialize with parameters."""
+        self.reader = NotionPageReader(integration_token=integration_token)
+
+    def get_fn_schema_from_fn_name(self, fn_name: str) -> Optional[Type[BaseModel]]:
+        """Return map from function name."""
+        if fn_name == "load_data":
+            return NotionLoadDataSchema
+        elif fn_name == "search_data":
+            return NotionSearchDataSchema
+        else:
+            raise ValueError(f"Invalid function name: {fn_name}")
+
+    def load_data(
+        self, page_ids: Optional[List[str]] = None, database_id: Optional[str] = None
+    ) -> str:
+        """Loads content from a set of page ids or a database id.
 
-        Errors if embedding is None.
+        Don't use this endpoint if you don't know the page ids or database id.
 
         """
-        if self.embedding is None:
-            raise ValueError("embedding not set.")
-        return self.embedding
-
-    @property
-    def extra_info_str(self) -> Optional[str]:
-        """Extra info string."""
-        if self.extra_info is None:
-            return None
+        page_ids = page_ids or []
+        docs = self.reader.load_data(page_ids=page_ids, database_id=database_id)
+        return "\n".join([doc.get_content() for doc in docs])
+
+    def search_data(
+        self,
+        query: str,
+        direction: Optional[str] = None,
+        timestamp: Optional[str] = None,
+        value: Optional[str] = None,
+        property: Optional[str] = None,
+        page_size: int = 100,
+    ) -> str:
+        """Search a list of relevant pages.
+
+        Contains metadata for each page (but not the page content).
 
-        return "\n".join([f"{k}: {str(v)}" for k, v in self.extra_info.items()])
+        """
+        payload: Dict[str, Any] = {
+            "query": query,
+            "page_size": page_size,
+        }
+        if direction is not None or timestamp is not None:
+            payload["sort"] = {}
+            if direction is not None:
+                payload["sort"]["direction"] = direction
+            if timestamp is not None:
+                payload["sort"]["timestamp"] = timestamp
+
+        if value is not None or property is not None:
+            payload["filter"] = {}
+            if value is not None:
+                payload["filter"]["value"] = value
+            if property is not None:
+                payload["filter"]["property"] = property
+
+        response = requests.post(SEARCH_URL, json=payload, headers=self.reader.headers)
+        response_json = response.json()
+        response_results = response_json["results"]
+        return response_results
```

### Comparing `llama_index-0.6.9/llama_index/selectors/llm_selectors.py` & `llama_index-0.7.0/llama_index/selectors/llm_selectors.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,21 +1,23 @@
 from typing import Any, List, Optional, Sequence, cast
 
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
-from llama_index.llm_predictor.base import LLMPredictor
-from llama_index.output_parsers.base import BaseOutputParser, StructuredOutput
+from llama_index.llm_predictor.base import BaseLLMPredictor
+from llama_index.output_parsers.base import StructuredOutput
+from llama_index.types import BaseOutputParser
 from llama_index.output_parsers.selection import Answer, SelectionOutputParser
+from llama_index.prompts.prompt_type import PromptType
 from llama_index.selectors.prompts import (
     DEFAULT_MULTI_SELECT_PROMPT_TMPL,
     DEFAULT_SINGLE_SELECT_PROMPT_TMPL,
     MultiSelectPrompt,
     SingleSelectPrompt,
 )
-from llama_index.selectors.types import BaseSelector, SelectorResult
+from llama_index.selectors.types import BaseSelector, SelectorResult, SingleSelection
 from llama_index.tools.types import ToolMetadata
 
 
 def _build_choices_text(choices: Sequence[ToolMetadata]) -> str:
     """Convert sequence of metadata to enumeration text."""
     texts: List[str] = []
     for ind, choice in enumerate(choices):
@@ -25,32 +27,36 @@
     return "\n\n".join(texts)
 
 
 def _structured_output_to_selector_result(output: Any) -> SelectorResult:
     """Convert structured output to selector result."""
     structured_output = cast(StructuredOutput, output)
     answers = cast(List[Answer], structured_output.parsed_output)
-    inds = [answer.choice - 1 for answer in answers]  # for zero indexing
-    reasons = [answer.reason for answer in answers]
-    return SelectorResult(inds=inds, reasons=reasons)
+
+    # adjust for zero indexing
+    selections = [
+        SingleSelection(index=answer.choice - 1, reason=answer.reason)
+        for answer in answers
+    ]
+    return SelectorResult(selections=selections)
 
 
 class LLMSingleSelector(BaseSelector):
     """LLM single selector
 
     LLM-based selector that chooses one out of many options.
 
     Args:
-        llm_predictor (LLMPredictor): An LLM predictor.
+        llm_predictor (BaseLLMPredictor): An LLM predictor.
         prompt (SingleSelectPrompt): A LLM prompt for selecting one out of many options.
     """
 
     def __init__(
         self,
-        llm_predictor: LLMPredictor,
+        llm_predictor: BaseLLMPredictor,
         prompt: SingleSelectPrompt,
     ) -> None:
         self._llm_predictor = llm_predictor
         self._prompt = prompt
 
         if self._prompt.output_parser is None:
             raise ValueError("Prompt should have output parser.")
@@ -68,26 +74,28 @@
         output_parser = output_parser or SelectionOutputParser()
 
         # add output formatting to prompt template
         prompt_template_str = output_parser.format(prompt_template_str)
 
         # construct prompt
         prompt = SingleSelectPrompt(
-            template=prompt_template_str, output_parser=output_parser
+            template=prompt_template_str,
+            output_parser=output_parser,
+            prompt_type=PromptType.SINGLE_SELECT,
         )
         return cls(service_context.llm_predictor, prompt)
 
     def _select(
         self, choices: Sequence[ToolMetadata], query: QueryBundle
     ) -> SelectorResult:
         # prepare input
         choices_text = _build_choices_text(choices)
 
         # predict
-        prediction, _ = self._llm_predictor.predict(
+        prediction = self._llm_predictor.predict(
             prompt=self._prompt,
             num_choices=len(choices),
             context_list=choices_text,
             query_str=query.query_str,
         )
 
         # parse output
@@ -98,15 +106,15 @@
     async def _aselect(
         self, choices: Sequence[ToolMetadata], query: QueryBundle
     ) -> SelectorResult:
         # prepare input
         choices_text = _build_choices_text(choices)
 
         # predict
-        prediction, _ = await self._llm_predictor.apredict(
+        prediction = await self._llm_predictor.apredict(
             prompt=self._prompt,
             num_choices=len(choices),
             context_list=choices_text,
             query_str=query.query_str,
         )
 
         # parse output
@@ -124,15 +132,15 @@
         llm_predictor (LLMPredictor): An LLM predictor.
         prompt (SingleSelectPrompt): A LLM prompt for selecting multiple out of many
             options.
     """
 
     def __init__(
         self,
-        llm_predictor: LLMPredictor,
+        llm_predictor: BaseLLMPredictor,
         prompt: MultiSelectPrompt,
         max_outputs: Optional[int] = None,
     ) -> None:
         self._llm_predictor = llm_predictor
         self._prompt = prompt
         self._max_outputs = max_outputs
 
@@ -152,26 +160,28 @@
         output_parser = output_parser or SelectionOutputParser()
 
         # add output formatting
         prompt_template_str = output_parser.format(prompt_template_str)
 
         # construct prompt
         prompt = MultiSelectPrompt(
-            template=prompt_template_str, output_parser=output_parser
+            template=prompt_template_str,
+            output_parser=output_parser,
+            prompt_type=PromptType.MULTI_SELECT,
         )
         return cls(service_context.llm_predictor, prompt, max_outputs)
 
     def _select(
         self, choices: Sequence[ToolMetadata], query: QueryBundle
     ) -> SelectorResult:
         # prepare input
         context_list = _build_choices_text(choices)
         max_outputs = self._max_outputs or len(choices)
 
-        prediction, _ = self._llm_predictor.predict(
+        prediction = self._llm_predictor.predict(
             prompt=self._prompt,
             num_choices=len(choices),
             max_outputs=max_outputs,
             context_list=context_list,
             query_str=query.query_str,
         )
 
@@ -182,15 +192,15 @@
     async def _aselect(
         self, choices: Sequence[ToolMetadata], query: QueryBundle
     ) -> SelectorResult:
         # prepare input
         context_list = _build_choices_text(choices)
         max_outputs = self._max_outputs or len(choices)
 
-        prediction, _ = await self._llm_predictor.apredict(
+        prediction = await self._llm_predictor.apredict(
             prompt=self._prompt,
             num_choices=len(choices),
             max_outputs=max_outputs,
             context_list=context_list,
             query_str=query.query_str,
         )
```

### Comparing `llama_index-0.6.9/llama_index/selectors/prompts.py` & `llama_index-0.7.0/llama_index/selectors/prompts.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,28 +1,29 @@
-from typing import List
 from llama_index.prompts.base import Prompt
 from llama_index.prompts.prompt_type import PromptType
 
+"""Single select prompt.
 
-class SingleSelectPrompt(Prompt):
-    """Single select prompt.
+Prompt to select one out of `num_choices` options provided in `context_list`,
+given a query `query_str`.
 
-    Prompt to select one out of `num_choices` options provided in `context_list`,
-    given a query `query_str`.
+Required template variables: `num_chunks`, `context_list`, `query_str`
 
-    Required template variables: `num_chunks`, `context_list`, `query_str`
+"""
+SingleSelectPrompt = Prompt
 
-    Args:
-        template (str): Template for the prompt.
-        **prompt_kwargs: Keyword arguments for the prompt.
+"""Multiple select prompt.
 
-    """
+Prompt to select multiple candidates (up to `max_outputs`) out of `num_choices`
+options provided in `context_list`, given a query `query_str`.
 
-    prompt_type: PromptType = PromptType.SINGLE_SELECT
-    input_variables: List[str] = ["num_choices", "context_list", "query_str"]
+Required template variables: `num_chunks`, `context_list`, `query_str`,
+    `max_outputs`
+"""
+MultiSelectPrompt = Prompt
 
 
 # single select
 DEFAULT_SINGLE_SELECT_PROMPT_TMPL = (
     "Some choices are given below. It is provided in a numbered list "
     "(1 to {num_choices}),"
     "where each item in the list corresponds to a summary.\n"
@@ -30,53 +31,57 @@
     "{context_list}"
     "\n---------------------\n"
     "Using only the choices above and not prior knowledge, return "
     "the choice that is most relevant to the question: '{query_str}'\n"
 )
 
 
-DEFAULT_SINGLE_SELECT_PROMPT = SingleSelectPrompt(
-    template=DEFAULT_SINGLE_SELECT_PROMPT_TMPL
+DEFAULT_SINGLE_SELECT_PROMPT = Prompt(
+    template=DEFAULT_SINGLE_SELECT_PROMPT_TMPL, prompt_type=PromptType.SINGLE_SELECT
 )
 
 
-class MultiSelectPrompt(Prompt):
-    """Multiple select prompt.
-
-    Prompt to select multiple candidates (up to `max_outputs`) out of `num_choices`
-    options provided in `context_list`, given a query `query_str`.
-
-    Required template variables: `num_chunks`, `context_list`, `query_str`,
-        `max_outputs`
-
-    Args:
-        template (str): Template for the prompt.
-        **prompt_kwargs: Keyword arguments for the prompt.
-
-    """
-
-    prompt_type: PromptType = PromptType.MULTI_SELECT
-    input_variables: List[str] = [
-        "num_choices",
-        "context_list",
-        "query_str",
-        "max_outputs",
-    ]
-
-
 # multiple select
 DEFAULT_MULTI_SELECT_PROMPT_TMPL = (
     "Some choices are given below. It is provided in a numbered "
     "list (1 to {num_choices}), "
     "where each item in the list corresponds to a summary.\n"
     "---------------------\n"
     "{context_list}"
     "\n---------------------\n"
     "Using only the choices above and not prior knowledge, return the top choices "
-    "(no more than {max_outputs}, ranked by most relevant to least) that "
+    "(no more than {max_outputs}, but only select what is needed) that "
     "are most relevant to the question: '{query_str}'\n"
 )
 
 
-DEFAULT_MULTIPLE_SELECT_PROMPT = MultiSelectPrompt(
-    template=DEFAULT_MULTI_SELECT_PROMPT_TMPL
+DEFAULT_MULTIPLE_SELECT_PROMPT = Prompt(
+    template=DEFAULT_MULTI_SELECT_PROMPT_TMPL, prompt_type=PromptType.MULTI_SELECT
+)
+
+# single pydantic select
+DEFAULT_SINGLE_PYD_SELECT_PROMPT_TMPL = (
+    "Some choices are given below. It is provided in a numbered list "
+    "(1 to {num_choices}),"
+    "where each item in the list corresponds to a summary.\n"
+    "---------------------\n"
+    "{context_list}"
+    "\n---------------------\n"
+    "Using only the choices above and not prior knowledge, generate "
+    "the selection object and reason that is most relevant to the "
+    "question: '{query_str}'\n"
+)
+
+
+# multiple pydantic select
+DEFAULT_MULTI_PYD_SELECT_PROMPT_TMPL = (
+    "Some choices are given below. It is provided in a numbered "
+    "list (1 to {num_choices}), "
+    "where each item in the list corresponds to a summary.\n"
+    "---------------------\n"
+    "{context_list}"
+    "\n---------------------\n"
+    "Using only the choices above and not prior knowledge, return the top choice(s) "
+    "(no more than {max_outputs}, but only select what is needed) by generating "
+    "the selection object and reasons that are most relevant to the "
+    "question: '{query_str}'\n"
 )
```

### Comparing `llama_index-0.6.9/llama_index/selectors/types.py` & `llama_index-0.7.0/llama_index/selectors/types.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,37 +1,56 @@
 from abc import ABC, abstractmethod
-from dataclasses import dataclass
+from pydantic import BaseModel
 from typing import List, Sequence, Union
 
 from llama_index.indices.query.schema import QueryBundle, QueryType
 from llama_index.tools.types import ToolMetadata
 
 MetadataType = Union[str, ToolMetadata]
 
 
-@dataclass
-class SelectorResult:
-    inds: List[int]
-    reasons: List[str]
+class SingleSelection(BaseModel):
+    """A single selection of a choice."""
+
+    index: int
+    reason: str
+
+
+class MultiSelection(BaseModel):
+    """A multi-selection of choices."""
+
+    selections: List[SingleSelection]
 
     @property
     def ind(self) -> int:
-        if len(self.inds) != 1:
+        if len(self.selections) != 1:
             raise ValueError(
-                f"There are {len(self.inds)} selections, " "please use .inds."
+                f"There are {len(self.selections)} selections, " "please use .inds."
             )
-        return self.inds[0]
+        return self.selections[0].index
 
     @property
     def reason(self) -> str:
         if len(self.reasons) != 1:
             raise ValueError(
                 f"There are {len(self.reasons)} selections, " "please use .reasons."
             )
-        return self.reasons[0]
+        return self.selections[0].reason
+
+    @property
+    def inds(self) -> List[int]:
+        return [x.index for x in self.selections]
+
+    @property
+    def reasons(self) -> List[str]:
+        return [x.reason for x in self.selections]
+
+
+# separate name for clarity and to not confuse function calling model
+SelectorResult = MultiSelection
 
 
 def _wrap_choice(choice: MetadataType) -> ToolMetadata:
     if isinstance(choice, ToolMetadata):
         return choice
     elif isinstance(choice, str):
         return ToolMetadata(description=choice)
```

### Comparing `llama_index-0.6.9/llama_index/storage/docstore/__init__.py` & `llama_index-0.7.0/llama_index/storage/docstore/__init__.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 from llama_index.storage.docstore.types import BaseDocumentStore
 from llama_index.storage.docstore.simple_docstore import SimpleDocumentStore
 from llama_index.storage.docstore.mongo_docstore import MongoDocumentStore
 from llama_index.storage.docstore.keyval_docstore import KVDocumentStore
+from llama_index.storage.docstore.redis_docstore import RedisDocumentStore
 
 # alias for backwards compatibility
 from llama_index.storage.docstore.simple_docstore import DocumentStore
 
 
 __all__ = [
     "BaseDocumentStore",
     "DocumentStore",
     "SimpleDocumentStore",
     "MongoDocumentStore",
     "KVDocumentStore",
+    "RedisDocumentStore",
 ]
```

### Comparing `llama_index-0.6.9/llama_index/storage/docstore/mongo_docstore.py` & `llama_index-0.7.0/llama_index/storage/docstore/mongo_docstore.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/storage/docstore/registry.py` & `llama_index-0.7.0/llama_index/storage/docstore/registry.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/storage/docstore/simple_docstore.py` & `llama_index-0.7.0/llama_index/storage/docstore/simple_docstore.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,39 +1,42 @@
 import os
-import fsspec
 from typing import Optional
+
+import fsspec
+
 from llama_index.storage.docstore.keyval_docstore import KVDocumentStore
-from llama_index.storage.kvstore.simple_kvstore import SimpleKVStore
-from llama_index.storage.kvstore.types import BaseInMemoryKVStore
 from llama_index.storage.docstore.types import (
-    DEFAULT_PERSIST_PATH,
     DEFAULT_PERSIST_DIR,
     DEFAULT_PERSIST_FNAME,
+    DEFAULT_PERSIST_PATH,
 )
+from llama_index.storage.kvstore.simple_kvstore import SimpleKVStore
+from llama_index.storage.kvstore.types import BaseInMemoryKVStore
+from llama_index.utils import concat_dirs
 
 
 class SimpleDocumentStore(KVDocumentStore):
     """Simple Document (Node) store.
 
     An in-memory store for Document and Node objects.
 
     Args:
         simple_kvstore (SimpleKVStore): simple key-value store
-        name_space (str): namespace for the docstore
+        namespace (str): namespace for the docstore
 
     """
 
     def __init__(
         self,
         simple_kvstore: Optional[SimpleKVStore] = None,
-        name_space: Optional[str] = None,
+        namespace: Optional[str] = None,
     ) -> None:
         """Init a SimpleDocumentStore."""
         simple_kvstore = simple_kvstore or SimpleKVStore()
-        super().__init__(simple_kvstore, name_space)
+        super().__init__(simple_kvstore, namespace)
 
     @classmethod
     def from_persist_dir(
         cls,
         persist_dir: str = DEFAULT_PERSIST_DIR,
         namespace: Optional[str] = None,
         fs: Optional[fsspec.AbstractFileSystem] = None,
@@ -43,15 +46,18 @@
         Args:
             persist_dir (str): directory to persist the store
             namespace (Optional[str]): namespace for the docstore
             fs (Optional[fsspec.AbstractFileSystem]): filesystem to use
 
         """
 
-        persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)
+        if fs is not None:
+            persist_path = concat_dirs(persist_dir, DEFAULT_PERSIST_FNAME)
+        else:
+            persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)
         return cls.from_persist_path(persist_path, namespace=namespace, fs=fs)
 
     @classmethod
     def from_persist_path(
         cls,
         persist_path: str,
         namespace: Optional[str] = None,
```

### Comparing `llama_index-0.6.9/llama_index/storage/index_store/keyval_index_store.py` & `llama_index-0.7.0/llama_index/storage/index_store/keyval_index_store.py`

 * *Files 6% similar despite different names*

```diff
@@ -18,16 +18,16 @@
         namespace (str): namespace for the index store
 
     """
 
     def __init__(self, kvstore: BaseKVStore, namespace: Optional[str] = None) -> None:
         """Init a KVIndexStore."""
         self._kvstore = kvstore
-        namespace = namespace or DEFAULT_NAMESPACE
-        self._collection = f"{namespace}/data"
+        self._namespace = namespace or DEFAULT_NAMESPACE
+        self._collection = f"{self._namespace}/data"
 
     def add_index_struct(self, index_struct: IndexStruct) -> None:
         """Add an index struct.
 
         Args:
             index_struct (IndexStruct): index struct
```

### Comparing `llama_index-0.6.9/llama_index/storage/index_store/mongo_index_store.py` & `llama_index-0.7.0/llama_index/storage/index_store/mongo_index_store.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/storage/index_store/simple_index_store.py` & `llama_index-0.7.0/llama_index/storage/index_store/simple_index_store.py`

 * *Files 14% similar despite different names*

```diff
@@ -5,14 +5,15 @@
 from llama_index.storage.kvstore.simple_kvstore import SimpleKVStore
 from llama_index.storage.kvstore.types import BaseInMemoryKVStore
 from llama_index.storage.index_store.types import (
     DEFAULT_PERSIST_DIR,
     DEFAULT_PERSIST_FNAME,
     DEFAULT_PERSIST_PATH,
 )
+from llama_index.utils import concat_dirs
 
 
 class SimpleIndexStore(KVIndexStore):
     """Simple in-memory Index store.
 
     Args:
         simple_kvstore (SimpleKVStore): simple key-value store
@@ -30,15 +31,18 @@
     @classmethod
     def from_persist_dir(
         cls,
         persist_dir: str = DEFAULT_PERSIST_DIR,
         fs: Optional[fsspec.AbstractFileSystem] = None,
     ) -> "SimpleIndexStore":
         """Create a SimpleIndexStore from a persist directory."""
-        persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)
+        if fs is not None:
+            persist_path = concat_dirs(persist_dir, DEFAULT_PERSIST_FNAME)
+        else:
+            persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)
         return cls.from_persist_path(persist_path, fs=fs)
 
     @classmethod
     def from_persist_path(
         cls,
         persist_path: str,
         fs: Optional[fsspec.AbstractFileSystem] = None,
```

### Comparing `llama_index-0.6.9/llama_index/storage/index_store/types.py` & `llama_index-0.7.0/llama_index/storage/index_store/types.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/storage/index_store/utils.py` & `llama_index-0.7.0/llama_index/storage/index_store/utils.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,17 +2,20 @@
 from llama_index.data_structs.data_structs import IndexStruct
 from llama_index.data_structs.registry import INDEX_STRUCT_TYPE_TO_INDEX_STRUCT_CLASS
 
 
 def index_struct_to_json(index_struct: IndexStruct) -> dict:
     index_struct_dict = {
         TYPE_KEY: index_struct.get_type(),
-        DATA_KEY: index_struct.to_dict(),
+        DATA_KEY: index_struct.to_json(),
     }
     return index_struct_dict
 
 
 def json_to_index_struct(struct_dict: dict) -> IndexStruct:
     type = struct_dict[TYPE_KEY]
     data_dict = struct_dict[DATA_KEY]
     cls = INDEX_STRUCT_TYPE_TO_INDEX_STRUCT_CLASS[type]
-    return cls.from_dict(data_dict)
+    try:
+        return cls.from_json(data_dict)
+    except TypeError:
+        return cls.from_dict(data_dict)
```

### Comparing `llama_index-0.6.9/llama_index/storage/kvstore/mongodb_kvstore.py` & `llama_index-0.7.0/llama_index/storage/kvstore/mongodb_kvstore.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/storage/kvstore/s3_kvstore.py` & `llama_index-0.7.0/llama_index/storage/kvstore/s3_kvstore.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/storage/kvstore/simple_kvstore.py` & `llama_index-0.7.0/llama_index/storage/kvstore/simple_kvstore.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/storage/kvstore/types.py` & `llama_index-0.7.0/llama_index/storage/kvstore/types.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/storage/storage_context.py` & `llama_index-0.7.0/llama_index/storage/storage_context.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,125 +1,157 @@
 from dataclasses import dataclass
 from pathlib import Path
 from typing import Optional
 
 import fsspec
 
-from llama_index.constants import DOC_STORE_KEY, INDEX_STORE_KEY, VECTOR_STORE_KEY
+from llama_index.constants import (
+    DOC_STORE_KEY,
+    GRAPH_STORE_KEY,
+    INDEX_STORE_KEY,
+    VECTOR_STORE_KEY,
+)
+from llama_index.graph_stores.simple import DEFAULT_PERSIST_FNAME as GRAPH_STORE_FNAME
+from llama_index.graph_stores.simple import SimpleGraphStore
+from llama_index.graph_stores.types import GraphStore
 from llama_index.storage.docstore.simple_docstore import SimpleDocumentStore
 from llama_index.storage.docstore.types import DEFAULT_PERSIST_FNAME as DOCSTORE_FNAME
 from llama_index.storage.docstore.types import BaseDocumentStore
 from llama_index.storage.index_store.simple_index_store import SimpleIndexStore
 from llama_index.storage.index_store.types import (
     DEFAULT_PERSIST_FNAME as INDEX_STORE_FNAME,
 )
 from llama_index.storage.index_store.types import BaseIndexStore
 from llama_index.vector_stores.simple import DEFAULT_PERSIST_FNAME as VECTOR_STORE_FNAME
 from llama_index.vector_stores.simple import SimpleVectorStore
 from llama_index.vector_stores.types import VectorStore
+from llama_index.utils import concat_dirs
 
 DEFAULT_PERSIST_DIR = "./storage"
 
 
 @dataclass
 class StorageContext:
     """Storage context.
 
     The storage context container is a utility container for storing nodes,
     indices, and vectors. It contains the following:
     - docstore: BaseDocumentStore
     - index_store: BaseIndexStore
     - vector_store: VectorStore
+    - graph_store: GraphStore
 
     """
 
     docstore: BaseDocumentStore
     index_store: BaseIndexStore
     vector_store: VectorStore
+    graph_store: GraphStore
 
     @classmethod
     def from_defaults(
         cls,
         docstore: Optional[BaseDocumentStore] = None,
         index_store: Optional[BaseIndexStore] = None,
         vector_store: Optional[VectorStore] = None,
+        graph_store: Optional[GraphStore] = None,
         persist_dir: Optional[str] = None,
         fs: Optional[fsspec.AbstractFileSystem] = None,
     ) -> "StorageContext":
         """Create a StorageContext from defaults.
 
         Args:
             docstore (Optional[BaseDocumentStore]): document store
             index_store (Optional[BaseIndexStore]): index store
             vector_store (Optional[VectorStore]): vector store
+            graph_store (Optional[GraphStore]): graph store
 
         """
         if persist_dir is None:
             docstore = docstore or SimpleDocumentStore()
             index_store = index_store or SimpleIndexStore()
             vector_store = vector_store or SimpleVectorStore()
+            graph_store = graph_store or SimpleGraphStore()
         else:
             docstore = docstore or SimpleDocumentStore.from_persist_dir(
                 persist_dir, fs=fs
             )
             index_store = index_store or SimpleIndexStore.from_persist_dir(
                 persist_dir, fs=fs
             )
             vector_store = vector_store or SimpleVectorStore.from_persist_dir(
                 persist_dir, fs=fs
             )
+            graph_store = graph_store or SimpleGraphStore.from_persist_dir(
+                persist_dir, fs=fs
+            )
 
-        return cls(docstore, index_store, vector_store)
+        return cls(docstore, index_store, vector_store, graph_store)
 
     def persist(
         self,
         persist_dir: str = DEFAULT_PERSIST_DIR,
         docstore_fname: str = DOCSTORE_FNAME,
         index_store_fname: str = INDEX_STORE_FNAME,
         vector_store_fname: str = VECTOR_STORE_FNAME,
+        graph_store_fname: str = GRAPH_STORE_FNAME,
         fs: Optional[fsspec.AbstractFileSystem] = None,
     ) -> None:
         """Persist the storage context.
 
         Args:
             persist_dir (str): directory to persist the storage context
 
         """
-        docstore_path = str(Path(persist_dir) / docstore_fname)
-        index_store_path = str(Path(persist_dir) / index_store_fname)
-        vector_store_path = str(Path(persist_dir) / vector_store_fname)
+        if fs is not None:
+            docstore_path = concat_dirs(persist_dir, docstore_fname)
+            index_store_path = concat_dirs(persist_dir, index_store_fname)
+            vector_store_path = concat_dirs(persist_dir, vector_store_fname)
+            graph_store_path = concat_dirs(persist_dir, graph_store_fname)
+        else:
+            docstore_path = str(Path(persist_dir) / docstore_fname)
+            index_store_path = str(Path(persist_dir) / index_store_fname)
+            vector_store_path = str(Path(persist_dir) / vector_store_fname)
+            graph_store_path = str(Path(persist_dir) / graph_store_fname)
+
         self.docstore.persist(persist_path=docstore_path, fs=fs)
         self.index_store.persist(persist_path=index_store_path, fs=fs)
         self.vector_store.persist(persist_path=vector_store_path, fs=fs)
+        self.graph_store.persist(persist_path=graph_store_path, fs=fs)
 
     def to_dict(self) -> dict:
         all_simple = (
             isinstance(self.vector_store, SimpleVectorStore)
             and isinstance(self.docstore, SimpleDocumentStore)
             and isinstance(self.index_store, SimpleIndexStore)
+            and isinstance(self.graph_store, SimpleGraphStore)
         )
         if not all_simple:
             raise ValueError(
                 "to_dict only available when using simple doc/index/vector stores"
             )
 
         assert isinstance(self.vector_store, SimpleVectorStore)
         assert isinstance(self.docstore, SimpleDocumentStore)
         assert isinstance(self.index_store, SimpleIndexStore)
+        assert isinstance(self.graph_store, SimpleGraphStore)
 
         return {
             VECTOR_STORE_KEY: self.vector_store.to_dict(),
             DOC_STORE_KEY: self.docstore.to_dict(),
             INDEX_STORE_KEY: self.index_store.to_dict(),
+            GRAPH_STORE_KEY: self.graph_store.to_dict(),
         }
 
     @classmethod
     def from_dict(cls, save_dict: dict) -> "StorageContext":
         """Create a StorageContext from dict."""
         docstore = SimpleDocumentStore.from_dict(save_dict[DOC_STORE_KEY])
         vector_store = SimpleVectorStore.from_dict(save_dict[VECTOR_STORE_KEY])
         index_store = SimpleIndexStore.from_dict(save_dict[INDEX_STORE_KEY])
+        graph_store = SimpleGraphStore.from_dict(save_dict[GRAPH_STORE_KEY])
         return cls(
             docstore=docstore,
             index_store=index_store,
             vector_store=vector_store,
+            graph_store=graph_store,
         )
```

### Comparing `llama_index-0.6.9/llama_index/token_counter/mock_embed_model.py` & `llama_index-0.7.0/llama_index/token_counter/mock_embed_model.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/token_counter/utils.py` & `llama_index-0.7.0/llama_index/token_counter/utils.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/tools/query_engine.py` & `llama_index-0.7.0/llama_index/tools/query_engine.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from typing import Any, Optional, cast
+
 from llama_index.indices.query.base import BaseQueryEngine
+from llama_index.langchain_helpers.agents.tools import IndexToolConfig, LlamaIndexTool
 from llama_index.tools.types import BaseTool, ToolMetadata
 
-
 DEFAULT_NAME = "Query Engine Tool"
 DEFAULT_DESCRIPTION = """Useful for running a natural language query
 against a knowledge base and get back a natural language response.
 """
 
 
 class QueryEngineTool(BaseTool):
@@ -47,7 +48,15 @@
     def metadata(self) -> ToolMetadata:
         return self._metadata
 
     def __call__(self, input: Any) -> Any:
         query_str = cast(str, input)
         response = self._query_engine.query(query_str)
         return str(response)
+
+    def as_langchain_tool(self) -> LlamaIndexTool:
+        tool_config = IndexToolConfig(
+            query_engine=self.query_engine,
+            name=self.metadata.name,
+            description=self.metadata.description,
+        )
+        return LlamaIndexTool.from_tool_config(tool_config=tool_config)
```

### Comparing `llama_index-0.6.9/llama_index/tts/bark.py` & `llama_index-0.7.0/llama_index/tts/bark.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/tts/base.py` & `llama_index-0.7.0/llama_index/tts/base.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/tts/elevenlabs.py` & `llama_index-0.7.0/llama_index/tts/elevenlabs.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/llama_index/utils.py` & `llama_index-0.7.0/llama_index/utils.py`

 * *Files 17% similar despite different names*

```diff
@@ -16,14 +16,15 @@
     Optional,
     Set,
     Type,
     cast,
     Union,
     Iterable,
 )
+import os
 
 
 class GlobalsHelper:
     """Helper to retrieve globals.
 
     Helpful for global caching of certain variables that can be expensive to load.
     (e.g. tokenization)
@@ -33,41 +34,23 @@
     _tokenizer: Optional[Callable[[str], List]] = None
     _stopwords: Optional[List[str]] = None
 
     @property
     def tokenizer(self) -> Callable[[str], List]:
         """Get tokenizer."""
         if self._tokenizer is None:
-            # if python version >= 3.9, then use tiktoken
-            # else use GPT2TokenizerFast
-            if sys.version_info >= (3, 9):
-                tiktoken_import_err = (
-                    "`tiktoken` package not found, please run `pip install tiktoken`"
-                )
-                try:
-                    import tiktoken
-                except ImportError:
-                    raise ImportError(tiktoken_import_err)
-                enc = tiktoken.get_encoding("gpt2")
-                self._tokenizer = cast(Callable[[str], List], enc.encode)
-            else:
-                try:
-                    import transformers
-                except ImportError:
-                    raise ImportError(
-                        "`transformers` package not found, "
-                        "please run `pip install transformers`"
-                    )
-
-                tokenizer = transformers.GPT2TokenizerFast.from_pretrained("gpt2")
-
-                def tokenizer_fn(text: str) -> List:
-                    return tokenizer(text)["input_ids"]
-
-                self._tokenizer = tokenizer_fn
+            tiktoken_import_err = (
+                "`tiktoken` package not found, please run `pip install tiktoken`"
+            )
+            try:
+                import tiktoken
+            except ImportError:
+                raise ImportError(tiktoken_import_err)
+            enc = tiktoken.get_encoding("gpt2")
+            self._tokenizer = cast(Callable[[str], List], enc.encode)
         return self._tokenizer
 
     @property
     def stopwords(self) -> List[str]:
         """Get stopwords."""
         if self._stopwords is None:
             try:
@@ -76,15 +59,16 @@
             except ImportError:
                 raise ImportError(
                     "`nltk` package not found, please run `pip install nltk`"
                 )
             try:
                 nltk.data.find("corpora/stopwords")
             except LookupError:
-                nltk.download("stopwords")
+                nltk_data_dir = os.environ.get("NLTK_DATA", None)
+                nltk.download("stopwords", download_dir=nltk_data_dir)
             self._stopwords = stopwords.words("english")
         return self._stopwords
 
 
 globals_helper = GlobalsHelper()
 
 
@@ -200,7 +184,58 @@
     """
     source_iter = iter(iterable)
     while source_iter:
         b = list(islice(source_iter, size))
         if len(b) == 0:
             break
         yield b
+
+
+def concat_dirs(dir1: str, dir2: str) -> str:
+    """
+    Concat dir1 and dir2 while avoiding backslashes when running on windows.
+    os.path.join(dir1,dir2) will add a backslash before dir2 if dir1 does not
+    end with a slash, so we make sure it does.
+    """
+    dir1 += "/" if dir1[-1] != "/" else ""
+    return os.path.join(dir1, dir2)
+
+
+def get_tqdm_iterable(items: Iterable, show_progress: bool, desc: str) -> Iterable:
+    """
+    Optionally get a tqdm iterable. Ensures tqdm.auto is used.
+    """
+    _iterator = items
+    if show_progress:
+        try:
+            from tqdm.auto import tqdm
+
+            return tqdm(items, desc=desc)
+        except ImportError:
+            pass
+    return _iterator
+
+
+def count_tokens(text: str) -> int:
+    tokens = globals_helper.tokenizer(text)
+    return len(tokens)
+
+
+def get_transformer_tokenizer_fin(model_name: str) -> Callable[[str], List[str]]:
+    """
+    Args:
+        model_name(str): the model name of the tokenizer.
+                        For instance, fxmarty/tiny-llama-fast-tokenizer
+    """
+    try:
+        from transformers import AutoTokenizer
+    except ImportError:
+        raise ValueError(
+            "`transformers` package not found, please run `pip install transformers`"
+        )
+    tokenizer = AutoTokenizer.from_pretrained(model_name)
+    return tokenizer.tokenize
+
+
+def get_large_chinese_tokenizer_fn() -> Callable[[str], List[str]]:
+    # Here gives an example of large-chinese-tokenizer
+    return get_transformer_tokenizer_fin("GanymedeNil/text2vec-large-chinese")
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/__init__.py` & `llama_index-0.7.0/llama_index/vector_stores/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,15 +12,22 @@
     OpensearchVectorClient,
     OpensearchVectorStore,
 )
 from llama_index.vector_stores.pinecone import PineconeVectorStore
 from llama_index.vector_stores.qdrant import QdrantVectorStore
 from llama_index.vector_stores.redis import RedisVectorStore
 from llama_index.vector_stores.simple import SimpleVectorStore
+from llama_index.vector_stores.tair import TairVectorStore
+from llama_index.vector_stores.supabase import SupabaseVectorStore
 from llama_index.vector_stores.weaviate import WeaviateVectorStore
+from llama_index.vector_stores.postgres import PGVectorStore
+from llama_index.vector_stores.docarray import (
+    DocArrayHnswVectorStore,
+    DocArrayInMemoryVectorStore,
+)
 
 __all__ = [
     "SimpleVectorStore",
     "RedisVectorStore",
     "FaissVectorStore",
     "PineconeVectorStore",
     "WeaviateVectorStore",
@@ -30,8 +37,13 @@
     "OpensearchVectorStore",
     "OpensearchVectorClient",
     "ChatGPTRetrievalPluginClient",
     "MilvusVectorStore",
     "DeepLakeVectorStore",
     "MyScaleVectorStore",
     "LanceDBVectorStore",
+    "TairVectorStore",
+    "DocArrayInMemoryVectorStore",
+    "DocArrayHnswVectorStore",
+    "SupabaseVectorStore",
+    "PGVectorStore",
 ]
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/chatgpt_plugin.py` & `llama_index-0.7.0/llama_index/vector_stores/chatgpt_plugin.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,53 +1,53 @@
 """ChatGPT Plugin vector store."""
 
 import os
 from typing import Any, Dict, List, Optional
 
 import requests
 from requests.adapters import HTTPAdapter, Retry
-from tqdm.auto import tqdm
 
-from llama_index.data_structs.node import DocumentRelationship, Node
+from llama_index.schema import MetadataMode, NodeRelationship, RelatedNodeInfo, TextNode
 from llama_index.vector_stores.types import (
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryResult,
 )
+from llama_index.utils import get_tqdm_iterable
 
 
 def convert_docs_to_json(embedding_results: List[NodeWithEmbedding]) -> List[Dict]:
     """Convert docs to JSON."""
     docs = []
     for embedding_result in embedding_results:
         # TODO: add information for other fields as well
         # fields taken from
         # https://rb.gy/nmac9u
         doc_dict = {
             "id": embedding_result.id,
-            "text": embedding_result.node.get_text(),
+            "text": embedding_result.node.get_content(metadata_mode=MetadataMode.NONE),
             # NOTE: this is the doc_id to reference document
             "source_id": embedding_result.ref_doc_id,
             # "url": "...",
             # "created_at": ...,
             # "author": "..."",
         }
-        extra_info = embedding_result.node.extra_info
-        if extra_info is not None:
-            if "source" in extra_info:
-                doc_dict["source"] = extra_info["source"]
-            if "source_id" in extra_info:
-                doc_dict["source_id"] = extra_info["source_id"]
-            if "url" in extra_info:
-                doc_dict["url"] = extra_info["url"]
-            if "created_at" in extra_info:
-                doc_dict["created_at"] = extra_info["created_at"]
-            if "author" in extra_info:
-                doc_dict["author"] = extra_info["author"]
+        metadata = embedding_result.node.metadata
+        if metadata is not None:
+            if "source" in metadata:
+                doc_dict["source"] = metadata["source"]
+            if "source_id" in metadata:
+                doc_dict["source_id"] = metadata["source_id"]
+            if "url" in metadata:
+                doc_dict["url"] = metadata["url"]
+            if "created_at" in metadata:
+                doc_dict["created_at"] = metadata["created_at"]
+            if "author" in metadata:
+                doc_dict["author"] = metadata["author"]
 
         docs.append(doc_dict)
     return docs
 
 
 class ChatGPTRetrievalPluginClient(VectorStore):
     """ChatGPT Retrieval Plugin Client.
@@ -90,31 +90,42 @@
         self,
         embedding_results: List[NodeWithEmbedding],
     ) -> List[str]:
         """Add embedding_results to index."""
         headers = {"Authorization": f"Bearer {self._bearer_token}"}
 
         docs_to_upload = convert_docs_to_json(embedding_results)
-        for i in tqdm(range(0, len(docs_to_upload), self._batch_size)):
+        iterable_docs = get_tqdm_iterable(
+            range(0, len(docs_to_upload), self._batch_size),
+            show_progress=True,
+            desc="Uploading documents",
+        )
+        for i in iterable_docs:
             i_end = min(i + self._batch_size, len(docs_to_upload))
             self._s.post(
                 f"{self._endpoint_url}/upsert",
                 headers=headers,
                 json={"documents": docs_to_upload[i:i_end]},
             )
 
         return [result.id for result in embedding_results]
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document."""
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
+
+        Args:
+            ref_doc_id (str): The doc_id of the document to delete.
+
+        """
         headers = {"Authorization": f"Bearer {self._bearer_token}"}
         self._s.post(
             f"{self._endpoint_url}/delete",
             headers=headers,
-            json={"ids": [doc_id]},
+            json={"ids": [ref_doc_id]},
         )
 
     def query(
         self,
         query: VectorStoreQuery,
         **kwargs: Any,
     ) -> VectorStoreQueryResult:
@@ -136,18 +147,22 @@
         ids = []
         for query_result in res.json()["results"]:
             for result in query_result["results"]:
                 result_id = result["id"]
                 result_txt = result["text"]
                 result_score = result["score"]
                 result_ref_doc_id = result["source_id"]
-                node = Node(
-                    doc_id=result_id,
+                node = TextNode(
+                    id_=result_id,
                     text=result_txt,
-                    relationships={DocumentRelationship.SOURCE: result_ref_doc_id},
+                    relationships={
+                        NodeRelationship.SOURCE: RelatedNodeInfo(
+                            node_id=result_ref_doc_id
+                        )
+                    },
                 )
                 nodes.append(node)
                 similarities.append(result_score)
                 ids.append(result_id)
 
             # NOTE: there should only be one query
             break
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/chroma.py` & `llama_index-0.7.0/llama_index/vector_stores/chroma.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,43 +1,38 @@
 """Chroma vector store."""
 import logging
 import math
-from typing import Any, List, Tuple, cast
+from typing import Any, List, cast
 
-from llama_index.data_structs.node import DocumentRelationship, Node
+from llama_index.schema import TextNode, MetadataMode
 from llama_index.utils import truncate_text
 from llama_index.vector_stores.types import (
     MetadataFilters,
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryResult,
 )
-from llama_index.vector_stores.utils import metadata_dict_to_node, node_to_metadata_dict
+from llama_index.vector_stores.utils import (
+    metadata_dict_to_node,
+    node_to_metadata_dict,
+    legacy_metadata_dict_to_node,
+)
 
 logger = logging.getLogger(__name__)
 
 
 def _to_chroma_filter(standard_filters: MetadataFilters) -> dict:
     """Translate standard metadata filters to Chroma specific spec."""
     filters = {}
     for filter in standard_filters.filters:
         filters[filter.key] = filter.value
     return filters
 
 
-def _legacy_metadata_dict_to_node(metadata: dict) -> Tuple[dict, dict, dict]:
-    extra_info = metadata
-    node_info: dict = {}
-    relationships = {
-        DocumentRelationship.SOURCE: metadata["document_id"],
-    }
-    return extra_info, node_info, relationships
-
-
 class ChromaVectorStore(VectorStore):
     """Chroma vector store.
 
     In this vector store, embeddings are stored within a ChromaDB collection.
 
     During query time, the index uses ChromaDB to query for the top
     k most similar nodes.
@@ -45,14 +40,15 @@
     Args:
         chroma_collection (chromadb.api.models.Collection.Collection):
             ChromaDB collection instance
 
     """
 
     stores_text: bool = True
+    flat_metadata: bool = True
 
     def __init__(self, chroma_collection: Any, **kwargs: Any) -> None:
         """Init params."""
         import_err_msg = (
             "`chromadb` package not found, please run `pip install chromadb`"
         )
         try:
@@ -75,34 +71,41 @@
 
         embeddings = []
         metadatas = []
         ids = []
         documents = []
         for result in embedding_results:
             embeddings.append(result.embedding)
-            metadatas.append(node_to_metadata_dict(result.node))
+            metadatas.append(
+                node_to_metadata_dict(
+                    result.node, remove_text=True, flat_metadata=self.flat_metadata
+                )
+            )
             ids.append(result.id)
-            documents.append(result.node.text or "")
+            documents.append(
+                result.node.get_content(metadata_mode=MetadataMode.NONE) or ""
+            )
 
         self._collection.add(
             embeddings=embeddings,
             ids=ids,
             metadatas=metadatas,
             documents=documents,
         )
         return ids
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document.
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id (str): document id
+            ref_doc_id (str): The doc_id of the document to delete.
 
         """
-        self._collection.delete(where={"document_id": doc_id})
+        self._collection.delete(where={"document_id": ref_doc_id})
 
     @property
     def client(self) -> Any:
         """Return client."""
         return self._collection
 
     def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:
@@ -138,28 +141,31 @@
         for node_id, text, metadata, distance in zip(
             results["ids"][0],
             results["documents"][0],
             results["metadatas"][0],
             results["distances"][0],
         ):
             try:
-                extra_info, node_info, relationships = metadata_dict_to_node(metadata)
+                node = metadata_dict_to_node(metadata)
+                node.set_content(text)
             except Exception:
                 # NOTE: deprecated legacy logic for backward compatibility
-                extra_info, node_info, relationships = _legacy_metadata_dict_to_node(
+                metadata, node_info, relationships = legacy_metadata_dict_to_node(
                     metadata
                 )
 
-            node = Node(
-                doc_id=node_id,
-                text=text,
-                extra_info=extra_info,
-                node_info=node_info,
-                relationships=relationships,
-            )
+                node = TextNode(
+                    text=text,
+                    id_=node_id,
+                    metadata=metadata,
+                    start_char_idx=node_info.get("start", None),
+                    end_char_idx=node_info.get("end", None),
+                    relationships=relationships,
+                )
+
             nodes.append(node)
 
             similarity_score = 1.0 - math.exp(-distance)
             similarities.append(similarity_score)
 
             logger.debug(
                 f"> [Node {node_id}] [Similarity score: {similarity_score}] "
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/deeplake.py` & `llama_index-0.7.0/llama_index/vector_stores/deeplake.py`

 * *Files 11% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 
 """
 import logging
 from typing import Any, Dict, List, Optional, cast
 
 import numpy as np
 
+from llama_index.schema import MetadataMode
 from llama_index.indices.query.embedding_utils import get_top_k_embeddings
 from llama_index.vector_stores.types import (
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryResult,
 )
@@ -59,14 +60,15 @@
 
 
     Returns:
         DeepLakeVectorstore: Vectorstore that supports add, delete, and query.
     """
 
     stores_text: bool = False
+    flat_metadata: bool = False
 
     def __init__(
         self,
         dataset_path: str = "llama_index",
         token: Optional[str] = None,
         read_only: Optional[bool] = False,
         ingestion_batch_size: int = 1024,
@@ -158,18 +160,18 @@
             List[str]: List of ids inserted.
         """
         data_to_injest = []
         ids = []
 
         for result in embedding_results:
             embedding = result.embedding
-            extra_info = result.node.extra_info or {}
-            metadata = {**extra_info, **{"document_id": result.ref_doc_id}}
+            metadata = result.node.metadata or {}
+            metadata = {**metadata, **{"document_id": result.ref_doc_id}}
             id = result.id
-            text = result.node.get_text()
+            text = result.node.get_content(metadata_mode=MetadataMode.NONE)
 
             data_to_injest.append(
                 {
                     "text": text,
                     "metadata": metadata,
                     "ids": id,
                     "embedding": embedding,
@@ -197,22 +199,25 @@
             num_workers=min(self.num_workers, len(batched) // self.num_workers),
         )
 
         self.ds.commit(allow_empty=True)
         self.ds.summary()
         return ids
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete the entities in the dataset
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
+
         Args:
-            id (Optional[str], optional): The id to delete.
+            ref_doc_id (str): The doc_id of the document to delete.
+
         """
         view = None
-        if doc_id:
-            view = self.ds.filter(lambda x: x["ids"].numpy().tolist() == [doc_id])
+        if ref_doc_id:
+            view = self.ds.filter(lambda x: x["ids"].numpy().tolist() == [ref_doc_id])
             ids = list(view.sample_indices)
 
         with self.ds:
             for id in sorted(ids)[::-1]:
                 self.ds.pop(id)
 
             self.ds.commit(f"deleted {len(ids)} samples", allow_empty=True)
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/faiss.py` & `llama_index-0.7.0/llama_index/vector_stores/faiss.py`

 * *Files 3% similar despite different names*

```diff
@@ -135,23 +135,20 @@
 
         dirpath = os.path.dirname(persist_path)
         if not os.path.exists(dirpath):
             os.makedirs(dirpath)
 
         faiss.write_index(self._faiss_index, persist_path)
 
-    def delete(
-        self,
-        doc_id: str,
-        **delete_kwargs: Any,
-    ) -> None:
-        """Delete a document.
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id (str): document id
+            ref_doc_id (str): The doc_id of the document to delete.
 
         """
         raise NotImplementedError("Delete not yet implemented for Faiss index.")
 
     def query(
         self,
         query: VectorStoreQuery,
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/lancedb.py` & `llama_index-0.7.0/llama_index/vector_stores/lancedb.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 """LanceDB vector store."""
 from typing import Any, List, Optional
 
-from llama_index.data_structs.node import DocumentRelationship, Node
+from llama_index.schema import MetadataMode, NodeRelationship, RelatedNodeInfo, TextNode
 from llama_index.vector_stores.types import (
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryResult,
 )
 
@@ -71,31 +71,32 @@
         ids = []
         for result in embedding_results:
             data.append(
                 {
                     "id": result.id,
                     "doc_id": result.ref_doc_id,
                     "vector": result.embedding,
-                    "text": result.node.get_text(),
+                    "text": result.node.get_content(metadata_mode=MetadataMode.NONE),
                 }
             )
             ids.append(result.id)
 
         if self.table_name in self.connection.table_names():
             tbl = self.connection.open_table(self.table_name)
             tbl.add(data)
         else:
             self.connection.create_table(self.table_name, data)
         return ids
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document.
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id (str): document id
+            ref_doc_id (str): The doc_id of the document to delete.
 
         """
         raise NotImplementedError("Delete not yet implemented for LanceDB.")
 
     def query(
         self,
         query: VectorStoreQuery,
@@ -114,19 +115,19 @@
 
         if self.refine_factor is not None:
             lance_query.refine_factor(self.refine_factor)
 
         results = lance_query.to_df()
         nodes = []
         for _, item in results.iterrows():
-            node = Node(
-                doc_id=item.id,
+            node = TextNode(
                 text=item.text,
+                id_=item.id,
                 relationships={
-                    DocumentRelationship.SOURCE: item.doc_id,
+                    NodeRelationship.SOURCE: RelatedNodeInfo(node_id=item.doc_id),
                 },
             )
             nodes.append(node)
 
         return VectorStoreQueryResult(
             nodes=nodes,
             similarities=results["score"].tolist(),
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/metal.py` & `llama_index-0.7.0/llama_index/vector_stores/metal.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,45 +1,37 @@
-import json
 import math
-from typing import Any, Dict, List, Tuple
+from typing import Any, List
 
-from llama_index.data_structs.node import DocumentRelationship, Node
+from llama_index.schema import MetadataMode, TextNode
 from llama_index.vector_stores.types import (
     MetadataFilters,
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryResult,
 )
-from llama_index.vector_stores.utils import metadata_dict_to_node, node_to_metadata_dict
+from llama_index.vector_stores.utils import (
+    metadata_dict_to_node,
+    node_to_metadata_dict,
+    legacy_metadata_dict_to_node,
+)
 
 
 def _to_metal_filters(standard_filters: MetadataFilters) -> list:
     filters = []
     for filter in standard_filters.filters:
         filters.append(
             {
                 "field": filter.key,
                 "value": filter.value,
             }
         )
     return filters
 
 
-def _legacy_metadata_dict_to_node(metadata: Dict[str, Any]) -> Tuple[dict, dict, dict]:
-    if "extra_info" in metadata:
-        extra_info = json.loads(metadata["extra_info"])
-    else:
-        extra_info = {}
-    ref_doc_id = metadata["doc_id"]
-    relationships = {DocumentRelationship.SOURCE: ref_doc_id}
-    node_info: dict = {}
-    return extra_info, node_info, relationships
-
-
 class MetalVectorStore(VectorStore):
     def __init__(
         self,
         api_key: str,
         client_id: str,
         index_id: str,
     ):
@@ -55,14 +47,15 @@
 
         self.api_key = api_key
         self.client_id = client_id
         self.index_id = index_id
 
         self.metal_client = Metal(api_key, client_id, index_id)
         self.stores_text = True
+        self.flat_metadata = False
         self.is_embedding_query = True
 
     def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:
         if query.filters is not None:
             if "filters" in kwargs:
                 raise ValueError(
                     "Cannot specify filter via both query and kwargs. "
@@ -85,30 +78,31 @@
 
         for item in response["data"]:
             text = item["text"]
             id_ = item["id"]
 
             # load additional Node data
             try:
-                extra_info, node_info, relationships = metadata_dict_to_node(
-                    item["metadata"]
-                )
+                node = metadata_dict_to_node(item["metadata"])
+                node.text = text
             except Exception:
                 # NOTE: deprecated legacy logic for backward compatibility
-                extra_info, node_info, relationships = _legacy_metadata_dict_to_node(
+                metadata, node_info, relationships = legacy_metadata_dict_to_node(
                     item["metadata"]
                 )
 
-            node = Node(
-                text=text,
-                doc_id=id_,
-                extra_info=extra_info,
-                node_info=node_info,
-                relationships=relationships,
-            )
+                node = TextNode(
+                    text=text,
+                    id_=id_,
+                    metadata=metadata,
+                    start_char_idx=node_info.get("start", None),
+                    end_char_idx=node_info.get("end", None),
+                    relationships=relationships,
+                )
+
             nodes.append(node)
             ids.append(id_)
 
             similarity_score = 1.0 - math.exp(-item["dist"])
             similarities.append(similarity_score)
 
         return VectorStoreQueryResult(nodes=nodes, similarities=similarities, ids=ids)
@@ -129,33 +123,38 @@
             raise ValueError("metal_client not initialized")
 
         ids = []
         for result in embedding_results:
             ids.append(result.id)
 
             metadata = {}
-            metadata["text"] = result.node.text or ""
+            metadata["text"] = (
+                result.node.get_content(metadata_mode=MetadataMode.NONE) or ""
+            )
 
-            additional_metadata = node_to_metadata_dict(result.node)
+            additional_metadata = node_to_metadata_dict(
+                result.node, remove_text=True, flat_metadata=self.flat_metadata
+            )
             metadata.update(additional_metadata)
 
             payload = {
                 "embedding": result.embedding,
                 "metadata": metadata,
                 "id": result.id,
             }
 
             self.metal_client.index(payload)
 
         return ids
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete nodes from index.
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id (str): document id
+            ref_doc_id (str): The doc_id of the document to delete.
 
         """
         if not self.metal_client:
             raise ValueError("metal_client not initialized")
 
-        self.metal_client.deleteOne(doc_id)
+        self.metal_client.deleteOne(ref_doc_id)
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/milvus.py` & `llama_index-0.7.0/llama_index/vector_stores/milvus.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,22 +3,23 @@
 An index that is built within Milvus.
 
 """
 import logging
 from typing import Any, List, Optional
 from uuid import uuid4
 
-from llama_index.data_structs.node import DocumentRelationship, Node
+from llama_index.schema import MetadataMode, NodeRelationship, RelatedNodeInfo, TextNode
 from llama_index.vector_stores.types import (
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryMode,
     VectorStoreQueryResult,
 )
+from llama_index.vector_stores.utils import node_to_metadata_dict, metadata_dict_to_node
 
 logger = logging.getLogger(__name__)
 
 
 class MilvusVectorStore(VectorStore):
     """The Milvus Vector Store.
 
@@ -52,15 +53,15 @@
             under Debug.
 
     Returns:
         MilvusVectorstore: Vectorstore that supports add, delete, and query.
     """
 
     stores_text: bool = True
-    stores_node: bool = False
+    stores_node: bool = True
 
     def __init__(
         self,
         collection_name: str = "llamalection",
         index_params: Optional[dict] = None,
         search_params: Optional[dict] = None,
         dim: Optional[int] = None,
@@ -212,14 +213,20 @@
                 ),
                 FieldSchema(
                     name="embedding",
                     dtype=DataType.FLOAT_VECTOR,
                     description="The embedding vector",
                     dim=self.dim,
                 ),
+                FieldSchema(
+                    name="node",
+                    dtype=DataType.VARCHAR,
+                    description="The node content",
+                    max_length=65535,
+                ),
             ]
 
             col_schema = CollectionSchema(fields=fields)
             self.collection = Collection(
                 self.collection_name,
                 col_schema,
                 using=self.alias,
@@ -313,54 +320,60 @@
         elif len(embedding_results) == 0:
             return []
 
         ids = []
         doc_ids = []
         texts = []
         embeddings = []
+        nodes = []
 
         # Process that data we are going to insert
         for result in embedding_results:
             ids.append(result.id)
             doc_ids.append(result.ref_doc_id)
-            texts.append(result.node.get_text())
+            texts.append(result.node.get_content(metadata_mode=MetadataMode.NONE))
             embeddings.append(result.embedding)
 
+            # Store node without text
+            metadata = node_to_metadata_dict(result.node, remove_text=True)
+            nodes.append(metadata["_node_content"])
+
         try:
             # Insert the data into milvus
-            self.collection.insert([ids, doc_ids, texts, embeddings])
+            self.collection.insert([ids, doc_ids, texts, embeddings, nodes])
             logger.debug(
                 f"Successfully inserted embeddings into: {self.collection_name} "
                 f"Num Inserted: {len(ids)}"
             )
         except MilvusException as e:
             logger.debug(f"Failed to insert embeddings into: {self.collection_name}")
             raise e
         return ids
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document from Milvus.
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id (str): The document id to delete.
+            ref_doc_id (str): The doc_id of the document to delete.
 
         Raises:
             MilvusException: Failed to delete the doc.
         """
         from pymilvus import MilvusException
 
         if self.collection is None:
             return
 
         # Adds ability for multiple doc delete in future.
         doc_ids: List[str]
-        if type(doc_id) != list:
-            doc_ids = [doc_id]
+        if type(ref_doc_id) != list:
+            doc_ids = [ref_doc_id]
         else:
-            doc_ids = doc_id  # type: ignore
+            doc_ids = ref_doc_id  # type: ignore
 
         try:
             # Begin by querying for the primary keys to delete
             doc_ids = ['"' + entry + '"' for entry in doc_ids]
             entries = self.collection.query(f"doc_id in [{','.join(doc_ids)}]")
             ids = [entry["id"] for entry in entries]
             ids = ['"' + entry + '"' for entry in ids]
@@ -396,38 +409,53 @@
 
         try:
             res = self.collection.search(
                 [query.query_embedding],
                 "embedding",
                 self.search_params,
                 limit=query.similarity_top_k,
-                output_fields=["doc_id", "text"],
+                output_fields=["doc_id", "text", "node"],
                 expr=expr,
             )
             logger.debug(
                 f"Successfully searched embedding in collection: {self.collection_name}"
                 f" Num Results: {len(res[0])}"
             )
-        except MilvusException as e:
+        except MilvusException:
+            # TODO: Legacy support for old dbs
+            res = self.collection.search(
+                [query.query_embedding],
+                "embedding",
+                self.search_params,
+                limit=query.similarity_top_k,
+                output_fields=["doc_id", "text"],
+                expr=expr,
+            )
             logger.debug(
-                f"Unsuccessfully searched embedding in collection: "
-                f"{self.collection_name}"
+                f"Successfully searched embedding in collection: {self.collection_name}"
+                f" Num Results: {len(res[0])}"
             )
-            raise e
 
         nodes = []
         similarities = []
         ids = []
 
         for hit in res[0]:
-            node = Node(
-                doc_id=hit.id,
-                text=hit.entity.get("text"),
-                relationships={
-                    DocumentRelationship.SOURCE: hit.entity.get("doc_id"),
-                },
-            )
+            try:
+                node = metadata_dict_to_node({"_node_content": hit.entity.get("node")})
+                node.text = hit.entity.get("text")
+            except Exception:
+                # TODO: Legacy support for old nodes
+                node = TextNode(
+                    text=hit.entity.get("text"),
+                    id_=hit.id,
+                    relationships={
+                        NodeRelationship.SOURCE: RelatedNodeInfo(
+                            node_id=hit.entity.get("doc_id")
+                        ),
+                    },
+                )
             nodes.append(node)
             similarities.append(hit.score)
             ids.append(hit.id)
 
         return VectorStoreQueryResult(nodes=nodes, similarities=similarities, ids=ids)
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/myscale.py` & `llama_index-0.7.0/llama_index/vector_stores/myscale.py`

 * *Files 10% similar despite different names*

```diff
@@ -3,21 +3,21 @@
 An index that is built on top of an existing MyScale cluster.
 
 """
 import json
 import logging
 from typing import Any, Dict, List, Optional, cast
 
-from llama_index.data_structs.node import DocumentRelationship, Node
 from llama_index.indices.service_context import ServiceContext
 from llama_index.readers.myscale import (
     MyScaleSettings,
     escape_str,
     format_list_to_string,
 )
+from llama_index.schema import MetadataMode, NodeRelationship, RelatedNodeInfo, TextNode
 from llama_index.utils import iter_batch
 from llama_index.vector_stores.types import (
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryResult,
 )
@@ -98,27 +98,29 @@
 
         # schema column name, type, and construct format method
         self.column_config: Dict = {
             "id": {"type": "String", "extract_func": lambda x: x.id},
             "doc_id": {"type": "String", "extract_func": lambda x: x.ref_doc_id},
             "text": {
                 "type": "String",
-                "extract_func": lambda x: escape_str(x.node.text or ""),
+                "extract_func": lambda x: escape_str(
+                    x.node.get_content(metadata_mode=MetadataMode.NONE) or ""
+                ),
             },
             "vector": {
                 "type": "Array(Float32)",
                 "extract_func": lambda x: format_list_to_string(x.embedding),
             },
             "node_info": {
                 "type": "JSON",
                 "extract_func": lambda x: json.dumps(x.node.node_info),
             },
-            "extra_info": {
+            "metadata": {
                 "type": "JSON",
-                "extract_func": lambda x: json.dumps(x.node.extra_info),
+                "extract_func": lambda x: json.dumps(x.node.metadata),
             },
         }
 
         if service_context is not None:
             service_context = cast(ServiceContext, service_context)
             dimension = len(
                 service_context.embed_model.get_query_embedding("try this out")
@@ -190,19 +192,20 @@
 
         for result_batch in iter_batch(embedding_results, self.config.batch_size):
             insert_statement = self._build_insert_statement(values=result_batch)
             self._client.command(insert_statement)
 
         return [result.id for result in embedding_results]
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document.
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id (str): document id
+            ref_doc_id (str): The doc_id of the document to delete.
 
         """
         raise NotImplementedError("Delete not yet implemented for MyScale index.")
 
     def drop(self) -> None:
         """Drop MyScale Index and table"""
         self._client.command(
@@ -233,19 +236,28 @@
             limit=query.similarity_top_k,
         )
 
         nodes = []
         ids = []
         similarities = []
         for r in self._client.query(query_statement).named_results():
-            node = Node(
-                doc_id=r["doc_id"],
+            start_char_idx = None
+            end_char_idx = None
+            if isinstance(r["node_info"], dict):
+                start_char_idx = r["node_info"].get("start", None)
+                end_char_idx = r["node_info"].get("end", None)
+
+            node = TextNode(
+                id_=r["doc_id"],
                 text=r["text"],
-                extra_info=r["extra_info"],
-                node_info=r["node_info"],
-                relationships={DocumentRelationship.SOURCE: r["doc_id"]},
+                metadata=r["metadata"],
+                start_char_idx=start_char_idx,
+                end_char_idx=end_char_idx,
+                relationships={
+                    NodeRelationship.SOURCE: RelatedNodeInfo(node_id=r["doc_id"])
+                },
             )
 
             nodes.append(node)
             similarities.append(r["dist"])
             ids.append(r["id"])
         return VectorStoreQueryResult(nodes=nodes, similarities=similarities, ids=ids)
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/opensearch.py` & `llama_index-0.7.0/llama_index/vector_stores/opensearch.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """Elasticsearch/Opensearch vector store."""
 import json
 from typing import Any, Dict, List, Optional, cast
 
-from llama_index.data_structs import Node
+from llama_index.schema import MetadataMode, TextNode
 from llama_index.vector_stores.types import (
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryResult,
 )
+from llama_index.vector_stores.utils import metadata_dict_to_node, node_to_metadata_dict
 
 
 class OpensearchVectorClient:
     """Object encapsulating an Opensearch index that has vector search enabled.
 
     If the index does not yet exist, it is created during init.
     Therefore, the underlying index is assumed to either:
@@ -36,14 +37,15 @@
     def __init__(
         self,
         endpoint: str,
         index: str,
         dim: int,
         embedding_field: str = "embedding",
         text_field: str = "content",
+        metadata_field: str = "metadata",
         method: Optional[dict] = None,
         auth: Optional[dict] = None,
     ):
         """Init params."""
         if method is None:
             method = {
                 "name": "hnsw",
@@ -68,21 +70,24 @@
                 # when accessing with HTTPS, https://localhost:9200.
                 auth["verify"] = False
             if "basic_auth" not in auth:
                 # 'admin:admin' is the default username/password for the "Open search"
                 # docker image.
                 auth["basic_auth"] = ("admin", "admin")
             self._client = httpx.Client(
-                base_url=endpoint, verify=auth["verify"], auth=auth["basic_auth"]
+                base_url=endpoint,
+                verify=auth["verify"],
+                auth=auth["basic_auth"],
             )
 
         self._endpoint = endpoint
         self._dim = dim
         self._index = index
         self._text_field = text_field
+        self._metadata_field = metadata_field
         # initialize mapping
         idx_conf = {
             "settings": {"index": {"knn": True, "knn.algo_param.ef_search": 100}},
             "mappings": {
                 "properties": {
                     embedding_field: {
                         "type": "knn_vector",
@@ -97,23 +102,30 @@
         assert res.status_code == 200 or res.status_code == 400
 
     def index_results(self, results: List[NodeWithEmbedding]) -> List[str]:
         """Store results in the index."""
         bulk_req: List[Dict[Any, Any]] = []
         for result in results:
             bulk_req.append({"index": {"_index": self._index, "_id": result.id}})
+
+            metadata = node_to_metadata_dict(result.node, remove_text=True)
             bulk_req.append(
                 {
-                    self._text_field: result.node.get_text(),
+                    self._text_field: result.node.get_content(
+                        metadata_mode=MetadataMode.NONE
+                    ),
                     self._embedding_field: result.embedding,
+                    self._metadata_field: metadata,
                 }
             )
         bulk = "\n".join([json.dumps(v) for v in bulk_req]) + "\n"
         res = self._client.post(
-            "/_bulk", headers={"Content-Type": "application/x-ndjson"}, content=bulk
+            "/_bulk",
+            headers={"Content-Type": "application/x-ndjson"},
+            content=bulk,
         )
         assert res.status_code == 200
         assert not res.json()["errors"], "expected no errors while indexing docs"
         return [r.id for r in results]
 
     def delete_doc_id(self, doc_id: str) -> None:
         """Delete a document.
@@ -128,27 +140,54 @@
     ) -> VectorStoreQueryResult:
         """Do approximate knn."""
         res = self._client.post(
             f"{self._index}/_search",
             json={
                 "size": k,
                 "query": {
-                    "knn": {self._embedding_field: {"vector": query_embedding, "k": k}}
+                    "knn": {
+                        self._embedding_field: {
+                            "vector": query_embedding,
+                            "k": k,
+                        }
+                    }
                 },
             },
         )
         nodes = []
         ids = []
         scores = []
         for hit in res.json()["hits"]["hits"]:
             source = hit["_source"]
+            node_id = hit["_id"]
             text = source[self._text_field]
-            doc_id = hit["_id"]
-            node = Node(text=text, extra_info=source, doc_id=doc_id)
-            ids.append(doc_id)
+            metadata = source.get(self._metadata_field, None)
+
+            try:
+                node = metadata_dict_to_node(metadata)
+                node.text = text
+            except Exception:
+                # TODO: Legacy support for old nodes
+                node_info = source.get("node_info")
+                relationships = source.get("relationships")
+                start_char_idx = None
+                end_char_idx = None
+                if isinstance(node_info, dict):
+                    start_char_idx = node_info.get("start", None)
+                    end_char_idx = node_info.get("end", None)
+
+                node = TextNode(
+                    text=text,
+                    metadata=metadata,
+                    id_=node_id,
+                    start_char_idx=start_char_idx,
+                    end_char_idx=end_char_idx,
+                    relationships=relationships,
+                )
+            ids.append(node_id)
             nodes.append(node)
             scores.append(hit["_score"])
         return VectorStoreQueryResult(nodes=nodes, ids=ids, similarities=scores)
 
 
 class OpensearchVectorStore(VectorStore):
     """Elasticsearch/Opensearch vector store.
@@ -187,22 +226,23 @@
         Args
             embedding_results: List[NodeWithEmbedding]: list of embedding results
 
         """
         self._client.index_results(embedding_results)
         return [result.id for result in embedding_results]
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document.
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id (str): document id
+            ref_doc_id (str): The doc_id of the document to delete.
 
         """
-        self._client.delete_doc_id(doc_id)
+        self._client.delete_doc_id(ref_doc_id)
 
     def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:
         """Query index for top k most similar nodes.
 
         Args:
             query_embedding (List[float]): query embedding
             similarity_top_k (int): top k most similar nodes
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/pinecone.py` & `llama_index-0.7.0/llama_index/vector_stores/pinecone.py`

 * *Files 9% similar despite different names*

```diff
@@ -4,39 +4,40 @@
 
 """
 
 import logging
 import os
 from collections import Counter
 from functools import partial
-from typing import Any, Callable, Dict, List, Optional, Tuple, cast
+from typing import Any, Callable, Dict, List, Optional, cast
 
-from llama_index.data_structs.node import DocumentRelationship, Node
+from llama_index.schema import MetadataMode, TextNode
 from llama_index.vector_stores.types import (
     MetadataFilters,
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryMode,
     VectorStoreQueryResult,
 )
-from llama_index.vector_stores.utils import metadata_dict_to_node, node_to_metadata_dict
+from llama_index.vector_stores.utils import (
+    DEFAULT_TEXT_KEY,
+    metadata_dict_to_node,
+    node_to_metadata_dict,
+    legacy_metadata_dict_to_node,
+)
 
-_logger = logging.getLogger(__name__)
+ID_KEY = "id"
+VECTOR_KEY = "values"
+SPARSE_VECTOR_KEY = "sparse_values"
+METADATA_KEY = "metadata"
 
+DEFAULT_BATCH_SIZE = 100
 
-def _get_node_info_from_metadata(
-    metadata: Dict[str, Any], field_prefix: str
-) -> Dict[str, Any]:
-    """Get node extra info from metadata."""
-    node_extra_info = {}
-    for key, value in metadata.items():
-        if key.startswith(field_prefix + "_"):
-            node_extra_info[key.replace(field_prefix + "_", "")] = value
-    return node_extra_info
+_logger = logging.getLogger(__name__)
 
 
 def build_dict(input_batch: List[List[int]]) -> List[Dict[str, Any]]:
     """Build a list of sparse dictionaries from a batch of input_ids.
 
     NOTE: taken from https://www.pinecone.io/learn/hybrid-search-intro/.
 
@@ -95,22 +96,14 @@
     """Convert from standard dataclass to pinecone filter dict."""
     filters = {}
     for filter in standard_filters.filters:
         filters[filter.key] = filter.value
     return filters
 
 
-def _legacy_metadata_dict_to_node(metadata: Dict[str, Any]) -> Tuple[dict, dict, dict]:
-    extra_info = _get_node_info_from_metadata(metadata, "extra_info")
-    node_info = _get_node_info_from_metadata(metadata, "node_info")
-    doc_id = metadata["doc_id"]
-    relationships = {DocumentRelationship.SOURCE: doc_id}
-    return extra_info, node_info, relationships
-
-
 class PineconeVectorStore(VectorStore):
     """Pinecone Vector Store.
 
     In this vector store, embeddings and docs are stored within a
     Pinecone index.
 
     During query time, the index uses Pinecone to query for the top
@@ -121,24 +114,27 @@
         insert_kwargs (Optional[Dict]): insert kwargs during `upsert` call.
         add_sparse_vector (bool): whether to add sparse vector to index.
         tokenizer (Optional[Callable]): tokenizer to use to generate sparse
 
     """
 
     stores_text: bool = True
+    flat_metadata: bool = True
 
     def __init__(
         self,
         pinecone_index: Optional[Any] = None,
         index_name: Optional[str] = None,
         environment: Optional[str] = None,
         namespace: Optional[str] = None,
         insert_kwargs: Optional[Dict] = None,
         add_sparse_vector: bool = False,
         tokenizer: Optional[Callable] = None,
+        text_key: str = DEFAULT_TEXT_KEY,
+        batch_size: int = DEFAULT_BATCH_SIZE,
         **kwargs: Any,
     ) -> None:
         """Initialize params."""
         import_err_msg = (
             "`pinecone` package not found, please run `pip install pinecone-client`"
         )
         try:
@@ -168,63 +164,73 @@
 
         self._insert_kwargs = insert_kwargs or {}
 
         self._add_sparse_vector = add_sparse_vector
         if tokenizer is None:
             tokenizer = get_default_tokenizer()
         self._tokenizer = tokenizer
+        self._text_key = text_key
+        self._batch_size = batch_size
 
     def add(
         self,
         embedding_results: List[NodeWithEmbedding],
     ) -> List[str]:
         """Add embedding results to index.
 
         Args
             embedding_results: List[NodeWithEmbedding]: list of embedding results
 
         """
         ids = []
+        entries = []
         for result in embedding_results:
             node_id = result.id
             node = result.node
 
-            metadata = {
-                "text": node.text or "",
-                "id": node_id,
-            }
-
-            additional_metadata = node_to_metadata_dict(node)
-            metadata.update(additional_metadata)
+            metadata = node_to_metadata_dict(
+                node, remove_text=False, flat_metadata=self.flat_metadata
+            )
 
             entry = {
-                "id": node_id,
-                "values": result.embedding,
-                "metadata": metadata,
+                ID_KEY: node_id,
+                VECTOR_KEY: result.embedding,
+                METADATA_KEY: metadata,
             }
             if self._add_sparse_vector:
                 sparse_vector = generate_sparse_vectors(
-                    [node.get_text()], self._tokenizer
+                    [node.get_content(metadata_mode=MetadataMode.EMBED)],
+                    self._tokenizer,
                 )[0]
-                entry.update({"sparse_values": sparse_vector})
-            self._pinecone_index.upsert(
-                [entry], namespace=self._namespace, **self._insert_kwargs
-            )
+                entry[SPARSE_VECTOR_KEY] = sparse_vector
+
             ids.append(node_id)
+            entries.append(entry)
+        self._pinecone_index.upsert(
+            entries,
+            namespace=self._namespace,
+            batch_size=self._batch_size,
+            **self._insert_kwargs,
+        )
         return ids
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document.
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id (str): document id
+            ref_doc_id (str): The doc_id of the document to delete.
 
         """
         # delete by filtering on the doc_id metadata
-        self._pinecone_index.delete(filter={"doc_id": {"$eq": doc_id}}, **delete_kwargs)
+        self._pinecone_index.delete(
+            filter={"doc_id": {"$eq": ref_doc_id}},
+            namespace=self._namespace,
+            **delete_kwargs,
+        )
 
     @property
     def client(self) -> Any:
         """Return Pinecone client."""
         return self._pinecone_index
 
     def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:
@@ -278,36 +284,35 @@
             **kwargs,
         )
 
         top_k_nodes = []
         top_k_ids = []
         top_k_scores = []
         for match in response.matches:
-            text = match.metadata["text"]
-            id = match.metadata["id"]
             try:
-                extra_info, node_info, relationships = metadata_dict_to_node(
-                    match.metadata
-                )
+                node = metadata_dict_to_node(match.metadata)
             except Exception:
+                # NOTE: deprecated legacy logic for backward compatibility
                 _logger.debug(
                     "Failed to parse Node metadata, fallback to legacy logic."
                 )
-                # NOTE: deprecated legacy logic for backward compatibility
-                extra_info, node_info, relationships = _legacy_metadata_dict_to_node(
-                    match.metadata
+                metadata, node_info, relationships = legacy_metadata_dict_to_node(
+                    match.metadata, text_key=self._text_key
                 )
 
-            node = Node(
-                text=text,
-                doc_id=id,
-                extra_info=extra_info,
-                node_info=node_info,
-                relationships=relationships,
-            )
+                text = match.metadata[self._text_key]
+                id = match.id
+                node = TextNode(
+                    text=text,
+                    id_=id,
+                    metadata=metadata,
+                    start_char_idx=node_info.get("start", None),
+                    end_char_idx=node_info.get("end", None),
+                    relationships=relationships,
+                )
             top_k_ids.append(match.id)
             top_k_nodes.append(node)
             top_k_scores.append(match.score)
 
         return VectorStoreQueryResult(
             nodes=top_k_nodes, similarities=top_k_scores, ids=top_k_ids
         )
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/qdrant.py` & `llama_index-0.7.0/llama_index/vector_stores/qdrant.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,37 +1,32 @@
 """Qdrant vector store index.
 
 An index that is built on top of an existing Qdrant collection.
 
 """
 import logging
-from typing import Any, List, Optional, Tuple, cast
+from typing import Any, List, Optional, cast
 
-from llama_index.data_structs.node import DocumentRelationship, Node
+from llama_index.schema import TextNode
 from llama_index.utils import iter_batch
 from llama_index.vector_stores.types import (
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryResult,
 )
-from llama_index.vector_stores.utils import metadata_dict_to_node, node_to_metadata_dict
+from llama_index.vector_stores.utils import (
+    metadata_dict_to_node,
+    node_to_metadata_dict,
+    legacy_metadata_dict_to_node,
+)
 
 logger = logging.getLogger(__name__)
 
 
-def _legacy_metadata_dict_to_node(payload: Any) -> Tuple[dict, dict, dict]:
-    extra_info = payload.get("extra_info", {})
-    relationships = {
-        DocumentRelationship.SOURCE: payload.get("doc_id", "None"),
-    }
-    node_info: dict = {}
-    return extra_info, node_info, relationships
-
-
 class QdrantVectorStore(VectorStore):
     """Qdrant Vector Store.
 
     In this vector store, embeddings and docs are stored within a
     Qdrant collection.
 
     During query time, the index uses Qdrant to query for the top
@@ -39,14 +34,15 @@
 
     Args:
         collection_name: (str): name of the Qdrant collection
         client (Optional[Any]): QdrantClient instance from `qdrant-client` package
     """
 
     stores_text: bool = True
+    flat_metadata: bool = False
 
     def __init__(
         self, collection_name: str, client: Optional[Any] = None, **kwargs: Any
     ) -> None:
         """Init params."""
         import_err_msg = (
             "`qdrant-client` package not found, please run `pip install qdrant-client`"
@@ -83,51 +79,52 @@
         ids = []
         for result_batch in iter_batch(embedding_results, self._batch_size):
             node_ids = []
             vectors = []
             payloads = []
             for result in result_batch:
                 assert isinstance(result, NodeWithEmbedding)
+                assert isinstance(result.node, TextNode)
                 node_ids.append(result.id)
                 vectors.append(result.embedding)
                 node = result.node
 
-                metadata = {}
-                metadata["text"] = node.text or ""
-                additional_metadata = node_to_metadata_dict(node)
-                metadata.update(additional_metadata)
+                metadata = node_to_metadata_dict(
+                    node, remove_text=False, flat_metadata=self.flat_metadata
+                )
 
                 payloads.append(metadata)
 
             self._client.upsert(
                 collection_name=self._collection_name,
                 points=rest.Batch.construct(
                     ids=node_ids,
                     vectors=vectors,
                     payloads=payloads,
                 ),
             )
             ids.extend(node_ids)
         return ids
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document.
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id: (str): document id
+            ref_doc_id (str): The doc_id of the document to delete.
 
         """
         from qdrant_client.http import models as rest
 
         self._client.delete(
             collection_name=self._collection_name,
             points_selector=rest.Filter(
                 must=[
                     rest.FieldCondition(
-                        key="doc_id", match=rest.MatchValue(value=doc_id)
+                        key="doc_id", match=rest.MatchValue(value=ref_doc_id)
                     )
                 ]
             ),
         )
 
     @property
     def client(self) -> Any:
@@ -183,47 +180,86 @@
 
         nodes = []
         similarities = []
         ids = []
         for point in response:
             payload = cast(Payload, point.payload)
             try:
-                extra_info, node_info, relationships = metadata_dict_to_node(payload)
+                node = metadata_dict_to_node(payload)
             except Exception:
+                # NOTE: deprecated legacy logic for backward compatibility
                 logger.debug("Failed to parse Node metadata, fallback to legacy logic.")
-                extra_info, node_info, relationships = _legacy_metadata_dict_to_node(
+                metadata, node_info, relationships = legacy_metadata_dict_to_node(
                     payload
                 )
 
-            node = Node(
-                doc_id=str(point.id),
-                text=payload.get("text"),
-                extra_info=extra_info,
-                node_info=node_info,
-                relationships=relationships,
-            )
+                node = TextNode(
+                    id_=str(point.id),
+                    text=payload.get("text"),
+                    metadata=metadata,
+                    start_char_idx=node_info.get("start", None),
+                    end_char_idx=node_info.get("end", None),
+                    relationships=relationships,
+                )
             nodes.append(node)
             similarities.append(point.score)
             ids.append(str(point.id))
 
         return VectorStoreQueryResult(nodes=nodes, similarities=similarities, ids=ids)
 
     def _build_query_filter(self, query: VectorStoreQuery) -> Optional[Any]:
         if not query.doc_ids and not query.query_str:
             return None
 
-        from qdrant_client.http.models import FieldCondition, Filter, MatchAny
+        from qdrant_client.http.models import (
+            FieldCondition,
+            Filter,
+            MatchAny,
+            MatchValue,
+            Range,
+        )
 
         must_conditions = []
 
         if query.doc_ids:
             must_conditions.append(
                 FieldCondition(
                     key="doc_id",
-                    match=MatchAny(any=[doc_id for doc_id in query.doc_ids]),
+                    match=MatchAny(any=query.doc_ids),
+                )
+            )
+
+        if query.node_ids:
+            must_conditions.append(
+                FieldCondition(
+                    key="id",
+                    match=MatchAny(any=query.node_ids),
                 )
             )
-        # TODO: implement this
-        if query.filters is not None:
-            raise ValueError("Metadata filters not implemented for Qdrant yet.")
+
+        # Qdrant does not use the query.query_str property for the filtering. Full-text
+        # filtering cannot handle longer queries and can effectively filter our all the
+        # nodes. See: https://github.com/jerryjliu/llama_index/pull/1181
+
+        if query.filters is None:
+            return Filter(must=must_conditions)
+
+        for subfilter in query.filters.filters:
+            if isinstance(subfilter.value, float):
+                must_conditions.append(
+                    FieldCondition(
+                        key=subfilter.key,
+                        range=Range(
+                            gte=subfilter.value,
+                            lte=subfilter.value,
+                        ),
+                    )
+                )
+            else:
+                must_conditions.append(
+                    FieldCondition(
+                        key=subfilter.key,
+                        match=MatchValue(value=subfilter.value),
+                    )
+                )
 
         return Filter(must=must_conditions)
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/redis.py` & `llama_index-0.7.0/llama_index/vector_stores/redis.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,50 +1,55 @@
 """Redis Vector store index.
 
 An index that that is built on top of an existing vector store.
 """
 import logging
-import fsspec
 from typing import TYPE_CHECKING, Any, Dict, List, Optional
 
-from llama_index.data_structs.node import DocumentRelationship, Node
+import fsspec
+
 from llama_index.readers.redis.utils import (
     TokenEscaper,
     array_to_buffer,
     check_redis_modules_exist,
     convert_bytes,
     get_redis_query,
 )
+from llama_index.schema import MetadataMode, NodeRelationship, RelatedNodeInfo, TextNode
 from llama_index.vector_stores.types import (
+    MetadataFilters,
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryResult,
 )
-from llama_index.vector_stores.utils import node_to_metadata_dict
+from llama_index.vector_stores.utils import node_to_metadata_dict, metadata_dict_to_node
 
 _logger = logging.getLogger(__name__)
 
 
 if TYPE_CHECKING:
     from redis.client import Redis as RedisType
     from redis.commands.search.field import VectorField
 
 
 class RedisVectorStore(VectorStore):
     stores_text = True
     stores_node = True
+    flat_metadata = False
 
     tokenizer = TokenEscaper()
 
     def __init__(
         self,
         index_name: str,
         index_prefix: str = "llama_index",
+        prefix_ending: str = "/vector",
         index_args: Optional[Dict[str, Any]] = None,
+        metadata_fields: Optional[List[str]] = None,
         redis_url: str = "redis://localhost:6379",
         overwrite: bool = False,
         **kwargs: Any,
     ) -> None:
         """Initialize RedisVectorStore.
 
         For index arguments that can be passed to RediSearch, see
@@ -54,15 +59,22 @@
         are two available index types
             - FLAT: a flat index that uses brute force search
             - HNSW: a hierarchical navigable small world graph index
 
         Args:
             index_name (str): Name of the index.
             index_prefix (str): Prefix for the index. Defaults to "llama_index".
+                The actual prefix used by Redis will be
+                "{index_prefix}{prefix_ending}".
+            prefix_ending (str): Prefix ending for the index. Be careful when
+                changing this: https://github.com/jerryjliu/llama_index/pull/6665.
+                Defaults to "/vector".
             index_args (Dict[str, Any]): Arguments for the index. Defaults to None.
+            metadata_fields (List[str]): List of metadata fields to store in the index
+                (only supports TAG fields).
             redis_url (str): URL for the redis instance.
                 Defaults to "redis://localhost:6379".
             overwrite (bool): Whether to overwrite the index if it already exists.
                 Defaults to False.
             kwargs (Any): Additional arguments to pass to the redis client.
 
         Raises:
@@ -93,17 +105,18 @@
             self._redis_client = redis.from_url(redis_url, **kwargs)
             # check if redis has redisearch module installed
             check_redis_modules_exist(self._redis_client)
         except ValueError as e:
             raise ValueError(f"Redis failed to connect: {e}")
 
         # index identifiers
-        self._prefix = index_prefix
+        self._prefix = index_prefix + prefix_ending
         self._index_name = index_name
         self._index_args = index_args if index_args is not None else {}
+        self._metadata_fields = metadata_fields if metadata_fields is not None else []
         self._overwrite = overwrite
         self._vector_field = str(self._index_args.get("vector_field", "vector"))
         self._vector_key = str(self._index_args.get("vector_key", "vector"))
 
     @property
     def client(self) -> "RedisType":
         """Return the redis client instance"""
@@ -139,44 +152,47 @@
             self._create_index()
 
         ids = []
         for result in embedding_results:
             mapping = {
                 "id": result.id,
                 "doc_id": result.ref_doc_id,
-                "text": result.node.get_text(),
+                "text": result.node.get_content(metadata_mode=MetadataMode.NONE),
                 self._vector_key: array_to_buffer(result.embedding),
             }
-            additional_metadata = node_to_metadata_dict(result.node)
+            additional_metadata = node_to_metadata_dict(
+                result.node, remove_text=True, flat_metadata=self.flat_metadata
+            )
             mapping.update(additional_metadata)
 
             ids.append(result.id)
             key = "_".join([self._prefix, str(result.id)])
             self._redis_client.hset(key, mapping=mapping)  # type: ignore
 
         _logger.info(f"Added {len(ids)} documents to index {self._index_name}")
         return ids
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a specific document from the index by doc_id
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
 
         Args:
-            doc_id (str): The doc_id of the document to delete.
-            delete_kwargs (Any): Additional arguments to pass to the delete method.
+            ref_doc_id (str): The doc_id of the document to delete.
 
         """
         # use tokenizer to escape dashes in query
-        query_str = "@doc_id:{%s}" % self.tokenizer.escape(doc_id)
+        query_str = "@doc_id:{%s}" % self.tokenizer.escape(ref_doc_id)
         # find all documents that match a doc_id
         results = self._redis_client.ft(self._index_name).search(query_str)
         if len(results.docs) == 0:
             # don't raise an error but warn the user that document wasn't found
             # could be a result of eviction policy
             _logger.warning(
-                f"Document with doc_id {doc_id} not found in index {self._index_name}"
+                f"Document with doc_id {ref_doc_id} not found "
+                f"in index {self._index_name}"
             )
             return
 
         for doc in results.docs:
             self._redis_client.delete(doc.id)
         _logger.info(
             f"Deleted {len(results.docs)} documents from index {self._index_name}"
@@ -196,29 +212,39 @@
         Returns:
             VectorStoreQueryResult: query result
 
         Raises:
             ValueError: If query.query_embedding is None.
             redis.exceptions.RedisError: If there is an error querying the index.
             redis.exceptions.TimeoutError: If there is a timeout querying the index.
+            ValueError: If no documents are found when querying the index.
         """
         from redis.exceptions import RedisError
         from redis.exceptions import TimeoutError as RedisTimeoutError
 
-        # TODO: implement this
-        if query.filters is not None:
-            raise ValueError("Metadata filters not implemented for Redis yet.")
+        return_fields = [
+            "id",
+            "doc_id",
+            "text",
+            self._vector_key,
+            "vector_score",
+            "_node_content",
+        ]
 
-        return_fields = ["id", "doc_id", "text", self._vector_key, "vector_score"]
+        filters = _to_redis_filters(query.filters) if query.filters is not None else "*"
+
+        _logger.info(f"Using filters: {filters}")
 
         redis_query = get_redis_query(
             return_fields=return_fields,
             top_k=query.similarity_top_k,
             vector_field=self._vector_field,
+            filters=filters,
         )
+
         if not query.query_embedding:
             raise ValueError("Query embedding is required for querying.")
 
         query_params = {
             "vector": array_to_buffer(query.query_embedding),
         }
         _logger.info(f"Querying index {self._index_name}")
@@ -230,25 +256,39 @@
         except RedisTimeoutError as e:
             _logger.error(f"Query timed out on {self._index_name}: {e}")
             raise e
         except RedisError as e:
             _logger.error(f"Error querying {self._index_name}: {e}")
             raise e
 
+        if len(results.docs) == 0:
+            raise ValueError(
+                f"No docs found on index '{self._index_name}' with "
+                f"prefix '{self._prefix}' and filters '{filters}'. "
+                "* Did you originally create the index with a different prefix? "
+                "* Did you index your metadata fields when you created the index?"
+            )
+
         ids = []
         nodes = []
         scores = []
         for doc in results.docs:
-            # TODO: properly retrieve metadata
-            node = Node(
-                text=doc.text,
-                doc_id=doc.id,
-                embedding=None,
-                relationships={DocumentRelationship.SOURCE: doc.doc_id},
-            )
+            try:
+                node = metadata_dict_to_node({"_node_content": doc._node_content})
+                node.text = doc.text
+            except Exception:
+                # TODO: Legacy support for old metadata format
+                node = TextNode(
+                    text=doc.text,
+                    id_=doc.id,
+                    embedding=None,
+                    relationships={
+                        NodeRelationship.SOURCE: RelatedNodeInfo(node_id=doc.doc_id)
+                    },
+                )
             ids.append(doc.id)
             nodes.append(node)
             scores.append(1 - float(doc.vector_score))
         _logger.info(f"Found {len(nodes)} results for query with id {ids}")
 
         return VectorStoreQueryResult(nodes=nodes, ids=ids, similarities=scores)
 
@@ -293,18 +333,26 @@
         default_fields = [
             TextField("text", weight=1.0),
             TagField("doc_id", sortable=False),
             TagField("id", sortable=False),
         ]
         # add vector field to list of index fields. Create lazily to allow user
         # to specify index and search attributes in creation.
+
         fields = default_fields + [
             self._create_vector_field(self._vector_field, **self._index_args)
         ]
 
+        # add metadata fields to list of index fields or we won't be able to search them
+        for metadata_field in self._metadata_fields:
+            # TODO: allow addition of text fields as metadata
+            # TODO: make sure we're preventing overwriting other keys (e.g. text,
+            #   doc_id, id, and other vector fields)
+            fields.append(TagField(metadata_field, sortable=False))
+
         _logger.info(f"Creating index {self._index_name}")
         self._redis_client.ft(self._index_name).create_index(
             fields=fields,
             definition=IndexDefinition(
                 prefix=[self._prefix], index_type=IndexType.HASH
             ),  # TODO support JSON
         )
@@ -382,17 +430,22 @@
                 )
         except DataError as e:
             raise ValueError(
                 f"Failed to create Redis index vector field with error: {e}"
             )
 
 
-def cast_metadata_types(mapping: Optional[Dict[str, Any]]) -> Dict[str, str]:
-    metadata = {}
-    if mapping:
-        for key, value in mapping.items():
-            try:
-                metadata[str(key)] = str(value)
-            except (TypeError, ValueError) as e:
-                # warn the user and continue
-                _logger.warning("Failed to cast metadata to string", e)
-    return metadata
+# currently only supports exact tag match - {} denotes a tag
+# must create the index with the correct metadata field before using a field as a
+#   filter, or it will return no results
+def _to_redis_filters(metadata_filters: MetadataFilters) -> str:
+    tokenizer = TokenEscaper()
+
+    filter_strings = []
+    for filter in metadata_filters.filters:
+        # adds quotes around the value to ensure that the filter is treated as an
+        #   exact match
+        filter_string = "@%s:{%s}" % (filter.key, tokenizer.escape(str(filter.value)))
+        filter_strings.append(filter_string)
+
+    joined_filter_strings = " & ".join(filter_strings)
+    return f"({joined_filter_strings})"
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/registry.py` & `llama_index-0.7.0/llama_index/vector_stores/registry.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 from llama_index.vector_stores.milvus import MilvusVectorStore
 from llama_index.vector_stores.myscale import MyScaleVectorStore
 from llama_index.vector_stores.opensearch import OpensearchVectorStore
 from llama_index.vector_stores.pinecone import PineconeVectorStore
 from llama_index.vector_stores.qdrant import QdrantVectorStore
 from llama_index.vector_stores.redis import RedisVectorStore
 from llama_index.vector_stores.simple import SimpleVectorStore
+from llama_index.vector_stores.supabase import SupabaseVectorStore
 from llama_index.vector_stores.types import VectorStore
 from llama_index.vector_stores.weaviate import WeaviateVectorStore
 
 
 class VectorStoreType(str, Enum):
     SIMPLE = "simple"
     REDIS = "redis"
@@ -27,22 +28,24 @@
     FAISS = "faiss"
     CHROMA = "chroma"
     CHATGPT_PLUGIN = "chatgpt_plugin"
     LANCEDB = "lancedb"
     MILVUS = "milvus"
     DEEPLAKE = "deeplake"
     MYSCALE = "myscale"
+    SUPABASE = "supabase"
 
 
 VECTOR_STORE_TYPE_TO_VECTOR_STORE_CLASS: Dict[VectorStoreType, Type[VectorStore]] = {
     VectorStoreType.SIMPLE: SimpleVectorStore,
     VectorStoreType.REDIS: RedisVectorStore,
     VectorStoreType.WEAVIATE: WeaviateVectorStore,
     VectorStoreType.QDRANT: QdrantVectorStore,
     VectorStoreType.LANCEDB: LanceDBVectorStore,
+    VectorStoreType.SUPABASE: SupabaseVectorStore,
     VectorStoreType.MILVUS: MilvusVectorStore,
     VectorStoreType.PINECONE: PineconeVectorStore,
     VectorStoreType.OPENSEARCH: OpensearchVectorStore,
     VectorStoreType.FAISS: FaissVectorStore,
     VectorStoreType.CHROMA: ChromaVectorStore,
     VectorStoreType.CHATGPT_PLUGIN: ChatGPTRetrievalPluginClient,
     VectorStoreType.DEEPLAKE: DeepLakeVectorStore,
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/simple.py` & `llama_index-0.7.0/llama_index/vector_stores/simple.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,53 +1,58 @@
 """Simple vector store index."""
 
 import json
 import logging
 import os
-import fsspec
 from dataclasses import dataclass, field
 from typing import Any, Dict, List, Optional, cast
 
+import fsspec
 from dataclasses_json import DataClassJsonMixin
 
 from llama_index.indices.query.embedding_utils import (
     get_top_k_embeddings,
     get_top_k_embeddings_learner,
+    get_top_k_mmr_embeddings,
 )
 from llama_index.vector_stores.types import (
     DEFAULT_PERSIST_DIR,
     DEFAULT_PERSIST_FNAME,
     NodeWithEmbedding,
     VectorStore,
     VectorStoreQuery,
     VectorStoreQueryMode,
     VectorStoreQueryResult,
 )
+from llama_index.utils import concat_dirs
 
 logger = logging.getLogger(__name__)
 
 LEARNER_MODES = {
     VectorStoreQueryMode.SVM,
     VectorStoreQueryMode.LINEAR_REGRESSION,
     VectorStoreQueryMode.LOGISTIC_REGRESSION,
 }
 
+MMR_MODE = VectorStoreQueryMode.MMR
+
 
 @dataclass
 class SimpleVectorStoreData(DataClassJsonMixin):
     """Simple Vector Store Data container.
 
     Args:
-        embedding_dict (Optional[dict]): dict mapping doc_ids to embeddings.
-        text_id_to_doc_id (Optional[dict]): dict mapping text_ids to doc_ids.
+        embedding_dict (Optional[dict]): dict mapping node_ids to embeddings.
+        text_id_to_ref_doc_id (Optional[dict]):
+            dict mapping text_ids/node_ids to ref_doc_ids.
 
     """
 
     embedding_dict: Dict[str, List[float]] = field(default_factory=dict)
-    text_id_to_doc_id: Dict[str, str] = field(default_factory=dict)
+    text_id_to_ref_doc_id: Dict[str, str] = field(default_factory=dict)
 
 
 class SimpleVectorStore(VectorStore):
     """Simple Vector Store.
 
     In this vector store, embeddings are stored within a simple, in-memory dictionary.
 
@@ -72,15 +77,18 @@
     @classmethod
     def from_persist_dir(
         cls,
         persist_dir: str = DEFAULT_PERSIST_DIR,
         fs: Optional[fsspec.AbstractFileSystem] = None,
     ) -> "SimpleVectorStore":
         """Load from persist dir."""
-        persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)
+        if fs is not None:
+            persist_path = concat_dirs(persist_dir, DEFAULT_PERSIST_FNAME)
+        else:
+            persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)
         return cls.from_persist_path(persist_path, fs=fs)
 
     @property
     def client(self) -> None:
         """Get client."""
         return None
 
@@ -91,53 +99,75 @@
     def add(
         self,
         embedding_results: List[NodeWithEmbedding],
     ) -> List[str]:
         """Add embedding_results to index."""
         for result in embedding_results:
             self._data.embedding_dict[result.id] = result.embedding
-            self._data.text_id_to_doc_id[result.id] = result.ref_doc_id
+            self._data.text_id_to_ref_doc_id[result.id] = result.ref_doc_id
         return [result.id for result in embedding_results]
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete a document."""
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id.
+
+        Args:
+            ref_doc_id (str): The doc_id of the document to delete.
+
+        """
         text_ids_to_delete = set()
-        for text_id, doc_id_ in self._data.text_id_to_doc_id.items():
-            if doc_id == doc_id_:
+        for text_id, ref_doc_id_ in self._data.text_id_to_ref_doc_id.items():
+            if ref_doc_id == ref_doc_id_:
                 text_ids_to_delete.add(text_id)
 
         for text_id in text_ids_to_delete:
             del self._data.embedding_dict[text_id]
-            del self._data.text_id_to_doc_id[text_id]
+            del self._data.text_id_to_ref_doc_id[text_id]
 
     def query(
         self,
         query: VectorStoreQuery,
         **kwargs: Any,
     ) -> VectorStoreQueryResult:
         """Get nodes for response."""
         if query.filters is not None:
             raise ValueError(
                 "Metadata filters not implemented for SimpleVectorStore yet."
             )
 
         # TODO: consolidate with get_query_text_embedding_similarities
         items = self._data.embedding_dict.items()
-        node_ids = [t[0] for t in items]
-        embeddings = [t[1] for t in items]
+
+        if query.node_ids:
+            available_ids = set(query.node_ids)
+
+            node_ids = [t[0] for t in items if t[0] in available_ids]
+            embeddings = [t[1] for t in items if t[0] in available_ids]
+        else:
+            node_ids = [t[0] for t in items]
+            embeddings = [t[1] for t in items]
 
         query_embedding = cast(List[float], query.query_embedding)
 
         if query.mode in LEARNER_MODES:
             top_similarities, top_ids = get_top_k_embeddings_learner(
                 query_embedding,
                 embeddings,
                 similarity_top_k=query.similarity_top_k,
                 embedding_ids=node_ids,
             )
+        elif query.mode == MMR_MODE:
+            mmr_threshold = kwargs.get("mmr_threshold", None)
+            top_similarities, top_ids = get_top_k_mmr_embeddings(
+                query_embedding,
+                embeddings,
+                similarity_top_k=query.similarity_top_k,
+                embedding_ids=node_ids,
+                mmr_threshold=mmr_threshold,
+            )
         elif query.mode == VectorStoreQueryMode.DEFAULT:
             top_similarities, top_ids = get_top_k_embeddings(
                 query_embedding,
                 embeddings,
                 similarity_top_k=query.similarity_top_k,
                 embedding_ids=node_ids,
             )
```

### Comparing `llama_index-0.6.9/llama_index/vector_stores/types.py` & `llama_index-0.7.0/llama_index/vector_stores/types.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,16 +1,16 @@
 """Vector store index types."""
 from dataclasses import dataclass
 from enum import Enum
-from typing import Any, List, Optional, Protocol, Union, runtime_checkable
+from typing import Any, List, Optional, Protocol, Sequence, Union, runtime_checkable
 
 import fsspec
-from pydantic import BaseModel
+from pydantic import BaseModel, StrictFloat, StrictInt, StrictStr
 
-from llama_index.data_structs.node import Node
+from llama_index.schema import BaseNode
 
 DEFAULT_PERSIST_DIR = "./storage"
 DEFAULT_PERSIST_FNAME = "vector_store.json"
 
 
 @dataclass
 class NodeWithEmbedding:
@@ -18,53 +18,62 @@
 
     Args:
         node (Node): Node
         embedding (List[float]): Embedding
 
     """
 
-    node: Node
+    node: BaseNode
     embedding: List[float]
 
     @property
     def id(self) -> str:
-        return self.node.get_doc_id()
+        return self.node.node_id
 
     @property
     def ref_doc_id(self) -> str:
         return self.node.ref_doc_id or "None"
 
 
 @dataclass
 class VectorStoreQueryResult:
     """Vector store query result."""
 
-    nodes: Optional[List[Node]] = None
+    nodes: Optional[Sequence[BaseNode]] = None
     similarities: Optional[List[float]] = None
     ids: Optional[List[str]] = None
 
 
 class VectorStoreQueryMode(str, Enum):
     """Vector store query mode."""
 
     DEFAULT = "default"
     SPARSE = "sparse"
     HYBRID = "hybrid"
+    TEXT_SEARCH = "text_search"
 
     # fit learners
     SVM = "svm"
     LOGISTIC_REGRESSION = "logistic_regression"
     LINEAR_REGRESSION = "linear_regression"
 
+    # maximum marginal relevance
+    MMR = "mmr"
+
 
 class ExactMatchFilter(BaseModel):
-    """Exact match metadata filter for vector stores."""
+    """Exact match metadata filter for vector stores.
+
+    Value uses Strict* types, as int, float and str are compatible types and were all
+    converted to string before.
+
+    See: https://docs.pydantic.dev/latest/usage/types/#strict-types"""
 
     key: str
-    value: Union[str, int, float]
+    value: Union[StrictInt, StrictFloat, StrictStr]
 
 
 class MetadataFilters(BaseModel):
     """Metadata filters for vector stores.
 
     Currently only supports exact match filters.
     TODO: support more advanced expressions.
@@ -109,24 +118,28 @@
 @dataclass
 class VectorStoreQuery:
     """Vector store query."""
 
     query_embedding: Optional[List[float]] = None
     similarity_top_k: int = 1
     doc_ids: Optional[List[str]] = None
+    node_ids: Optional[List[str]] = None
     query_str: Optional[str] = None
 
     mode: VectorStoreQueryMode = VectorStoreQueryMode.DEFAULT
 
     # NOTE: only for hybrid search (0 for bm25, 1 for vector search)
     alpha: Optional[float] = None
 
     # metadata filters
     filters: Optional[MetadataFilters] = None
 
+    # only for mmr
+    mmr_threshold: Optional[float] = None
+
 
 @runtime_checkable
 class VectorStore(Protocol):
     """Abstract vector store protocol."""
 
     stores_text: bool
     is_embedding_query: bool = True
@@ -139,16 +152,17 @@
     def add(
         self,
         embedding_results: List[NodeWithEmbedding],
     ) -> List[str]:
         """Add embedding results to vector store."""
         ...
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
-        """Delete doc."""
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
+        """
+        Delete nodes using with ref_doc_id."""
         ...
 
     def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:
         """Query vector store."""
         ...
 
     def persist(
```

### Comparing `llama_index-0.6.9/llama_index.egg-info/PKG-INFO` & `llama_index-0.7.0/README.md`

 * *Files 18% similar despite different names*

```diff
@@ -1,62 +1,49 @@
-Metadata-Version: 2.1
-Name: llama-index
-Version: 0.6.9
-Summary: Interface between LLMs and your data
-Home-page: https://github.com/jerryjliu/llama_index
-License: MIT
-Description-Content-Type: text/markdown
-License-File: LICENSE
-
 # 🗂️ LlamaIndex 🦙
 
-LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.
+LlamaIndex (GPT Index) is a data framework for your LLM application.
 
 PyPI: 
 - LlamaIndex: https://pypi.org/project/llama-index/.
 - GPT Index (duplicate): https://pypi.org/project/gpt-index/.
 
 Documentation: https://gpt-index.readthedocs.io/.
 
-Twitter: https://twitter.com/gpt_index.
+Twitter: https://twitter.com/llama_index.
 
 Discord: https://discord.gg/dGcwcsnxhU.
 
 ### Ecosystem
 
 - LlamaHub (community library of data loaders): https://llamahub.ai
 - LlamaLab (cutting-edge AGI projects using LlamaIndex): https://github.com/run-llama/llama-lab
 
 
 ## 🚀 Overview
 
 **NOTE**: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!
 
 ### Context
-- LLMs are a phenomenal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.
+- LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.
 - How do we best augment LLMs with our own private data?
-- One paradigm that has emerged is *in-context* learning (the other is finetuning), where we insert context into the input prompt. That way,
-we take advantage of the LLM's reasoning capabilities to generate a response.
 
-To perform LLM's data augmentation in a performant, efficient, and cheap manner, we need to solve two components:
-- Data Ingestion
-- Data Indexing
+We need a comprehensive toolkit to help perform this data augmentation for LLMs.
 
 ### Proposed Solution
 
-That's where the **LlamaIndex** comes in. LlamaIndex is a simple, flexible interface between your external data and LLMs. It provides the following tools in an easy-to-use fashion:
+That's where **LlamaIndex** comes in. LlamaIndex is a "data framework" to help you build LLM apps. It provides the following tools:
 
-- Offers **data connectors** to your existing data sources and data formats (API's, PDF's, docs, SQL, etc.)
-- Provides **indices** over your unstructured and structured data for use with LLM's. 
-These indices help to abstract away common boilerplate and pain points for in-context learning:
-   - Storing context in an easy-to-access format for prompt insertion.
-   - Dealing with prompt limitations (e.g. 4096 tokens for Davinci) when context is too big.
-   - Dealing with text splitting.
-- Provides users an interface to **query** the index (feed in an input prompt) and obtain a knowledge-augmented output.
-- Offers you a comprehensive toolset trading off cost and performance.
+- Offers **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)
+- Provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.
+- Provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.
+- Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, ChatGPT, anything else).
+
+LlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in
+5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules),
+to fit their needs.
 
 
 ## 💡 Contributing
 
 Interested in contributing? See our [Contribution Guide](CONTRIBUTING.md) for more details.
 
 ## 📄 Documentation
@@ -75,17 +62,17 @@
 Examples are in the `examples` folder. Indices are in the `indices` folder (see list of indices below).
 
 To build a simple vector store index:
 ```python
 import os
 os.environ["OPENAI_API_KEY"] = 'YOUR_OPENAI_API_KEY'
 
-from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader
+from llama_index import VectorStoreIndex, SimpleDirectoryReader
 documents = SimpleDirectoryReader('data').load_data()
-index = GPTVectorStoreIndex.from_documents(documents)
+index = VectorStoreIndex.from_documents(documents)
 ```
 
 
 To query:
 ```python
 query_engine = index.as_query_engine()
 query_engine.query("<question_text>?")
```

### Comparing `llama_index-0.6.9/llama_index.egg-info/SOURCES.txt` & `llama_index-0.7.0/llama_index.egg-info/SOURCES.txt`

 * *Files 14% similar despite different names*

```diff
@@ -13,38 +13,56 @@
 llama_index/types.py
 llama_index/utils.py
 llama_index.egg-info/PKG-INFO
 llama_index.egg-info/SOURCES.txt
 llama_index.egg-info/dependency_links.txt
 llama_index.egg-info/requires.txt
 llama_index.egg-info/top_level.txt
+llama_index/agent/__init__.py
+llama_index/agent/context_retriever_agent.py
+llama_index/agent/openai_agent.py
+llama_index/agent/retriever_openai_agent.py
+llama_index/bridge/__init__.py
+llama_index/bridge/langchain.py
 llama_index/callbacks/__init__.py
 llama_index/callbacks/aim.py
 llama_index/callbacks/base.py
 llama_index/callbacks/llama_debug.py
 llama_index/callbacks/schema.py
+llama_index/callbacks/token_counting.py
+llama_index/callbacks/wandb_callback.py
+llama_index/chat_engine/__init__.py
+llama_index/chat_engine/condense_question.py
+llama_index/chat_engine/react.py
+llama_index/chat_engine/simple.py
+llama_index/chat_engine/types.py
 llama_index/composability/__init__.py
 llama_index/composability/base.py
 llama_index/composability/joint_qa_summary.py
 llama_index/data_structs/__init__.py
 llama_index/data_structs/data_structs.py
 llama_index/data_structs/document_summary.py
-llama_index/data_structs/node.py
 llama_index/data_structs/registry.py
 llama_index/data_structs/struct_type.py
 llama_index/data_structs/table.py
 llama_index/embeddings/__init__.py
 llama_index/embeddings/base.py
 llama_index/embeddings/google.py
 llama_index/embeddings/langchain.py
 llama_index/embeddings/openai.py
 llama_index/embeddings/utils.py
 llama_index/evaluation/__init__.py
 llama_index/evaluation/base.py
 llama_index/evaluation/dataset_generation.py
+llama_index/evaluation/guideline_eval.py
+llama_index/graph_stores/__init__.py
+llama_index/graph_stores/nebulagraph.py
+llama_index/graph_stores/registery.py
+llama_index/graph_stores/simple.py
+llama_index/graph_stores/types.py
 llama_index/indices/__init__.py
 llama_index/indices/base.py
 llama_index/indices/base_retriever.py
 llama_index/indices/loading.py
 llama_index/indices/prompt_helper.py
 llama_index/indices/registry.py
 llama_index/indices/service_context.py
@@ -68,114 +86,152 @@
 llama_index/indices/keyword_table/base.py
 llama_index/indices/keyword_table/rake_base.py
 llama_index/indices/keyword_table/retrievers.py
 llama_index/indices/keyword_table/simple_base.py
 llama_index/indices/keyword_table/utils.py
 llama_index/indices/knowledge_graph/__init__.py
 llama_index/indices/knowledge_graph/base.py
-llama_index/indices/knowledge_graph/retrievers.py
+llama_index/indices/knowledge_graph/retriever.py
 llama_index/indices/list/__init__.py
 llama_index/indices/list/base.py
 llama_index/indices/list/retrievers.py
 llama_index/indices/postprocessor/__init__.py
 llama_index/indices/postprocessor/cohere_rerank.py
 llama_index/indices/postprocessor/llm_rerank.py
 llama_index/indices/postprocessor/node.py
 llama_index/indices/postprocessor/node_recency.py
+llama_index/indices/postprocessor/optimizer.py
 llama_index/indices/postprocessor/pii.py
 llama_index/indices/postprocessor/types.py
 llama_index/indices/query/__init__.py
 llama_index/indices/query/base.py
 llama_index/indices/query/embedding_utils.py
-llama_index/indices/query/response_synthesis.py
 llama_index/indices/query/schema.py
 llama_index/indices/query/query_transform/__init__.py
 llama_index/indices/query/query_transform/base.py
+llama_index/indices/query/query_transform/feedback_transform.py
 llama_index/indices/query/query_transform/prompts.py
-llama_index/indices/response/__init__.py
-llama_index/indices/response/accumulate.py
-llama_index/indices/response/base_builder.py
-llama_index/indices/response/compact_and_refine.py
-llama_index/indices/response/factory.py
-llama_index/indices/response/generation.py
-llama_index/indices/response/refine.py
-llama_index/indices/response/response_builder.py
-llama_index/indices/response/simple_summarize.py
-llama_index/indices/response/tree_summarize.py
-llama_index/indices/response/type.py
 llama_index/indices/struct_store/__init__.py
 llama_index/indices/struct_store/base.py
 llama_index/indices/struct_store/container_builder.py
+llama_index/indices/struct_store/json_query.py
 llama_index/indices/struct_store/pandas.py
-llama_index/indices/struct_store/pandas_query.py
 llama_index/indices/struct_store/sql.py
 llama_index/indices/struct_store/sql_query.py
 llama_index/indices/tree/__init__.py
 llama_index/indices/tree/all_leaf_retriever.py
 llama_index/indices/tree/base.py
 llama_index/indices/tree/inserter.py
 llama_index/indices/tree/select_leaf_embedding_retriever.py
 llama_index/indices/tree/select_leaf_retriever.py
 llama_index/indices/tree/tree_root_retriever.py
+llama_index/indices/tree/utils.py
 llama_index/indices/vector_store/__init__.py
 llama_index/indices/vector_store/base.py
 llama_index/indices/vector_store/retrievers/__init__.py
 llama_index/indices/vector_store/retrievers/retriever.py
 llama_index/indices/vector_store/retrievers/auto_retriever/__init__.py
 llama_index/indices/vector_store/retrievers/auto_retriever/auto_retriever.py
 llama_index/indices/vector_store/retrievers/auto_retriever/output_parser.py
 llama_index/indices/vector_store/retrievers/auto_retriever/prompts.py
 llama_index/langchain_helpers/__init__.py
-llama_index/langchain_helpers/chain_wrapper.py
 llama_index/langchain_helpers/memory_wrapper.py
 llama_index/langchain_helpers/sql_wrapper.py
 llama_index/langchain_helpers/streaming.py
 llama_index/langchain_helpers/text_splitter.py
 llama_index/langchain_helpers/agents/__init__.py
 llama_index/langchain_helpers/agents/agents.py
 llama_index/langchain_helpers/agents/toolkits.py
 llama_index/langchain_helpers/agents/tools.py
 llama_index/llm_predictor/__init__.py
 llama_index/llm_predictor/base.py
-llama_index/llm_predictor/chatgpt.py
-llama_index/llm_predictor/huggingface.py
+llama_index/llm_predictor/mock.py
 llama_index/llm_predictor/structured.py
+llama_index/llm_predictor/utils.py
+llama_index/llm_predictor/vellum/__init__.py
+llama_index/llm_predictor/vellum/exceptions.py
+llama_index/llm_predictor/vellum/predictor.py
+llama_index/llm_predictor/vellum/prompt_registry.py
+llama_index/llm_predictor/vellum/types.py
+llama_index/llm_predictor/vellum/utils.py
+llama_index/llms/__init__.py
+llama_index/llms/base.py
+llama_index/llms/custom.py
+llama_index/llms/generic_utils.py
+llama_index/llms/huggingface.py
+llama_index/llms/langchain.py
+llama_index/llms/langchain_utils.py
+llama_index/llms/mock.py
+llama_index/llms/openai.py
+llama_index/llms/openai_utils.py
+llama_index/llms/palm.py
+llama_index/llms/utils.py
 llama_index/logger/__init__.py
 llama_index/logger/base.py
 llama_index/node_parser/__init__.py
 llama_index/node_parser/interface.py
 llama_index/node_parser/node_utils.py
 llama_index/node_parser/simple.py
-llama_index/optimization/__init__.py
-llama_index/optimization/optimizer.py
+llama_index/objects/__init__.py
+llama_index/objects/base.py
+llama_index/objects/base_node_mapping.py
+llama_index/objects/table_node_mapping.py
+llama_index/objects/tool_node_mapping.py
 llama_index/output_parsers/__init__.py
 llama_index/output_parsers/base.py
 llama_index/output_parsers/guardrails.py
 llama_index/output_parsers/langchain.py
+llama_index/output_parsers/pydantic.py
 llama_index/output_parsers/selection.py
 llama_index/output_parsers/utils.py
 llama_index/playground/__init__.py
 llama_index/playground/base.py
+llama_index/program/__init__.py
+llama_index/program/base_program.py
+llama_index/program/guidance_program.py
+llama_index/program/llm_program.py
+llama_index/program/llm_prompt_program.py
+llama_index/program/openai_program.py
+llama_index/program/predefined/__init__.py
+llama_index/program/predefined/df.py
+llama_index/program/predefined/evaporate/__init__.py
+llama_index/program/predefined/evaporate/base.py
+llama_index/program/predefined/evaporate/extractor.py
+llama_index/program/predefined/evaporate/prompts.py
 llama_index/prompts/__init__.py
 llama_index/prompts/base.py
 llama_index/prompts/chat_prompts.py
 llama_index/prompts/choice_select.py
-llama_index/prompts/default_choice_select.py
 llama_index/prompts/default_prompt_selectors.py
 llama_index/prompts/default_prompts.py
+llama_index/prompts/guidance_utils.py
+llama_index/prompts/prompt_selector.py
 llama_index/prompts/prompt_type.py
 llama_index/prompts/prompts.py
+llama_index/prompts/utils.py
 llama_index/query_engine/__init__.py
+llama_index/query_engine/citation_query_engine.py
 llama_index/query_engine/graph_query_engine.py
 llama_index/query_engine/multistep_query_engine.py
+llama_index/query_engine/pandas_query_engine.py
 llama_index/query_engine/retriever_query_engine.py
+llama_index/query_engine/retry_query_engine.py
+llama_index/query_engine/retry_source_query_engine.py
 llama_index/query_engine/router_query_engine.py
+llama_index/query_engine/sql_join_query_engine.py
+llama_index/query_engine/sql_vector_query_engine.py
 llama_index/query_engine/sub_question_query_engine.py
 llama_index/query_engine/transform_query_engine.py
+llama_index/query_engine/flare/__init__.py
+llama_index/query_engine/flare/answer_inserter.py
+llama_index/query_engine/flare/base.py
+llama_index/query_engine/flare/output_parser.py
+llama_index/query_engine/flare/schema.py
 llama_index/question_gen/__init__.py
+llama_index/question_gen/guidance_generator.py
 llama_index/question_gen/llm_generators.py
 llama_index/question_gen/output_parser.py
 llama_index/question_gen/prompts.py
 llama_index/question_gen/types.py
 llama_index/readers/__init__.py
 llama_index/readers/base.py
 llama_index/readers/chroma.py
@@ -190,14 +246,15 @@
 llama_index/readers/metal.py
 llama_index/readers/milvus.py
 llama_index/readers/mongo.py
 llama_index/readers/myscale.py
 llama_index/readers/notion.py
 llama_index/readers/obsidian.py
 llama_index/readers/pinecone.py
+llama_index/readers/psychic.py
 llama_index/readers/qdrant.py
 llama_index/readers/slack.py
 llama_index/readers/string_iterable.py
 llama_index/readers/twitter.py
 llama_index/readers/web.py
 llama_index/readers/wikipedia.py
 llama_index/readers/youtube_transcript.py
@@ -228,83 +285,122 @@
 llama_index/readers/redis/__init__.py
 llama_index/readers/redis/utils.py
 llama_index/readers/schema/__init__.py
 llama_index/readers/schema/base.py
 llama_index/readers/steamship/__init__.py
 llama_index/readers/steamship/file_reader.py
 llama_index/readers/weaviate/__init__.py
-llama_index/readers/weaviate/client.py
 llama_index/readers/weaviate/reader.py
-llama_index/readers/weaviate/utils.py
 llama_index/response/__init__.py
 llama_index/response/notebook_utils.py
 llama_index/response/pprint_utils.py
 llama_index/response/schema.py
 llama_index/response/utils.py
+llama_index/response_synthesizers/__init__.py
+llama_index/response_synthesizers/accumulate.py
+llama_index/response_synthesizers/base.py
+llama_index/response_synthesizers/compact_and_accumulate.py
+llama_index/response_synthesizers/compact_and_refine.py
+llama_index/response_synthesizers/factory.py
+llama_index/response_synthesizers/generation.py
+llama_index/response_synthesizers/refine.py
+llama_index/response_synthesizers/simple_summarize.py
+llama_index/response_synthesizers/tree_summarize.py
+llama_index/response_synthesizers/type.py
 llama_index/retrievers/__init__.py
 llama_index/retrievers/transform_retriever.py
 llama_index/selectors/__init__.py
 llama_index/selectors/llm_selectors.py
 llama_index/selectors/prompts.py
+llama_index/selectors/pydantic_selectors.py
 llama_index/selectors/types.py
 llama_index/storage/__init__.py
 llama_index/storage/storage_context.py
 llama_index/storage/docstore/__init__.py
+llama_index/storage/docstore/dynamodb_docstore.py
 llama_index/storage/docstore/keyval_docstore.py
 llama_index/storage/docstore/mongo_docstore.py
+llama_index/storage/docstore/redis_docstore.py
 llama_index/storage/docstore/registry.py
 llama_index/storage/docstore/simple_docstore.py
 llama_index/storage/docstore/types.py
 llama_index/storage/docstore/utils.py
 llama_index/storage/index_store/__init__.py
+llama_index/storage/index_store/dynamodb_index_store.py
 llama_index/storage/index_store/keyval_index_store.py
 llama_index/storage/index_store/mongo_index_store.py
+llama_index/storage/index_store/redis_index_store.py
 llama_index/storage/index_store/simple_index_store.py
 llama_index/storage/index_store/types.py
 llama_index/storage/index_store/utils.py
 llama_index/storage/kvstore/__init__.py
+llama_index/storage/kvstore/dynamodb_kvstore.py
 llama_index/storage/kvstore/mongodb_kvstore.py
+llama_index/storage/kvstore/redis_kvstore.py
 llama_index/storage/kvstore/s3_kvstore.py
 llama_index/storage/kvstore/simple_kvstore.py
 llama_index/storage/kvstore/types.py
 llama_index/token_counter/__init__.py
-llama_index/token_counter/mock_chain_wrapper.py
 llama_index/token_counter/mock_embed_model.py
-llama_index/token_counter/token_counter.py
 llama_index/token_counter/utils.py
 llama_index/tools/__init__.py
+llama_index/tools/function_tool.py
+llama_index/tools/ondemand_loader_tool.py
 llama_index/tools/query_engine.py
+llama_index/tools/query_plan.py
 llama_index/tools/types.py
+llama_index/tools/utils.py
+llama_index/tools/tool_spec/__init__.py
+llama_index/tools/tool_spec/base.py
+llama_index/tools/tool_spec/notion/__init__.py
+llama_index/tools/tool_spec/notion/base.py
+llama_index/tools/tool_spec/slack/__init__.py
+llama_index/tools/tool_spec/slack/base.py
 llama_index/tts/__init__.py
 llama_index/tts/bark.py
 llama_index/tts/base.py
 llama_index/tts/elevenlabs.py
 llama_index/vector_stores/__init__.py
 llama_index/vector_stores/chatgpt_plugin.py
 llama_index/vector_stores/chroma.py
 llama_index/vector_stores/deeplake.py
+llama_index/vector_stores/dynamodb.py
 llama_index/vector_stores/faiss.py
 llama_index/vector_stores/lancedb.py
 llama_index/vector_stores/metal.py
 llama_index/vector_stores/milvus.py
+llama_index/vector_stores/mongodb.py
 llama_index/vector_stores/myscale.py
 llama_index/vector_stores/opensearch.py
 llama_index/vector_stores/pinecone.py
+llama_index/vector_stores/postgres.py
 llama_index/vector_stores/qdrant.py
 llama_index/vector_stores/redis.py
 llama_index/vector_stores/registry.py
 llama_index/vector_stores/simple.py
+llama_index/vector_stores/supabase.py
+llama_index/vector_stores/tair.py
 llama_index/vector_stores/types.py
+llama_index/vector_stores/typesense.py
 llama_index/vector_stores/utils.py
 llama_index/vector_stores/weaviate.py
+llama_index/vector_stores/weaviate_utils.py
+llama_index/vector_stores/docarray/__init__.py
+llama_index/vector_stores/docarray/base.py
+llama_index/vector_stores/docarray/hnsw.py
+llama_index/vector_stores/docarray/in_memory.py
 tests/__init__.py
 tests/conftest.py
 tests/test_utils.py
 tests/callbacks/__init__.py
 tests/callbacks/test_llama_debug.py
+tests/callbacks/test_token_counter.py
+tests/chat_engine/__init__.py
+tests/chat_engine/test_condense_question.py
+tests/chat_engine/test_simple.py
 tests/embeddings/__init__.py
 tests/embeddings/test_base.py
 tests/indices/__init__.py
 tests/indices/conftest.py
 tests/indices/test_loading.py
 tests/indices/test_loading_graph.py
 tests/indices/test_node_utils.py
@@ -327,26 +423,28 @@
 tests/indices/knowledge_graph/test_retrievers.py
 tests/indices/list/__init__.py
 tests/indices/list/test_index.py
 tests/indices/list/test_retrievers.py
 tests/indices/postprocessor/__init__.py
 tests/indices/postprocessor/test_base.py
 tests/indices/postprocessor/test_llm_rerank.py
+tests/indices/postprocessor/test_optimizer.py
 tests/indices/query/__init__.py
 tests/indices/query/conftest.py
 tests/indices/query/test_compose.py
 tests/indices/query/test_compose_vector.py
+tests/indices/query/test_embedding_utils.py
 tests/indices/query/test_query_bundle.py
 tests/indices/query/query_transform/__init__.py
 tests/indices/query/query_transform/mock_utils.py
 tests/indices/query/query_transform/test_base.py
 tests/indices/struct_store/__init__.py
 tests/indices/struct_store/conftest.py
 tests/indices/struct_store/test_base.py
-tests/indices/struct_store/test_pandas.py
+tests/indices/struct_store/test_json_query.py
 tests/indices/struct_store/test_sql_query.py
 tests/indices/tree/__init__.py
 tests/indices/tree/conftest.py
 tests/indices/tree/test_embedding_retriever.py
 tests/indices/tree/test_index.py
 tests/indices/tree/test_retrievers.py
 tests/indices/vector_store/__init__.py
@@ -361,43 +459,73 @@
 tests/indices/vector_store/utils.py
 tests/indices/vector_store/auto_retriever/__init__.py
 tests/indices/vector_store/auto_retriever/test_output_parser.py
 tests/langchain_helpers/__init__.py
 tests/langchain_helpers/test_text_splitter.py
 tests/llm_predictor/__init__.py
 tests/llm_predictor/test_base.py
+tests/llm_predictor/vellum/__init__.py
+tests/llm_predictor/vellum/conftest.py
+tests/llm_predictor/vellum/test_predictor.py
+tests/llm_predictor/vellum/test_prompt_registry.py
+tests/llm_predictor/vellum/test_utils.py
+tests/llms/__init__.py
+tests/llms/test_custom.py
+tests/llms/test_langchain.py
+tests/llms/test_openai.py
+tests/llms/test_openai_utils.py
+tests/llms/test_palm.py
 tests/logger/__init__.py
 tests/logger/test_base.py
 tests/mock_utils/__init__.py
 tests/mock_utils/mock_predict.py
 tests/mock_utils/mock_prompts.py
 tests/mock_utils/mock_text_splitter.py
 tests/mock_utils/mock_utils.py
-tests/optimization/__init__.py
-tests/optimization/test_base.py
+tests/objects/__init__.py
+tests/objects/test_base.py
+tests/objects/test_node_mapping.py
 tests/output_parsers/__init__.py
 tests/output_parsers/test_base.py
+tests/output_parsers/test_pydantic.py
 tests/output_parsers/test_selection.py
 tests/playground/__init__.py
 tests/playground/test_base.py
+tests/program/__init__.py
+tests/program/test_guidance.py
+tests/program/test_llm_program.py
 tests/prompts/__init__.py
 tests/prompts/test_base.py
+tests/prompts/test_guidance_utils.py
 tests/question_gen/__init__.py
+tests/question_gen/test_guidance_generator.py
 tests/question_gen/test_llm_generators.py
 tests/readers/__init__.py
 tests/readers/test_file.py
 tests/readers/test_json.py
 tests/readers/test_mongo.py
 tests/readers/test_string_iterable.py
 tests/selectors/__init__.py
 tests/selectors/test_llm_selectors.py
 tests/storage/__init__.py
 tests/storage/conftest.py
 tests/storage/test_storage_context.py
 tests/storage/docstore/__init__.py
+tests/storage/docstore/test_dynamodb_docstore.py
 tests/storage/docstore/test_mongo_docstore.py
+tests/storage/docstore/test_redis_docstore.py
 tests/storage/docstore/test_simple_docstore.py
 tests/token_predictor/__init__.py
 tests/token_predictor/test_base.py
+tests/tools/__init__.py
+tests/tools/conftest.py
+tests/tools/test_base.py
+tests/tools/test_ondemand_loader.py
+tests/tools/test_utils.py
+tests/tools/tool_spec/__init__.py
+tests/tools/tool_spec/test_base.py
 tests/vector_stores/__init__.py
+tests/vector_stores/test_docarray.py
+tests/vector_stores/test_postgres.py
 tests/vector_stores/test_qdrant.py
+tests/vector_stores/test_tair.py
 tests/vector_stores/test_weaviate.py
```

### Comparing `llama_index-0.6.9/tests/callbacks/test_llama_debug.py` & `llama_index-0.7.0/tests/callbacks/test_llama_debug.py`

 * *Files 7% similar despite different names*

```diff
@@ -14,67 +14,67 @@
     handler = LlamaDebugHandler()
 
     event_id = handler.on_event_start(
         CBEventType.LLM, payload=TEST_PAYLOAD, event_id=TEST_ID
     )
 
     assert event_id == TEST_ID
-    assert len(handler.events) == 1
+    assert len(handler.event_pairs_by_type) == 1
     assert len(handler.sequential_events) == 1
 
-    events = handler.events.get(CBEventType.LLM)
+    events = handler.event_pairs_by_type.get(CBEventType.LLM)
     assert isinstance(events, list)
     assert events[0].payload == TEST_PAYLOAD
 
 
 def test_on_event_end() -> None:
     """Test event end."""
     handler = LlamaDebugHandler()
 
     handler.on_event_end(CBEventType.EMBEDDING, payload=TEST_PAYLOAD, event_id=TEST_ID)
 
-    assert len(handler.events) == 1
+    assert len(handler.event_pairs_by_type) == 1
     assert len(handler.sequential_events) == 1
 
-    events = handler.events.get(CBEventType.EMBEDDING)
+    events = handler.event_pairs_by_type.get(CBEventType.EMBEDDING)
     assert isinstance(events, list)
     assert events[0].payload == TEST_PAYLOAD
     assert events[0].id_ == TEST_ID
 
 
 def test_get_event_stats() -> None:
     """Test get event stats."""
     handler = LlamaDebugHandler()
 
     event_id = handler.on_event_start(CBEventType.CHUNKING, payload=TEST_PAYLOAD)
     handler.on_event_end(CBEventType.CHUNKING, event_id=event_id)
 
-    assert len(handler.events[CBEventType.CHUNKING]) == 2
+    assert len(handler.event_pairs_by_type[CBEventType.CHUNKING]) == 2
 
     event_stats = handler.get_event_time_info(CBEventType.CHUNKING)
 
     assert event_stats.total_count == 1
-    assert event_stats.total_secs == 0
+    assert event_stats.total_secs > 0.0
 
 
 def test_flush_events() -> None:
     """Test flush events."""
     handler = LlamaDebugHandler()
 
     event_id = handler.on_event_start(CBEventType.CHUNKING, payload=TEST_PAYLOAD)
     handler.on_event_end(CBEventType.CHUNKING, event_id=event_id)
 
     event_id = handler.on_event_start(CBEventType.CHUNKING, payload=TEST_PAYLOAD)
     handler.on_event_end(CBEventType.CHUNKING, event_id=event_id)
 
-    assert len(handler.events[CBEventType.CHUNKING]) == 4
+    assert len(handler.event_pairs_by_type[CBEventType.CHUNKING]) == 4
 
     handler.flush_event_logs()
 
-    assert len(handler.events) == 0
+    assert len(handler.event_pairs_by_type) == 0
     assert len(handler.sequential_events) == 0
 
 
 def test_ignore_events() -> None:
     """Test ignore event starts and ends."""
     handler = LlamaDebugHandler(
         event_starts_to_ignore=[CBEventType.CHUNKING],
```

### Comparing `llama_index-0.6.9/tests/conftest.py` & `llama_index-0.7.0/tests/conftest.py`

 * *Files 18% similar despite different names*

```diff
@@ -2,17 +2,19 @@
 # import socket
 from typing import Any
 
 import pytest
 from llama_index.indices.service_context import ServiceContext
 from llama_index.langchain_helpers.text_splitter import TokenTextSplitter
 from llama_index.llm_predictor.base import LLMPredictor
+from llama_index.llms.base import LLMMetadata
 
 
 from tests.indices.vector_store.mock_services import MockEmbedding
+from llama_index.llms.mock import MockLLM
 from tests.mock_utils.mock_predict import (
     patch_llmpredictor_apredict,
     patch_llmpredictor_predict,
 )
 from tests.mock_utils.mock_text_splitter import (
     patch_token_splitter_newline,
     patch_token_splitter_newline_with_overlaps,
@@ -42,32 +44,37 @@
     )
 
 
 @pytest.fixture
 def patch_llm_predictor(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.setattr(
         LLMPredictor,
-        "total_tokens_used",
-        0,
-    )
-    monkeypatch.setattr(
-        LLMPredictor,
         "predict",
         patch_llmpredictor_predict,
     )
     monkeypatch.setattr(
         LLMPredictor,
         "apredict",
         patch_llmpredictor_apredict,
     )
     monkeypatch.setattr(
         LLMPredictor,
+        "llm",
+        MockLLM(),
+    )
+    monkeypatch.setattr(
+        LLMPredictor,
         "__init__",
         lambda x: None,
     )
+    monkeypatch.setattr(
+        LLMPredictor,
+        "metadata",
+        LLMMetadata(),
+    )
 
 
 @pytest.fixture()
 def mock_service_context(
     patch_token_text_splitter: Any, patch_llm_predictor: Any
 ) -> ServiceContext:
     return ServiceContext.from_defaults(embed_model=MockEmbedding())
```

### Comparing `llama_index-0.6.9/tests/embeddings/test_base.py` & `llama_index-0.7.0/tests/embeddings/test_base.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 """Embeddings."""
 from typing import Any, List
 from unittest.mock import patch
-from llama_index.embeddings.base import mean_agg
 
+from llama_index.embeddings.base import SimilarityMode, mean_agg
 from llama_index.embeddings.openai import OpenAIEmbedding
 
 
 def mock_get_text_embedding(text: str) -> List[float]:
     """Mock get text embedding."""
     # assume dimensions are 5
     if text == "Hello world.":
@@ -73,13 +73,27 @@
     embed_model = OpenAIEmbedding()
     text_embedding = [3.0, 4.0, 0.0]
     query_embedding = [0.0, 1.0, 0.0]
     cosine = embed_model.similarity(query_embedding, text_embedding)
     assert cosine == 0.8
 
 
+def test_embedding_similarity_euclidean() -> None:
+    embed_model = OpenAIEmbedding()
+    query_embedding = [1.0, 0.0]
+    text1_embedding = [0.0, 1.0]  # further from query_embedding distance=1.414
+    text2_embedding = [1.0, 1.0]  # closer to query_embedding distance=1.0
+    euclidean_similarity1 = embed_model.similarity(
+        query_embedding, text1_embedding, mode=SimilarityMode.EUCLIDEAN
+    )
+    euclidean_similarity2 = embed_model.similarity(
+        query_embedding, text2_embedding, mode=SimilarityMode.EUCLIDEAN
+    )
+    assert euclidean_similarity1 < euclidean_similarity2
+
+
 def test_mean_agg() -> None:
     """Test mean aggregation for embeddings."""
     embedding_0 = [3.0, 4.0, 0.0]
     embedding_1 = [0.0, 1.0, 0.0]
     output = mean_agg([embedding_0, embedding_1])
     assert output == [1.5, 2.5, 0.0]
```

### Comparing `llama_index-0.6.9/tests/indices/composability/test_utils.py` & `llama_index-0.7.0/tests/indices/composability/test_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -25,14 +25,14 @@
     def add(
         self,
         embedding_results: List[NodeWithEmbedding],
     ) -> List[str]:
         """Add embedding results to vector store."""
         raise NotImplementedError()
 
-    def delete(self, doc_id: str, **delete_kwargs: Any) -> None:
+    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:
         """Delete doc."""
         raise NotImplementedError()
 
     def query(self, query: VectorStoreQuery, **kwargs: Any) -> VectorStoreQueryResult:
         """Query vector store."""
         raise NotImplementedError()
```

### Comparing `llama_index-0.6.9/tests/indices/empty/test_base.py` & `llama_index-0.7.0/tests/indices/empty/test_base.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 """Test empty index."""
 
-from llama_index.data_structs.data_structs import EmptyIndex
-from llama_index.indices.empty.base import GPTEmptyIndex
+from llama_index.data_structs.data_structs import EmptyIndexStruct
+from llama_index.indices.empty.base import EmptyIndex
 from llama_index.indices.service_context import ServiceContext
 
 
 def test_empty(
     mock_service_context: ServiceContext,
 ) -> None:
     """Test build list."""
-    empty_index = GPTEmptyIndex(service_context=mock_service_context)
-    assert isinstance(empty_index.index_struct, EmptyIndex)
+    empty_index = EmptyIndex(service_context=mock_service_context)
+    assert isinstance(empty_index.index_struct, EmptyIndexStruct)
 
     retriever = empty_index.as_retriever()
     nodes = retriever.retrieve("What is?")
     assert len(nodes) == 0
```

### Comparing `llama_index-0.6.9/tests/indices/keyword_table/test_base.py` & `llama_index-0.7.0/tests/indices/keyword_table/test_base.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,49 +1,49 @@
 """Test keyword table index."""
 
 from typing import Any, List
 from unittest.mock import patch
 
 import pytest
 
-from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex
+from llama_index.indices.keyword_table.simple_base import SimpleKeywordTableIndex
 from llama_index.indices.service_context import ServiceContext
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 from tests.mock_utils.mock_utils import mock_extract_keywords
 
 
 @pytest.fixture
 def documents() -> List[Document]:
     """Get documents."""
     # NOTE: one document for now
     doc_text = (
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
-    return [Document(doc_text)]
+    return [Document(text=doc_text)]
 
 
 @patch(
     "llama_index.indices.keyword_table.simple_base.simple_extract_keywords",
     mock_extract_keywords,
 )
 def test_build_table(
     documents: List[Document], mock_service_context: ServiceContext
 ) -> None:
     """Test build table."""
     # test simple keyword table
     # NOTE: here the keyword extraction isn't mocked because we're using
     # the regex-based keyword extractor, not GPT
-    table = GPTSimpleKeywordTableIndex.from_documents(
+    table = SimpleKeywordTableIndex.from_documents(
         documents, service_context=mock_service_context
     )
     nodes = table.docstore.get_nodes(list(table.index_struct.node_ids))
-    table_chunks = {n.get_text() for n in nodes}
+    table_chunks = {n.get_content() for n in nodes}
     assert len(table_chunks) == 4
     assert "Hello world." in table_chunks
     assert "This is a test." in table_chunks
     assert "This is another test." in table_chunks
     assert "This is a test v2." in table_chunks
 
     # test that expected keys are present in table
@@ -70,19 +70,19 @@
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test build table."""
     # test simple keyword table
     # NOTE: here the keyword extraction isn't mocked because we're using
     # the regex-based keyword extractor, not GPT
-    table = GPTSimpleKeywordTableIndex.from_documents(
+    table = SimpleKeywordTableIndex.from_documents(
         documents, use_async=True, service_context=mock_service_context
     )
     nodes = table.docstore.get_nodes(list(table.index_struct.node_ids))
-    table_chunks = {n.get_text() for n in nodes}
+    table_chunks = {n.get_content() for n in nodes}
     assert len(table_chunks) == 4
     assert "Hello world." in table_chunks
     assert "This is a test." in table_chunks
     assert "This is another test." in table_chunks
     assert "This is a test v2." in table_chunks
 
     # test that expected keys are present in table
@@ -105,19 +105,19 @@
     mock_extract_keywords,
 )
 def test_insert(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test insert."""
-    table = GPTSimpleKeywordTableIndex([], service_context=mock_service_context)
+    table = SimpleKeywordTableIndex([], service_context=mock_service_context)
     assert len(table.index_struct.table.keys()) == 0
     table.insert(documents[0])
     nodes = table.docstore.get_nodes(list(table.index_struct.node_ids))
-    table_chunks = {n.get_text() for n in nodes}
+    table_chunks = {n.get_content() for n in nodes}
     assert "Hello world." in table_chunks
     assert "This is a test." in table_chunks
     assert "This is another test." in table_chunks
     assert "This is a test v2." in table_chunks
     # test that expected keys are present in table
     # NOTE: in mock keyword extractor, stopwords are not filtered
     assert table.index_struct.table.keys() == {
@@ -129,17 +129,17 @@
         "v2",
         "is",
         "a",
         "v2",
     }
 
     # test insert with doc_id
-    document1 = Document("This is", doc_id="test_id1")
-    document2 = Document("test v3", doc_id="test_id2")
-    table = GPTSimpleKeywordTableIndex([])
+    document1 = Document(text="This is", id_="test_id1")
+    document2 = Document(text="test v3", id_="test_id2")
+    table = SimpleKeywordTableIndex([])
     table.insert(document1)
     table.insert(document2)
     chunk_index1_1 = list(table.index_struct.table["this"])[0]
     chunk_index1_2 = list(table.index_struct.table["is"])[0]
     chunk_index2_1 = list(table.index_struct.table["test"])[0]
     chunk_index2_2 = list(table.index_struct.table["v3"])[0]
     nodes = table.docstore.get_nodes(
@@ -156,33 +156,44 @@
     mock_extract_keywords,
 )
 def test_delete(
     mock_service_context: ServiceContext,
 ) -> None:
     """Test insert."""
     new_documents = [
-        Document("Hello world.\nThis is a test.", doc_id="test_id_1"),
-        Document("This is another test.", doc_id="test_id_2"),
-        Document("This is a test v2.", doc_id="test_id_3"),
+        Document(text="Hello world.\nThis is a test.", id_="test_id_1"),
+        Document(text="This is another test.", id_="test_id_2"),
+        Document(text="This is a test v2.", id_="test_id_3"),
     ]
 
     # test delete
-    table = GPTSimpleKeywordTableIndex.from_documents(
+    table = SimpleKeywordTableIndex.from_documents(
         new_documents, service_context=mock_service_context
     )
-    table.delete("test_id_1")
+    # test delete
+    table.delete_ref_doc("test_id_1")
     assert len(table.index_struct.table.keys()) == 6
-    print(table.index_struct.table.keys())
     assert len(table.index_struct.table["this"]) == 2
+
+    # test node contents after delete
     nodes = table.docstore.get_nodes(list(table.index_struct.node_ids))
-    node_texts = {n.get_text() for n in nodes}
+    node_texts = {n.get_content() for n in nodes}
     assert node_texts == {"This is another test.", "This is a test v2."}
 
-    table = GPTSimpleKeywordTableIndex.from_documents(
+    table = SimpleKeywordTableIndex.from_documents(
         new_documents, service_context=mock_service_context
     )
-    table.delete("test_id_2")
+
+    # test ref doc info
+    all_ref_doc_info = table.ref_doc_info
+    for doc_id in all_ref_doc_info.keys():
+        assert doc_id in ("test_id_1", "test_id_2", "test_id_3")
+
+    # test delete
+    table.delete_ref_doc("test_id_2")
     assert len(table.index_struct.table.keys()) == 7
     assert len(table.index_struct.table["this"]) == 2
+
+    # test node contents after delete
     nodes = table.docstore.get_nodes(list(table.index_struct.node_ids))
-    node_texts = {n.get_text() for n in nodes}
+    node_texts = {n.get_content() for n in nodes}
     assert node_texts == {"Hello world.", "This is a test.", "This is a test v2."}
```

### Comparing `llama_index-0.6.9/tests/indices/keyword_table/test_retrievers.py` & `llama_index-0.7.0/tests/indices/keyword_table/test_retrievers.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import List
 from unittest.mock import patch
-from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex
+from llama_index.indices.keyword_table.simple_base import SimpleKeywordTableIndex
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 from tests.mock_utils.mock_utils import mock_extract_keywords
 
 
 @patch(
     "llama_index.indices.keyword_table.simple_base.simple_extract_keywords",
     mock_extract_keywords,
 )
@@ -18,15 +18,15 @@
 def test_retrieve(
     documents: List[Document], mock_service_context: ServiceContext
 ) -> None:
     """Test query."""
     # test simple keyword table
     # NOTE: here the keyword extraction isn't mocked because we're using
     # the regex-based keyword extractor, not GPT
-    table = GPTSimpleKeywordTableIndex.from_documents(
+    table = SimpleKeywordTableIndex.from_documents(
         documents, service_context=mock_service_context
     )
 
     retriever = table.as_retriever(retriever_mode="simple")
     nodes = retriever.retrieve(QueryBundle("Hello"))
     assert len(nodes) == 1
-    assert nodes[0].node.text == "Hello world."
+    assert nodes[0].node.get_content() == "Hello world."
```

### Comparing `llama_index-0.6.9/tests/indices/keyword_table/test_utils.py` & `llama_index-0.7.0/tests/indices/keyword_table/test_utils.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/indices/knowledge_graph/test_base.py` & `llama_index-0.7.0/tests/indices/knowledge_graph/test_base.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,19 @@
 """Test knowledge graph index."""
 
 from typing import Any, Dict, List, Tuple
 from unittest.mock import patch
 
 import pytest
-from llama_index.data_structs.node import Node
+
 from llama_index.embeddings.base import BaseEmbedding
-from llama_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex
+from llama_index.indices.knowledge_graph.base import KnowledgeGraphIndex
 from llama_index.indices.service_context import ServiceContext
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
+from llama_index.schema import TextNode
 from tests.mock_utils.mock_prompts import (
     MOCK_KG_TRIPLET_EXTRACT_PROMPT,
     MOCK_QUERY_KEYWORD_EXTRACT_PROMPT,
 )
 
 
 class MockEmbedding(BaseEmbedding):
@@ -58,37 +59,37 @@
 
         subj, pred, obj = tokens
         triplets.append((subj, pred, obj))
     return triplets
 
 
 @patch.object(
-    GPTKnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
+    KnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
 )
 def test_build_kg_manual(
     _patch_extract_triplets: Any,
     mock_service_context: ServiceContext,
 ) -> None:
     """Test build knowledge graph."""
-    index = GPTKnowledgeGraphIndex([], service_context=mock_service_context)
+    index = KnowledgeGraphIndex([], service_context=mock_service_context)
     tuples = [
         ("foo", "is", "bar"),
         ("hello", "is not", "world"),
         ("Jane", "is mother of", "Bob"),
     ]
-    nodes = [Node(str(tup)) for tup in tuples]
+    nodes = [TextNode(text=str(tup)) for tup in tuples]
     for tup, node in zip(tuples, nodes):
         # add node
         index.add_node([tup[0], tup[2]], node)
         # add triplet
         index.upsert_triplet(tup)
 
     # NOTE: in these unit tests, document text == triplets
-    nodes = index.docstore.get_nodes(list(index.index_struct.node_ids))
-    table_chunks = {n.get_text() for n in nodes}
+    docstore_nodes = index.docstore.get_nodes(list(index.index_struct.node_ids))
+    table_chunks = {n.get_content() for n in docstore_nodes}
     assert len(table_chunks) == 3
     assert "('foo', 'is', 'bar')" in table_chunks
     assert "('hello', 'is not', 'world')" in table_chunks
     assert "('Jane', 'is mother of', 'Bob')" in table_chunks
 
     # test that expected keys are present in table
     # NOTE: in mock keyword extractor, stopwords are not filtered
@@ -98,27 +99,27 @@
         "hello",
         "world",
         "Jane",
         "Bob",
     }
 
     # test upsert_triplet_and_node
-    index = GPTKnowledgeGraphIndex([], service_context=mock_service_context)
+    index = KnowledgeGraphIndex([], service_context=mock_service_context)
     tuples = [
         ("foo", "is", "bar"),
         ("hello", "is not", "world"),
         ("Jane", "is mother of", "Bob"),
     ]
-    nodes = [Node(str(tup)) for tup in tuples]
+    nodes = [TextNode(text=str(tup)) for tup in tuples]
     for tup, node in zip(tuples, nodes):
         index.upsert_triplet_and_node(tup, node)
 
     # NOTE: in these unit tests, document text == triplets
-    nodes = index.docstore.get_nodes(list(index.index_struct.node_ids))
-    table_chunks = {n.get_text() for n in nodes}
+    docstore_nodes = index.docstore.get_nodes(list(index.index_struct.node_ids))
+    table_chunks = {n.get_content() for n in docstore_nodes}
     assert len(table_chunks) == 3
     assert "('foo', 'is', 'bar')" in table_chunks
     assert "('hello', 'is not', 'world')" in table_chunks
     assert "('Jane', 'is mother of', 'Bob')" in table_chunks
 
     # test that expected keys are present in table
     # NOTE: in mock keyword extractor, stopwords are not filtered
@@ -128,58 +129,58 @@
         "hello",
         "world",
         "Jane",
         "Bob",
     }
 
     # try inserting same node twice
-    index = GPTKnowledgeGraphIndex([], service_context=mock_service_context)
-    node = Node(str(("foo", "is", "bar")), doc_id="test_node")
+    index = KnowledgeGraphIndex([], service_context=mock_service_context)
+    node = TextNode(text=str(("foo", "is", "bar")), id_="test_node")
     index.upsert_triplet_and_node(tup, node)
     index.upsert_triplet_and_node(tup, node)
 
 
 @patch.object(
-    GPTKnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
+    KnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
 )
 def test_build_kg_similarity(
     _patch_extract_triplets: Any,
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test build knowledge graph."""
     mock_service_context.embed_model = MockEmbedding()
 
-    index = GPTKnowledgeGraphIndex.from_documents(
+    index = KnowledgeGraphIndex.from_documents(
         documents, include_embeddings=True, service_context=mock_service_context
     )
     # get embedding dict from KG index struct
     rel_text_embeddings = index.index_struct.embedding_dict
 
     # check that all rel_texts were embedded
     assert len(rel_text_embeddings) == 3
     for rel_text, embedding in rel_text_embeddings.items():
         assert embedding == MockEmbedding().get_text_embedding(rel_text)
 
 
 @patch.object(
-    GPTKnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
+    KnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
 )
 def test_build_kg(
     _patch_extract_triplets: Any,
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test build knowledge graph."""
-    index = GPTKnowledgeGraphIndex.from_documents(
+    index = KnowledgeGraphIndex.from_documents(
         documents, service_context=mock_service_context
     )
     # NOTE: in these unit tests, document text == triplets
     nodes = index.docstore.get_nodes(list(index.index_struct.node_ids))
-    table_chunks = {n.get_text() for n in nodes}
+    table_chunks = {n.get_content() for n in nodes}
     assert len(table_chunks) == 3
     assert "(foo, is, bar)" in table_chunks
     assert "(hello, is not, world)" in table_chunks
     assert "(Jane, is mother of, Bob)" in table_chunks
 
     # test that expected keys are present in table
     # NOTE: in mock keyword extractor, stopwords are not filtered
@@ -187,7 +188,13 @@
         "foo",
         "bar",
         "hello",
         "world",
         "Jane",
         "Bob",
     }
+
+    # test ref doc info for three nodes, single doc
+    all_ref_doc_info = index.ref_doc_info
+    assert len(all_ref_doc_info) == 1
+    for ref_doc_info in all_ref_doc_info.values():
+        assert len(ref_doc_info.node_ids) == 3
```

### Comparing `llama_index-0.6.9/tests/indices/knowledge_graph/test_retrievers.py` & `llama_index-0.7.0/tests/indices/knowledge_graph/test_retrievers.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,111 +1,142 @@
 from typing import Any, List
 from unittest.mock import patch
-from llama_index.indices.knowledge_graph.base import GPTKnowledgeGraphIndex
-from llama_index.indices.knowledge_graph.retrievers import KGTableRetriever
+
+from llama_index.graph_stores import SimpleGraphStore
+from llama_index.indices.knowledge_graph.base import KnowledgeGraphIndex
+from llama_index.indices.knowledge_graph.retriever import KGTableRetriever
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
-from llama_index.readers.schema.base import Document
-from tests.indices.knowledge_graph.test_base import (
-    MockEmbedding,
-    mock_extract_triplets,
-)
+from llama_index.schema import Document
+from llama_index.storage.storage_context import StorageContext
+from tests.indices.knowledge_graph.test_base import MockEmbedding, mock_extract_triplets
 from tests.mock_utils.mock_prompts import MOCK_QUERY_KEYWORD_EXTRACT_PROMPT
 
 
 @patch.object(
-    GPTKnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
+    KnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
 )
 def test_as_retriever(
     _patch_extract_triplets: Any,
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test query."""
-    index = GPTKnowledgeGraphIndex.from_documents(
-        documents, service_context=mock_service_context
+    graph_store = SimpleGraphStore()
+    storage_context = StorageContext.from_defaults(graph_store=graph_store)
+    index = KnowledgeGraphIndex.from_documents(
+        documents, service_context=mock_service_context, storage_context=storage_context
     )
-    retriever = index.as_retriever()
+    retriever: KGTableRetriever = index.as_retriever()  # type: ignore
     nodes = retriever.retrieve(QueryBundle("foo"))
     # when include_text is True, the first node is the raw text
     # the second node is the query
     rel_initial_text = (
         "The following are knowledge triplets "
         "in the form of (subset, predicate, object):\n"
     )
-    raw_text = "(foo, is, bar)"
-    query = rel_initial_text + "('foo', 'is', 'bar')"
+    rel_initial_text = (
+        f"The following are knowledge triplets in max depth"
+        f" {retriever.graph_store_query_depth} "
+        f"in the form of "
+        f"`subject [predicate, object, predicate_next_hop, object_next_hop ...]`"
+    )
+    raw_text = "foo ['is', 'bar']"
+    query = rel_initial_text + "\n" + raw_text
     assert len(nodes) == 2
-    assert nodes[0].node.get_text() == raw_text
-    assert nodes[1].node.get_text() == query
+    assert nodes[1].node.get_content() == query
 
 
 @patch.object(
-    GPTKnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
+    KnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
 )
 def test_retrievers(
     _patch_extract_triplets: Any,
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     # test specific retriever class
-    index = GPTKnowledgeGraphIndex.from_documents(
-        documents, service_context=mock_service_context
+    graph_store = SimpleGraphStore()
+    storage_context = StorageContext.from_defaults(graph_store=graph_store)
+
+    index = KnowledgeGraphIndex.from_documents(
+        documents, service_context=mock_service_context, storage_context=storage_context
     )
     retriever = KGTableRetriever(
         index,
         query_keyword_extract_template=MOCK_QUERY_KEYWORD_EXTRACT_PROMPT,
+        graph_store=graph_store,
     )
     query_bundle = QueryBundle(query_str="foo", custom_embedding_strs=["foo"])
     nodes = retriever.retrieve(query_bundle)
-    assert nodes[0].node.get_text() == "(foo, is, bar)"
     assert (
-        nodes[1].node.get_text() == "The following are knowledge triplets in the "
-        "form of (subset, predicate, object):\n('foo', 'is', 'bar')"
+        nodes[1].node.get_content()
+        == "The following are knowledge triplets in max depth 2"
+        " in the form of "
+        "`subject [predicate, object, predicate_next_hop, object_next_hop ...]`"
+        "\nfoo ['is', 'bar']"
     )
 
 
 @patch.object(
-    GPTKnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
+    KnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
 )
 def test_retriever_no_text(
     _patch_extract_triplets: Any,
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     # test specific retriever class
-    index = GPTKnowledgeGraphIndex.from_documents(
-        documents, service_context=mock_service_context
+    graph_store = SimpleGraphStore()
+    storage_context = StorageContext.from_defaults(graph_store=graph_store)
+
+    index = KnowledgeGraphIndex.from_documents(
+        documents, service_context=mock_service_context, storage_context=storage_context
     )
     retriever = KGTableRetriever(
         index,
         query_keyword_extract_template=MOCK_QUERY_KEYWORD_EXTRACT_PROMPT,
         include_text=False,
+        graph_store=graph_store,
     )
     query_bundle = QueryBundle(query_str="foo", custom_embedding_strs=["foo"])
     nodes = retriever.retrieve(query_bundle)
     assert (
-        nodes[0].node.get_text()
-        == "The following are knowledge triplets in the form of "
-        "(subset, predicate, object):\n('foo', 'is', 'bar')"
+        nodes[0].node.get_content()
+        == "The following are knowledge triplets in max depth 2"
+        " in the form of "
+        "`subject [predicate, object, predicate_next_hop, object_next_hop ...]`"
+        "\nfoo ['is', 'bar']"
     )
 
 
 @patch.object(
-    GPTKnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
+    KnowledgeGraphIndex, "_extract_triplets", side_effect=mock_extract_triplets
 )
 def test_retrieve_similarity(
     _patch_extract_triplets: Any,
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test query."""
     mock_service_context.embed_model = MockEmbedding()
-    index = GPTKnowledgeGraphIndex.from_documents(
-        documents, include_embeddings=True, service_context=mock_service_context
+    graph_store = SimpleGraphStore()
+    storage_context = StorageContext.from_defaults(graph_store=graph_store)
+
+    index = KnowledgeGraphIndex.from_documents(
+        documents,
+        include_embeddings=True,
+        service_context=mock_service_context,
+        storage_context=storage_context,
     )
-    retriever = KGTableRetriever(index, similarity_top_k=2)
+    retriever = KGTableRetriever(index, similarity_top_k=2, graph_store=graph_store)
 
     # returns only two rel texts to use for generating response
     # uses hyrbid query by default
     nodes = retriever.retrieve(QueryBundle("foo"))
-    assert len(nodes) == 2
+    assert (
+        nodes[1].node.get_content()
+        == "The following are knowledge triplets in max depth 2"
+        " in the form of "
+        "`subject [predicate, object, predicate_next_hop, object_next_hop ...]`"
+        "\nfoo ['is', 'bar']"
+    )
```

### Comparing `llama_index-0.6.9/tests/indices/list/test_index.py` & `llama_index-0.7.0/tests/indices/list/test_index.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,178 +1,184 @@
 """Test list index."""
 
 from typing import Dict, List, Tuple
 
-from llama_index.data_structs.node import Node
 from llama_index.indices.base_retriever import BaseRetriever
-from llama_index.indices.list.base import GPTListIndex, ListRetrieverMode
+from llama_index.indices.list.base import ListIndex, ListRetrieverMode
 from llama_index.indices.service_context import ServiceContext
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
+from llama_index.schema import BaseNode
 
 
 def test_build_list(
     documents: List[Document], mock_service_context: ServiceContext
 ) -> None:
     """Test build list."""
-    list_index = GPTListIndex.from_documents(
+    list_index = ListIndex.from_documents(
         documents, service_context=mock_service_context
     )
     assert len(list_index.index_struct.nodes) == 4
     # check contents of nodes
     node_ids = list_index.index_struct.nodes
     nodes = list_index.docstore.get_nodes(node_ids)
-    assert nodes[0].text == "Hello world."
-    assert nodes[1].text == "This is a test."
-    assert nodes[2].text == "This is another test."
-    assert nodes[3].text == "This is a test v2."
+    assert nodes[0].get_content() == "Hello world."
+    assert nodes[1].get_content() == "This is a test."
+    assert nodes[2].get_content() == "This is another test."
+    assert nodes[3].get_content() == "This is a test v2."
 
 
 def test_refresh_list(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test build list."""
     # add extra document
-    more_documents = documents + [Document("Test document 2")]
+    more_documents = documents + [Document(text="Test document 2")]
 
     # ensure documents have doc_id
     for i in range(len(more_documents)):
-        more_documents[i].doc_id = str(i)
+        more_documents[i].doc_id = str(i)  # type: ignore[misc]
 
     # create index
-    list_index = GPTListIndex.from_documents(
+    list_index = ListIndex.from_documents(
         more_documents, service_context=mock_service_context
     )
 
     # check that no documents are refreshed
-    refreshed_docs = list_index.refresh(more_documents)
+    refreshed_docs = list_index.refresh_ref_docs(more_documents)
     assert refreshed_docs[0] is False
     assert refreshed_docs[1] is False
 
     # modify a document and test again
-    more_documents = documents + [Document("Test document 2, now with changes!")]
+    more_documents = documents + [Document(text="Test document 2, now with changes!")]
     for i in range(len(more_documents)):
-        more_documents[i].doc_id = str(i)
+        more_documents[i].doc_id = str(i)  # type: ignore[misc]
 
     # second document should refresh
-    refreshed_docs = list_index.refresh(more_documents)
+    refreshed_docs = list_index.refresh_ref_docs(more_documents)
     assert refreshed_docs[0] is False
     assert refreshed_docs[1] is True
 
     test_node = list_index.docstore.get_node(list_index.index_struct.nodes[-1])
-    assert test_node.text == "Test document 2, now with changes!"
+    assert test_node.get_content() == "Test document 2, now with changes!"
 
 
 def test_build_list_multiple(mock_service_context: ServiceContext) -> None:
     """Test build list multiple."""
     documents = [
-        Document("Hello world.\nThis is a test."),
-        Document("This is another test.\nThis is a test v2."),
+        Document(text="Hello world.\nThis is a test."),
+        Document(text="This is another test.\nThis is a test v2."),
     ]
-    list_index = GPTListIndex.from_documents(
+    list_index = ListIndex.from_documents(
         documents, service_context=mock_service_context
     )
     assert len(list_index.index_struct.nodes) == 4
     nodes = list_index.docstore.get_nodes(list_index.index_struct.nodes)
     # check contents of nodes
-    assert nodes[0].text == "Hello world."
-    assert nodes[1].text == "This is a test."
-    assert nodes[2].text == "This is another test."
-    assert nodes[3].text == "This is a test v2."
+    assert nodes[0].get_content() == "Hello world."
+    assert nodes[1].get_content() == "This is a test."
+    assert nodes[2].get_content() == "This is another test."
+    assert nodes[3].get_content() == "This is a test v2."
 
 
 def test_list_insert(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test insert to list."""
-    list_index = GPTListIndex([], service_context=mock_service_context)
+    list_index = ListIndex([], service_context=mock_service_context)
     assert len(list_index.index_struct.nodes) == 0
     list_index.insert(documents[0])
     nodes = list_index.docstore.get_nodes(list_index.index_struct.nodes)
     # check contents of nodes
-    assert nodes[0].text == "Hello world."
-    assert nodes[1].text == "This is a test."
-    assert nodes[2].text == "This is another test."
-    assert nodes[3].text == "This is a test v2."
+    assert nodes[0].get_content() == "Hello world."
+    assert nodes[1].get_content() == "This is a test."
+    assert nodes[2].get_content() == "This is another test."
+    assert nodes[3].get_content() == "This is a test v2."
 
     # test insert with ID
     document = documents[0]
-    document.doc_id = "test_id"
-    list_index = GPTListIndex([])
+    document.doc_id = "test_id"  # type: ignore[misc]
+    list_index = ListIndex([])
     list_index.insert(document)
     # check contents of nodes
     nodes = list_index.docstore.get_nodes(list_index.index_struct.nodes)
     # check contents of nodes
     for node in nodes:
         assert node.ref_doc_id == "test_id"
 
 
 def test_list_delete(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test insert to list and then delete."""
     new_documents = [
-        Document("Hello world.\nThis is a test.", doc_id="test_id_1"),
-        Document("This is another test.", doc_id="test_id_2"),
-        Document("This is a test v2.", doc_id="test_id_3"),
+        Document(text="Hello world.\nThis is a test.", id_="test_id_1"),
+        Document(text="This is another test.", id_="test_id_2"),
+        Document(text="This is a test v2.", id_="test_id_3"),
     ]
 
-    # delete from documents
-    list_index = GPTListIndex.from_documents(
+    list_index = ListIndex.from_documents(
         new_documents, service_context=mock_service_context
     )
-    list_index.delete("test_id_1")
+
+    # test ref doc info for three docs
+    all_ref_doc_info = list_index.ref_doc_info
+    for idx, ref_doc_id in enumerate(all_ref_doc_info.keys()):
+        assert new_documents[idx].doc_id == ref_doc_id
+
+    # delete from documents
+    list_index.delete_ref_doc("test_id_1")
     assert len(list_index.index_struct.nodes) == 2
     nodes = list_index.docstore.get_nodes(list_index.index_struct.nodes)
     assert nodes[0].ref_doc_id == "test_id_2"
-    assert nodes[0].text == "This is another test."
+    assert nodes[0].get_content() == "This is another test."
     assert nodes[1].ref_doc_id == "test_id_3"
-    assert nodes[1].text == "This is a test v2."
+    assert nodes[1].get_content() == "This is a test v2."
     # check that not in docstore anymore
     source_doc = list_index.docstore.get_document("test_id_1", raise_error=False)
     assert source_doc is None
 
-    list_index = GPTListIndex.from_documents(
+    list_index = ListIndex.from_documents(
         new_documents, service_context=mock_service_context
     )
-    list_index.delete("test_id_2")
+    list_index.delete_ref_doc("test_id_2")
     assert len(list_index.index_struct.nodes) == 3
     nodes = list_index.docstore.get_nodes(list_index.index_struct.nodes)
     assert nodes[0].ref_doc_id == "test_id_1"
-    assert nodes[0].text == "Hello world."
+    assert nodes[0].get_content() == "Hello world."
     assert nodes[1].ref_doc_id == "test_id_1"
-    assert nodes[1].text == "This is a test."
+    assert nodes[1].get_content() == "This is a test."
     assert nodes[2].ref_doc_id == "test_id_3"
-    assert nodes[2].text == "This is a test v2."
+    assert nodes[2].get_content() == "This is a test v2."
 
 
 def _get_embeddings(
-    query_str: str, nodes: List[Node]
+    query_str: str, nodes: List[BaseNode]
 ) -> Tuple[List[float], List[List[float]]]:
     """Get node text embedding similarity."""
     text_embed_map: Dict[str, List[float]] = {
         "Hello world.": [1.0, 0.0, 0.0, 0.0, 0.0],
         "This is a test.": [0.0, 1.0, 0.0, 0.0, 0.0],
         "This is another test.": [0.0, 0.0, 1.0, 0.0, 0.0],
         "This is a test v2.": [0.0, 0.0, 0.0, 1.0, 0.0],
     }
     node_embeddings = []
     for node in nodes:
-        node_embeddings.append(text_embed_map[node.get_text()])
+        node_embeddings.append(text_embed_map[node.get_content()])
 
     return [1.0, 0, 0, 0, 0], node_embeddings
 
 
 def test_as_retriever(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
-    list_index = GPTListIndex.from_documents(
+    list_index = ListIndex.from_documents(
         documents, service_context=mock_service_context
     )
     default_retriever = list_index.as_retriever(
         retriever_mode=ListRetrieverMode.DEFAULT
     )
     assert isinstance(default_retriever, BaseRetriever)
```

### Comparing `llama_index-0.6.9/tests/indices/list/test_retrievers.py` & `llama_index-0.7.0/tests/indices/list/test_retrievers.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,84 +1,83 @@
-from typing import Any, List, Tuple
+from typing import Any, List
 from unittest.mock import patch
-from llama_index.indices.list.base import GPTListIndex
+
+from llama_index.indices.list.base import ListIndex
 from llama_index.indices.list.retrievers import ListIndexEmbeddingRetriever
-from llama_index.llm_predictor.base import LLMPredictor
 from llama_index.indices.service_context import ServiceContext
-from llama_index.readers.schema.base import Document
-from llama_index.prompts.prompts import Prompt
+from llama_index.llm_predictor.base import LLMPredictor
 from llama_index.prompts.choice_select import ChoiceSelectPrompt
+from llama_index.prompts.prompts import Prompt
+from llama_index.schema import Document
 from tests.indices.list.test_index import _get_embeddings
 
 
 def test_retrieve_default(
     documents: List[Document], mock_service_context: ServiceContext
 ) -> None:
     """Test list query."""
-    index = GPTListIndex.from_documents(documents, service_context=mock_service_context)
+    index = ListIndex.from_documents(documents, service_context=mock_service_context)
 
     query_str = "What is?"
     retriever = index.as_retriever(retriever_mode="default")
     nodes = retriever.retrieve(query_str)
 
-    for node_with_score, line in zip(nodes, documents[0].get_text().split("\n")):
-        assert node_with_score.node.text == line
+    for node_with_score, line in zip(nodes, documents[0].get_content().split("\n")):
+        assert node_with_score.node.get_content() == line
 
 
 @patch.object(
     ListIndexEmbeddingRetriever,
     "_get_embeddings",
     side_effect=_get_embeddings,
 )
 def test_embedding_query(
     _patch_get_embeddings: Any,
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test embedding query."""
-    index = GPTListIndex.from_documents(documents, service_context=mock_service_context)
+    index = ListIndex.from_documents(documents, service_context=mock_service_context)
 
     # test embedding query
     query_str = "What is?"
     retriever = index.as_retriever(retriever_mode="embedding", similarity_top_k=1)
     nodes = retriever.retrieve(query_str)
     assert len(nodes) == 1
 
-    assert nodes[0].node.text == "Hello world."
+    assert nodes[0].node.get_content() == "Hello world."
 
 
-def mock_llmpredictor_predict(
-    self: Any, prompt: Prompt, **prompt_args: Any
-) -> Tuple[str, str]:
+def mock_llmpredictor_predict(self: Any, prompt: Prompt, **prompt_args: Any) -> str:
     """Patch llm predictor predict."""
     assert isinstance(prompt, ChoiceSelectPrompt)
-    return "Doc: 2, Relevance: 5", ""
+    return "Doc: 2, Relevance: 5"
 
 
 @patch.object(
     LLMPredictor,
     "predict",
     mock_llmpredictor_predict,
 )
 def test_llm_query(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test llm query."""
-    index = GPTListIndex.from_documents(documents, service_context=mock_service_context)
+    index = ListIndex.from_documents(documents, service_context=mock_service_context)
 
     # test llm query (batch size 10)
     query_str = "What is?"
     retriever = index.as_retriever(retriever_mode="llm")
     nodes = retriever.retrieve(query_str)
     assert len(nodes) == 1
 
-    assert nodes[0].node.text == "This is a test."
+    assert nodes[0].node.get_content() == "This is a test."
 
     # test llm query (batch size 2)
     query_str = "What is?"
     retriever = index.as_retriever(retriever_mode="llm", choice_batch_size=2)
     nodes = retriever.retrieve(query_str)
     assert len(nodes) == 2
 
-    assert nodes[0].node.text == "This is a test."
-    assert nodes[1].node.text == "This is a test v2."
+    assert nodes[0].node.get_content() == "This is a test."
+    assert nodes[1].node.get_content() == "This is a test v2."
```

### Comparing `llama_index-0.6.9/tests/indices/postprocessor/test_llm_rerank.py` & `llama_index-0.7.0/tests/indices/postprocessor/test_llm_rerank.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,23 +1,21 @@
 """Test LLM reranker."""
 
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.prompts.prompts import Prompt
-from llama_index.data_structs.node import Node, NodeWithScore
 from llama_index.llm_predictor import LLMPredictor
 from unittest.mock import patch
-from typing import List, Any, Tuple
+from typing import List, Any
 from llama_index.prompts.prompts import QuestionAnswerPrompt
 from llama_index.indices.postprocessor.llm_rerank import LLMRerank
 from llama_index.indices.service_context import ServiceContext
+from llama_index.schema import BaseNode, TextNode, NodeWithScore
 
 
-def mock_llmpredictor_predict(
-    self: Any, prompt: Prompt, **prompt_args: Any
-) -> Tuple[str, str]:
+def mock_llmpredictor_predict(self: Any, prompt: Prompt, **prompt_args: Any) -> str:
     """Patch llm predictor predict."""
     assert isinstance(prompt, QuestionAnswerPrompt)
     context_str = prompt_args["context_str"]
     node_strs = context_str.split("\n")
     node_to_choice_and_score = {
         "Test": (True, "1"),
         "Test2": (False, "0"),
@@ -31,50 +29,50 @@
     choices_and_scores = []
     for idx, node_str in enumerate(node_strs):
         choice, score = node_to_choice_and_score[node_str]
         if choice:
             choices_and_scores.append((idx + 1, score))
 
     result_strs = [f"Doc: {str(c)}, Relevance: {s}" for c, s in choices_and_scores]
-    return "\n".join(result_strs), ""
+    return "\n".join(result_strs)
 
 
-def mock_format_node_batch_fn(nodes: List[Node]) -> str:
+def mock_format_node_batch_fn(nodes: List[BaseNode]) -> str:
     """Mock format node batch fn."""
-    return "\n".join([node.get_text() for node in nodes])
+    return "\n".join([node.get_content() for node in nodes])
 
 
 @patch.object(
     LLMPredictor,
     "predict",
     mock_llmpredictor_predict,
 )
 def test_llm_rerank(mock_service_context: ServiceContext) -> None:
     """Test LLM rerank."""
     nodes = [
-        Node("Test"),
-        Node("Test2"),
-        Node("Test3"),
-        Node("Test4"),
-        Node("Test5"),
-        Node("Test6"),
-        Node("Test7"),
-        Node("Test8"),
+        TextNode(text="Test"),
+        TextNode(text="Test2"),
+        TextNode(text="Test3"),
+        TextNode(text="Test4"),
+        TextNode(text="Test5"),
+        TextNode(text="Test6"),
+        TextNode(text="Test7"),
+        TextNode(text="Test8"),
     ]
-    nodes_with_score = [NodeWithScore(n) for n in nodes]
+    nodes_with_score = [NodeWithScore(node=n) for n in nodes]
 
     # choice batch size 4 (so two batches)
     # take top-3 across all data
     llm_rerank = LLMRerank(
         format_node_batch_fn=mock_format_node_batch_fn,
         choice_batch_size=4,
         top_n=3,
         service_context=mock_service_context,
     )
     query_str = "What is?"
     result_nodes = llm_rerank.postprocess_nodes(
         nodes_with_score, QueryBundle(query_str)
     )
     assert len(result_nodes) == 3
-    assert result_nodes[0].node.text == "Test7"
-    assert result_nodes[1].node.text == "Test5"
-    assert result_nodes[2].node.text == "Test3"
+    assert result_nodes[0].node.get_content() == "Test7"
+    assert result_nodes[1].node.get_content() == "Test5"
+    assert result_nodes[2].node.get_content() == "Test3"
```

### Comparing `llama_index-0.6.9/tests/indices/query/conftest.py` & `llama_index-0.7.0/tests/indices/query/conftest.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from typing import Dict, List
 
 import pytest
 from llama_index.data_structs.struct_type import IndexStructType
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 from tests.mock_utils.mock_prompts import (
     MOCK_INSERT_PROMPT,
     MOCK_KEYWORD_EXTRACT_PROMPT,
     MOCK_QUERY_KEYWORD_EXTRACT_PROMPT,
     MOCK_QUERY_PROMPT,
     MOCK_REFINE_PROMPT,
@@ -54,17 +54,17 @@
     }
 
 
 @pytest.fixture
 def documents() -> List[Document]:
     """Get documents."""
     docs = [
-        Document("This is a test v2."),
-        Document("This is another test."),
-        Document("This is a test."),
-        Document("Hello world."),
-        Document("Hello world."),
-        Document("This is a test."),
-        Document("This is another test."),
-        Document("This is a test v2."),
+        Document(text="This is a test v2."),
+        Document(text="This is another test."),
+        Document(text="This is a test."),
+        Document(text="Hello world."),
+        Document(text="Hello world."),
+        Document(text="This is a test."),
+        Document(text="This is another test."),
+        Document(text="This is a test v2."),
     ]
     return docs
```

### Comparing `llama_index-0.6.9/tests/indices/query/query_transform/test_base.py` & `llama_index-0.7.0/tests/indices/query/query_transform/test_base.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/indices/query/test_compose.py` & `llama_index-0.7.0/tests/indices/query/test_compose.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,51 +1,51 @@
 """Test composing indices."""
 
 from typing import Dict, List
 
 from llama_index.indices.composability.graph import ComposableGraph
-from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex
-from llama_index.indices.list.base import GPTListIndex
+from llama_index.indices.keyword_table.simple_base import SimpleKeywordTableIndex
+from llama_index.indices.list.base import ListIndex
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.tree.base import GPTTreeIndex
-from llama_index.readers.schema.base import Document
+from llama_index.indices.tree.base import TreeIndex
+from llama_index.schema import Document
 
 
 def test_recursive_query_list_tree(
     documents: List[Document],
     mock_service_context: ServiceContext,
     index_kwargs: Dict,
 ) -> None:
     """Test query."""
     list_kwargs = index_kwargs["list"]
     tree_kwargs = index_kwargs["tree"]
     # try building a list for every two, then a tree
-    list1 = GPTListIndex.from_documents(
+    list1 = ListIndex.from_documents(
         documents[0:2], service_context=mock_service_context, **list_kwargs
     )
-    list2 = GPTListIndex.from_documents(
+    list2 = ListIndex.from_documents(
         documents[2:4], service_context=mock_service_context, **list_kwargs
     )
-    list3 = GPTListIndex.from_documents(
+    list3 = ListIndex.from_documents(
         documents[4:6], service_context=mock_service_context, **list_kwargs
     )
-    list4 = GPTListIndex.from_documents(
+    list4 = ListIndex.from_documents(
         documents[6:8], service_context=mock_service_context, **list_kwargs
     )
 
     summary1 = "summary1"
     summary2 = "summary2"
     summary3 = "summary3"
     summary4 = "summary4"
     summaries = [summary1, summary2, summary3, summary4]
 
     # there are two root nodes in this tree: one containing [list1, list2]
     # and the other containing [list3, list4]
     graph = ComposableGraph.from_indices(
-        GPTTreeIndex,
+        TreeIndex,
         [
             list1,
             list2,
             list3,
             list4,
         ],
         index_summaries=summaries,
@@ -69,31 +69,31 @@
     index_kwargs: Dict,
 ) -> None:
     """Test query."""
     list_kwargs = index_kwargs["list"]
     tree_kwargs = index_kwargs["tree"]
     # try building a tree for a group of 4, then a list
     # use a diff set of documents
-    tree1 = GPTTreeIndex.from_documents(
+    tree1 = TreeIndex.from_documents(
         documents[2:6], service_context=mock_service_context, **tree_kwargs
     )
-    tree2 = GPTTreeIndex.from_documents(
+    tree2 = TreeIndex.from_documents(
         documents[:2] + documents[6:],
         service_context=mock_service_context,
         **tree_kwargs
     )
     summaries = [
         "tree_summary1",
         "tree_summary2",
     ]
 
     # there are two root nodes in this tree: one containing [list1, list2]
     # and the other containing [list3, list4]
     graph = ComposableGraph.from_indices(
-        GPTListIndex,
+        ListIndex,
         [tree1, tree2],
         index_summaries=summaries,
         service_context=mock_service_context,
         **list_kwargs
     )
     assert isinstance(graph, ComposableGraph)
     query_str = "What is?"
@@ -112,27 +112,27 @@
     index_kwargs: Dict,
 ) -> None:
     """Test query."""
     list_kwargs = index_kwargs["list"]
     table_kwargs = index_kwargs["table"]
     # try building a tree for a group of 4, then a list
     # use a diff set of documents
-    table1 = GPTSimpleKeywordTableIndex.from_documents(
+    table1 = SimpleKeywordTableIndex.from_documents(
         documents[4:6], service_context=mock_service_context, **table_kwargs
     )
-    table2 = GPTSimpleKeywordTableIndex.from_documents(
+    table2 = SimpleKeywordTableIndex.from_documents(
         documents[2:3], service_context=mock_service_context, **table_kwargs
     )
     summaries = [
         "table_summary1",
         "table_summary2",
     ]
 
     graph = ComposableGraph.from_indices(
-        GPTListIndex,
+        ListIndex,
         [table1, table2],
         index_summaries=summaries,
         service_context=mock_service_context,
         **list_kwargs
     )
     assert isinstance(graph, ComposableGraph)
     query_str = "World?"
@@ -152,35 +152,35 @@
 ) -> None:
     """Test query."""
     list_kwargs = index_kwargs["list"]
     table_kwargs = index_kwargs["table"]
     # try building a tree for a group of 4, then a list
     # use a diff set of documents
     # try building a list for every two, then a tree
-    list1 = GPTListIndex.from_documents(
+    list1 = ListIndex.from_documents(
         documents[0:2], service_context=mock_service_context, **list_kwargs
     )
-    list2 = GPTListIndex.from_documents(
+    list2 = ListIndex.from_documents(
         documents[2:4], service_context=mock_service_context, **list_kwargs
     )
-    list3 = GPTListIndex.from_documents(
+    list3 = ListIndex.from_documents(
         documents[4:6], service_context=mock_service_context, **list_kwargs
     )
-    list4 = GPTListIndex.from_documents(
+    list4 = ListIndex.from_documents(
         documents[6:8], service_context=mock_service_context, **list_kwargs
     )
     summaries = [
         "foo bar",
         "apple orange",
         "toronto london",
         "cat dog",
     ]
 
     graph = ComposableGraph.from_indices(
-        GPTSimpleKeywordTableIndex,
+        SimpleKeywordTableIndex,
         [list1, list2, list3, list4],
         index_summaries=summaries,
         service_context=mock_service_context,
         **table_kwargs
     )
     assert isinstance(graph, ComposableGraph)
     query_str = "Foo?"
```

### Comparing `llama_index-0.6.9/tests/indices/query/test_compose_vector.py` & `llama_index-0.7.0/tests/indices/query/test_compose_vector.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,18 +4,18 @@
 from typing import Any, Dict, List
 
 import pytest
 
 from llama_index.data_structs.data_structs import IndexStruct
 from llama_index.embeddings.base import BaseEmbedding
 from llama_index.indices.composability.graph import ComposableGraph
-from llama_index.indices.keyword_table.simple_base import GPTSimpleKeywordTableIndex
+from llama_index.indices.keyword_table.simple_base import SimpleKeywordTableIndex
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
-from llama_index.readers.schema.base import Document
+from llama_index.indices.vector_store.base import VectorStoreIndex
+from llama_index.schema import Document
 from tests.indices.vector_store.utils import get_pinecone_storage_context
 from tests.mock_utils.mock_prompts import MOCK_QUERY_KEYWORD_EXTRACT_PROMPT
 
 
 class MockEmbedding(BaseEmbedding):
     def _get_query_embedding(self, query: str) -> List[float]:
         """Mock get query embedding."""
@@ -65,37 +65,37 @@
 ) -> None:
     """Test query."""
     vector_kwargs = index_kwargs["vector"]
     table_kwargs = index_kwargs["table"]
     # try building a tree for a group of 4, then a list
     # use a diff set of documents
     # try building a list for every two, then a tree
-    vector1 = GPTVectorStoreIndex.from_documents(
+    vector1 = VectorStoreIndex.from_documents(
         documents[0:2], service_context=mock_service_context, **vector_kwargs
     )
-    vector2 = GPTVectorStoreIndex.from_documents(
+    vector2 = VectorStoreIndex.from_documents(
         documents[2:4], service_context=mock_service_context, **vector_kwargs
     )
-    list3 = GPTVectorStoreIndex.from_documents(
+    list3 = VectorStoreIndex.from_documents(
         documents[4:6], service_context=mock_service_context, **vector_kwargs
     )
-    list4 = GPTVectorStoreIndex.from_documents(
+    list4 = VectorStoreIndex.from_documents(
         documents[6:8], service_context=mock_service_context, **vector_kwargs
     )
     indices = [vector1, vector2, list3, list4]
 
     summaries = [
         "foo bar",
         "apple orange",
         "toronto london",
         "cat dog",
     ]
 
     graph = ComposableGraph.from_indices(
-        GPTSimpleKeywordTableIndex,
+        SimpleKeywordTableIndex,
         indices,
         index_summaries=summaries,
         service_context=mock_service_context,
         **table_kwargs
     )
 
     custom_query_engines = {
@@ -129,31 +129,31 @@
 
     """
     vector_kwargs = index_kwargs["vector"]
     table_kwargs = index_kwargs["table"]
     # try building a tre for a group of 4, then a list
     # use a diff set of documents
     # try building a list for every two, then a tree
-    vector1 = GPTVectorStoreIndex.from_documents(
+    vector1 = VectorStoreIndex.from_documents(
         documents[0:2], service_context=mock_service_context, **vector_kwargs
     )
-    vector2 = GPTVectorStoreIndex.from_documents(
+    vector2 = VectorStoreIndex.from_documents(
         documents[2:4], service_context=mock_service_context, **vector_kwargs
     )
     assert isinstance(vector1.index_struct, IndexStruct)
     assert isinstance(vector2.index_struct, IndexStruct)
     vector1.index_struct.index_id = "vector1"
     vector2.index_struct.index_id = "vector2"
     summaries = [
         "foo bar",
         "apple orange",
     ]
 
     graph = ComposableGraph.from_indices(
-        GPTSimpleKeywordTableIndex,
+        SimpleKeywordTableIndex,
         [vector1, vector2],
         index_summaries=summaries,
         service_context=mock_service_context,
         **table_kwargs
     )
     assert isinstance(graph, ComposableGraph)
 
@@ -181,37 +181,37 @@
 ) -> None:
     """Test async query of table index over vector indices."""
     vector_kwargs = index_kwargs["vector"]
     table_kwargs = index_kwargs["table"]
     # try building a tree for a group of 4, then a list
     # use a diff set of documents
     # try building a list for every two, then a tree
-    vector1 = GPTVectorStoreIndex.from_documents(
+    vector1 = VectorStoreIndex.from_documents(
         documents[0:2], service_context=mock_service_context, **vector_kwargs
     )
-    vector2 = GPTVectorStoreIndex.from_documents(
+    vector2 = VectorStoreIndex.from_documents(
         documents[2:4], service_context=mock_service_context, **vector_kwargs
     )
-    list3 = GPTVectorStoreIndex.from_documents(
+    list3 = VectorStoreIndex.from_documents(
         documents[4:6], service_context=mock_service_context, **vector_kwargs
     )
-    list4 = GPTVectorStoreIndex.from_documents(
+    list4 = VectorStoreIndex.from_documents(
         documents[6:8], service_context=mock_service_context, **vector_kwargs
     )
     indices = [vector1, vector2, list3, list4]
 
     summaries = [
         "foo bar",
         "apple orange",
         "toronto london",
         "cat dog",
     ]
 
     graph = ComposableGraph.from_indices(
-        GPTSimpleKeywordTableIndex,
+        SimpleKeywordTableIndex,
         children_indices=indices,
         index_summaries=summaries,
         service_context=mock_service_context,
         **table_kwargs
     )
 
     custom_query_engines = {
@@ -233,37 +233,37 @@
     index_kwargs: Dict,
 ) -> None:
     """Test query."""
     vector_kwargs = index_kwargs["vector"]
     # try building a tree for a group of 4, then a list
     # use a diff set of documents
     # try building a list for every two, then a tree
-    vector1 = GPTVectorStoreIndex.from_documents(
+    vector1 = VectorStoreIndex.from_documents(
         documents[0:2], service_context=mock_service_context, **vector_kwargs
     )
-    vector2 = GPTVectorStoreIndex.from_documents(
+    vector2 = VectorStoreIndex.from_documents(
         documents[2:4], service_context=mock_service_context, **vector_kwargs
     )
-    list3 = GPTVectorStoreIndex.from_documents(
+    list3 = VectorStoreIndex.from_documents(
         documents[4:6], service_context=mock_service_context, **vector_kwargs
     )
-    list4 = GPTVectorStoreIndex.from_documents(
+    list4 = VectorStoreIndex.from_documents(
         documents[6:8], service_context=mock_service_context, **vector_kwargs
     )
 
     indices = [vector1, vector2, list3, list4]
 
     summary1 = "foo bar"
     summary2 = "apple orange"
     summary3 = "toronto london"
     summary4 = "cat dog"
     summaries = [summary1, summary2, summary3, summary4]
 
     graph = ComposableGraph.from_indices(
-        GPTVectorStoreIndex,
+        VectorStoreIndex,
         children_indices=indices,
         index_summaries=summaries,
         service_context=mock_service_context,
         **vector_kwargs
     )
     custom_query_engines = {
         index.index_id: index.as_query_engine(similarity_top_k=1) for index in indices
@@ -290,63 +290,63 @@
     index_kwargs: Dict,
 ) -> None:
     """Test composing pinecone index on top of pinecone index."""
     pinecone_kwargs = index_kwargs["pinecone"]
     # try building a tree for a group of 4, then a list
     # use a diff set of documents
     # try building a list for every two, then a tree
-    pinecone1 = GPTVectorStoreIndex.from_documents(
+    pinecone1 = VectorStoreIndex.from_documents(
         documents[0:2],
         storage_context=get_pinecone_storage_context(),
         service_context=mock_service_context,
         **pinecone_kwargs
     )
-    pinecone2 = GPTVectorStoreIndex.from_documents(
+    pinecone2 = VectorStoreIndex.from_documents(
         documents[2:4],
         storage_context=get_pinecone_storage_context(),
         service_context=mock_service_context,
         **pinecone_kwargs
     )
-    pinecone3 = GPTVectorStoreIndex.from_documents(
+    pinecone3 = VectorStoreIndex.from_documents(
         documents[4:6],
         storage_context=get_pinecone_storage_context(),
         service_context=mock_service_context,
         **pinecone_kwargs
     )
-    pinecone4 = GPTVectorStoreIndex.from_documents(
+    pinecone4 = VectorStoreIndex.from_documents(
         documents[6:8],
         storage_context=get_pinecone_storage_context(),
         service_context=mock_service_context,
         **pinecone_kwargs
     )
     indices = [pinecone1, pinecone2, pinecone3, pinecone4]
 
     summary1 = "foo bar"
     summary2 = "apple orange"
     summary3 = "toronto london"
     summary4 = "cat dog"
     summaries = [summary1, summary2, summary3, summary4]
 
     graph = ComposableGraph.from_indices(
-        GPTVectorStoreIndex,
+        VectorStoreIndex,
         children_indices=indices,
         index_summaries=summaries,
         storage_context=get_pinecone_storage_context(),
         service_context=mock_service_context,
         **pinecone_kwargs
     )
-    query_str = "Foo?"
     custom_query_engines = {
         index.index_id: index.as_query_engine(similarity_top_k=1) for index in indices
     }
     custom_query_engines[graph.root_id] = graph.root_index.as_query_engine(
         similarity_top_k=1
     )
     query_engine = graph.as_query_engine(custom_query_engines=custom_query_engines)
+    query_str = "Foo?"
     response = query_engine.query(query_str)
-    assert str(response) == ("Foo?:Foo?:This is another test.")
+    # assert str(response) == ("Foo?:Foo?:This is another test.")
     query_str = "Orange?"
     response = query_engine.query(query_str)
-    assert str(response) == ("Orange?:Orange?:This is a test.")
+    # assert str(response) == ("Orange?:Orange?:This is a test.")
     query_str = "Cat?"
     response = query_engine.query(query_str)
     assert str(response) == ("Cat?:Cat?:This is a test v2.")
```

### Comparing `llama_index-0.6.9/tests/indices/query/test_query_bundle.py` & `llama_index-0.7.0/tests/indices/query/test_query_bundle.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,32 +1,32 @@
 """Test query bundle."""
 
 from typing import Dict, List
 
 import pytest
-from llama_index.embeddings.base import BaseEmbedding
 
-from llama_index.indices.list.base import GPTListIndex
+from llama_index.embeddings.base import BaseEmbedding
+from llama_index.indices.list.base import ListIndex
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 @pytest.fixture
 def documents() -> List[Document]:
     """Get documents."""
     # NOTE: one document for now
     doc_text = (
         "Correct.\n"
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
-    return [Document(doc_text)]
+    return [Document(text=doc_text)]
 
 
 class MockEmbedding(BaseEmbedding):
     def _get_text_embedding(self, text: str) -> List[float]:
         """Get node text embedding."""
         text_embed_map: Dict[str, List[float]] = {
             "Correct.": [0.5, 0.5, 0.0, 0.0, 0.0],
@@ -50,21 +50,21 @@
 
 def test_embedding_query(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test embedding query."""
     mock_service_context.embed_model = MockEmbedding()
-    index = GPTListIndex.from_documents(documents, service_context=mock_service_context)
+    index = ListIndex.from_documents(documents, service_context=mock_service_context)
 
     # test embedding query
     query_bundle = QueryBundle(
         query_str="What is?",
         custom_embedding_strs=[
             "It is what it is.",
             "The meaning of life",
         ],
     )
     retriever = index.as_retriever(retriever_mode="embedding", similarity_top_k=1)
     nodes = retriever.retrieve(query_bundle)
     assert len(nodes) == 1
-    assert nodes[0].node.text == "Correct."
+    assert nodes[0].node.get_content() == "Correct."
```

### Comparing `llama_index-0.6.9/tests/indices/struct_store/conftest.py` & `llama_index-0.7.0/tests/indices/struct_store/conftest.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/indices/struct_store/test_base.py` & `llama_index-0.7.0/tests/indices/struct_store/test_base.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,105 +1,101 @@
 """Test struct store indices."""
 
 from typing import Any, Dict, List, Tuple
 
-
 from sqlalchemy import (
     Column,
     Integer,
     MetaData,
     String,
     Table,
     create_engine,
     delete,
     select,
 )
 
-from llama_index.indices.list.base import GPTListIndex
+from llama_index.indices.list.base import ListIndex
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
 from llama_index.indices.struct_store.sql import (
-    GPTSQLStructStoreIndex,
+    SQLStructStoreIndex,
     SQLContextContainerBuilder,
 )
-from llama_index.indices.struct_store.sql_query import GPTNLStructStoreQueryEngine
+from llama_index.indices.struct_store.sql_query import NLStructStoreQueryEngine
 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase
-from llama_index.readers.schema.base import Document
-from llama_index.schema import BaseDocument
-from tests.mock_utils.mock_prompts import (
-    MOCK_TABLE_CONTEXT_PROMPT,
-)
-from llama_index.data_structs.node import Node, DocumentRelationship
+from llama_index.schema import Document
+from llama_index.schema import BaseNode, NodeRelationship, TextNode, RelatedNodeInfo
+from tests.mock_utils.mock_prompts import MOCK_TABLE_CONTEXT_PROMPT
 
 
 def _delete_table_items(engine: Any, table: Table) -> None:
     """Delete items from a table."""
     delete_stmt = delete(table)
     with engine.connect() as connection:
         connection.execute(delete_stmt)
         connection.commit()
 
 
 def test_sql_index(
     mock_service_context: ServiceContext,
     struct_kwargs: Tuple[Dict, Dict],
 ) -> None:
-    """Test GPTSQLStructStoreIndex."""
+    """Test SQLStructStoreIndex."""
     engine = create_engine("sqlite:///:memory:")
     metadata_obj = MetaData()
     table_name = "test_table"
     test_table = Table(
         table_name,
         metadata_obj,
         Column("user_id", Integer, primary_key=True),
         Column("foo", String(16), nullable=False),
     )
     metadata_obj.create_all(engine)
     # NOTE: we can use the default output parser for this
     index_kwargs, _ = struct_kwargs
     docs = [Document(text="user_id:2,foo:bar"), Document(text="user_id:8,foo:hello")]
     sql_database = SQLDatabase(engine, metadata=metadata_obj)
-    index = GPTSQLStructStoreIndex.from_documents(
+    index = SQLStructStoreIndex.from_documents(
         docs,
         sql_database=sql_database,
         table_name=table_name,
         service_context=mock_service_context,
         **index_kwargs
     )
-    assert isinstance(index, GPTSQLStructStoreIndex)
+    assert isinstance(index, SQLStructStoreIndex)
 
     # test that the document is inserted
     stmt = select(test_table.c["user_id", "foo"])
     engine = index.sql_database.engine
     with engine.connect() as connection:
         results = connection.execute(stmt).fetchall()
         print(results)
         assert results == [(2, "bar"), (8, "hello")]
 
     # try with documents with more text chunks
     _delete_table_items(engine, test_table)
     docs = [Document(text="user_id:2,foo:bar\nuser_id:8,foo:hello")]
-    index = GPTSQLStructStoreIndex.from_documents(
+    index = SQLStructStoreIndex.from_documents(
         docs, sql_database=sql_database, table_name=table_name, **index_kwargs
     )
-    assert isinstance(index, GPTSQLStructStoreIndex)
+    assert isinstance(index, SQLStructStoreIndex)
     # test that the document is inserted
     stmt = select(test_table.c["user_id", "foo"])
     engine = index.sql_database.engine
     with engine.connect() as connection:
         results = connection.execute(stmt).fetchall()
         connection.commit()
         assert results == [(8, "hello")]
 
 
 def test_sql_index_nodes(
     mock_service_context: ServiceContext,
     struct_kwargs: Tuple[Dict, Dict],
 ) -> None:
-    """Test GPTSQLStructStoreIndex with nodes."""
+    """Test SQLStructStoreIndex with nodes."""
     engine = create_engine("sqlite:///:memory:")
     metadata_obj = MetaData()
     table_name = "test_table"
     test_table = Table(
         table_name,
         metadata_obj,
         Column("user_id", Integer, primary_key=True),
@@ -107,78 +103,78 @@
     )
     metadata_obj.create_all(engine)
     # NOTE: we can use the default output parser for this
     index_kwargs, _ = struct_kwargs
 
     # try with different parent ids
     nodes = [
-        Node(
+        TextNode(
             text="user_id:2,foo:bar",
-            relationships={DocumentRelationship.SOURCE: "test"},
+            relationships={NodeRelationship.SOURCE: RelatedNodeInfo(node_id="test1")},
         ),
-        Node(
+        TextNode(
             text="user_id:8,foo:hello",
-            relationships={DocumentRelationship.SOURCE: "test2"},
+            relationships={NodeRelationship.SOURCE: RelatedNodeInfo(node_id="test2")},
         ),
     ]
     sql_database = SQLDatabase(engine, metadata=metadata_obj)
-    index = GPTSQLStructStoreIndex(
+    index = SQLStructStoreIndex(
         nodes,
         sql_database=sql_database,
         table_name=table_name,
         service_context=mock_service_context,
         **index_kwargs
     )
-    assert isinstance(index, GPTSQLStructStoreIndex)
+    assert isinstance(index, SQLStructStoreIndex)
 
     # test that both nodes are inserted
     stmt = select(test_table.c["user_id", "foo"])
     engine = index.sql_database.engine
     with engine.connect() as connection:
         results = connection.execute(stmt).fetchall()
         print(results)
         assert results == [(2, "bar"), (8, "hello")]
 
     _delete_table_items(engine, test_table)
 
     # try with same parent ids
     nodes = [
-        Node(
+        TextNode(
             text="user_id:2,foo:bar",
-            relationships={DocumentRelationship.SOURCE: "test"},
+            relationships={NodeRelationship.SOURCE: RelatedNodeInfo(node_id="test1")},
         ),
-        Node(
+        TextNode(
             text="user_id:8,foo:hello",
-            relationships={DocumentRelationship.SOURCE: "test"},
+            relationships={NodeRelationship.SOURCE: RelatedNodeInfo(node_id="test1")},
         ),
     ]
     sql_database = SQLDatabase(engine, metadata=metadata_obj)
-    index = GPTSQLStructStoreIndex(
+    index = SQLStructStoreIndex(
         nodes,
         sql_database=sql_database,
         table_name=table_name,
         service_context=mock_service_context,
         **index_kwargs
     )
-    assert isinstance(index, GPTSQLStructStoreIndex)
+    assert isinstance(index, SQLStructStoreIndex)
 
     # test that only one node (the last one) is inserted
     stmt = select(test_table.c["user_id", "foo"])
     engine = index.sql_database.engine
     with engine.connect() as connection:
         results = connection.execute(stmt).fetchall()
         print(results)
         assert results == [(8, "hello")]
 
 
 def test_sql_index_with_context(
     mock_service_context: ServiceContext,
     struct_kwargs: Tuple[Dict, Dict],
 ) -> None:
-    """Test GPTSQLStructStoreIndex."""
+    """Test SQLStructStoreIndex."""
     # test setting table_context_dict
     engine = create_engine("sqlite:///:memory:")
     metadata_obj = MetaData()
     table_name = "test_table"
     test_table = Table(
         table_name,
         metadata_obj,
@@ -193,70 +189,70 @@
     table_context_dict = {"test_table": "test_table_context"}
 
     # test with ignore_db_schema=True
     sql_context_container = SQLContextContainerBuilder(
         sql_database, context_dict=table_context_dict
     ).build_context_container(ignore_db_schema=True)
 
-    index = GPTSQLStructStoreIndex.from_documents(
+    index = SQLStructStoreIndex.from_documents(
         docs,
         sql_database=sql_database,
         table_name=table_name,
         sql_context_container=sql_context_container,
         service_context=mock_service_context,
         **index_kwargs
     )
-    assert isinstance(index, GPTSQLStructStoreIndex)
+    assert isinstance(index, SQLStructStoreIndex)
     assert index.sql_context_container.context_dict == table_context_dict
     _delete_table_items(engine, test_table)
 
     # test with ignore_db_schema=False (default)
     sql_database = SQLDatabase(engine)
     sql_context_container = SQLContextContainerBuilder(
         sql_database, context_dict=table_context_dict
     ).build_context_container()
 
-    index = GPTSQLStructStoreIndex.from_documents(
+    index = SQLStructStoreIndex.from_documents(
         docs,
         sql_database=sql_database,
         table_name=table_name,
         sql_context_container=sql_context_container,
         **index_kwargs
     )
-    assert isinstance(index, GPTSQLStructStoreIndex)
+    assert isinstance(index, SQLStructStoreIndex)
     for k, v in table_context_dict.items():
         context_dict = index.sql_context_container.context_dict
         assert context_dict is not None
         assert len(context_dict[k]) > len(v)
         assert v in context_dict[k]
     _delete_table_items(engine, test_table)
 
     # test setting sql_context_builder
     sql_database = SQLDatabase(engine)
     # this should cause the mock QuestionAnswer prompt to run
-    context_documents_dict: Dict[str, List[BaseDocument]] = {
-        "test_table": [Document("test_table_context")]
+    context_documents_dict: Dict[str, List[BaseNode]] = {
+        "test_table": [Document(text="test_table_context")]
     }
     sql_context_builder = SQLContextContainerBuilder.from_documents(
         context_documents_dict,
         sql_database=sql_database,
         table_context_prompt=MOCK_TABLE_CONTEXT_PROMPT,
         table_context_task="extract_test",
     )
     sql_context_container = sql_context_builder.build_context_container(
         ignore_db_schema=True
     )
-    index = GPTSQLStructStoreIndex.from_documents(
+    index = SQLStructStoreIndex.from_documents(
         docs,
         sql_database=sql_database,
         table_name=table_name,
         sql_context_container=sql_context_container,
         **index_kwargs
     )
-    assert isinstance(index, GPTSQLStructStoreIndex)
+    assert isinstance(index, SQLStructStoreIndex)
     assert index.sql_context_container.context_dict == {
         "test_table": "extract_test:test_table_context"
     }
 
     # test error if both are set
     # TODO:
 
@@ -278,28 +274,28 @@
     sql_database = SQLDatabase(engine)
     table_context_dict = {"test_table": "test_table_context"}
 
     context_builder = SQLContextContainerBuilder(
         sql_database, context_dict=table_context_dict
     )
     context_index_no_ignore = context_builder.derive_index_from_context(
-        GPTListIndex,
+        ListIndex,
     )
     context_index_with_ignore = context_builder.derive_index_from_context(
-        GPTListIndex, ignore_db_schema=True
+        ListIndex, ignore_db_schema=True
     )
     assert len(context_index_with_ignore.index_struct.nodes) == 1
     assert len(context_index_no_ignore.index_struct.nodes) > 1
 
 
 def test_sql_index_with_index_context(
     mock_service_context: ServiceContext,
     struct_kwargs: Tuple[Dict, Dict],
 ) -> None:
-    """Test GPTSQLStructStoreIndex."""
+    """Test SQLStructStoreIndex."""
     # test setting table_context_dict
     engine = create_engine("sqlite:///:memory:")
     metadata_obj = MetaData()
     table_name = "test_table"
     Table(
         table_name,
         metadata_obj,
@@ -313,15 +309,15 @@
     sql_database = SQLDatabase(engine)
     table_context_dict = {"test_table": "test_table_context"}
 
     context_builder = SQLContextContainerBuilder(
         sql_database, context_dict=table_context_dict
     )
     context_index = context_builder.derive_index_from_context(
-        GPTListIndex, ignore_db_schema=True
+        ListIndex, ignore_db_schema=True
     )
     # NOTE: the response only contains the first line (metadata), since
     # with the mock patch, newlines are treated as separate calls.
     context_response = context_builder.query_index_for_context(
         context_index,
         "Context query?",
         query_tmpl="{orig_query_str}",
@@ -332,18 +328,18 @@
     )
     print(context_response)
     assert (
         context_response == "Context query?:table_name: test_table:test_table_context"
     )
     assert sql_context_container.context_str == context_response
 
-    index = GPTSQLStructStoreIndex.from_documents(
+    index = SQLStructStoreIndex.from_documents(
         docs,
         sql_database=sql_database,
         table_name=table_name,
         sql_context_container=sql_context_container,
         service_context=mock_service_context,
         **index_kwargs
     )
     # just assert this runs
-    sql_query_engine = GPTNLStructStoreQueryEngine(index)
+    sql_query_engine = NLStructStoreQueryEngine(index)
     sql_query_engine.query(QueryBundle("test_table:foo"))
```

### Comparing `llama_index-0.6.9/tests/indices/struct_store/test_sql_query.py` & `llama_index-0.7.0/tests/indices/struct_store/test_sql_query.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,27 +1,29 @@
 import asyncio
 from typing import Any, Dict, Tuple
 
 from sqlalchemy import Column, Integer, MetaData, String, Table, create_engine
+
 from llama_index.indices.service_context import ServiceContext
 from llama_index.indices.struct_store.base import default_output_parser
-from llama_index.indices.struct_store.sql import GPTSQLStructStoreIndex
+from llama_index.indices.struct_store.sql import SQLStructStoreIndex
 from llama_index.indices.struct_store.sql_query import (
-    GPTNLStructStoreQueryEngine,
-    GPTSQLStructStoreQueryEngine,
+    NLSQLTableQueryEngine,
+    NLStructStoreQueryEngine,
+    SQLStructStoreQueryEngine,
 )
 from llama_index.langchain_helpers.sql_wrapper import SQLDatabase
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 def test_sql_index_query(
     mock_service_context: ServiceContext,
     struct_kwargs: Tuple[Dict, Dict],
 ) -> None:
-    """Test GPTSQLStructStoreIndex."""
+    """Test SQLStructStoreIndex."""
     index_kwargs, query_kwargs = struct_kwargs
     docs = [Document(text="user_id:2,foo:bar"), Document(text="user_id:8,foo:hello")]
     engine = create_engine("sqlite:///:memory:")
     metadata_obj = MetaData()
     table_name = "test_table"
     # NOTE: table is created by tying to metadata_obj
     Table(
@@ -29,39 +31,43 @@
         metadata_obj,
         Column("user_id", Integer, primary_key=True),
         Column("foo", String(16), nullable=False),
     )
     metadata_obj.create_all(engine)
     sql_database = SQLDatabase(engine)
     # NOTE: we can use the default output parser for this
-    index = GPTSQLStructStoreIndex.from_documents(
+    index = SQLStructStoreIndex.from_documents(
         docs,
         sql_database=sql_database,
         table_name=table_name,
         service_context=mock_service_context,
         **index_kwargs
     )
 
     # query the index with SQL
-    sql_query_engine = GPTSQLStructStoreQueryEngine(index, **query_kwargs)
+    sql_query_engine = SQLStructStoreQueryEngine(index, **query_kwargs)
     response = sql_query_engine.query("SELECT user_id, foo FROM test_table")
     assert str(response) == "[(2, 'bar'), (8, 'hello')]"
 
     # query the index with natural language
-    nl_query_engine = GPTNLStructStoreQueryEngine(index, **query_kwargs)
+    nl_query_engine = NLStructStoreQueryEngine(index, **query_kwargs)
     response = nl_query_engine.query("test_table:user_id,foo")
     assert str(response) == "[(2, 'bar'), (8, 'hello')]"
 
+    nl_table_engine = NLSQLTableQueryEngine(index.sql_database)
+    response = nl_table_engine.query("test_table:user_id,foo")
+    assert str(response) == "[(2, 'bar'), (8, 'hello')]"
+
 
 def test_sql_index_async_query(
     allow_networking: Any,
     mock_service_context: ServiceContext,
     struct_kwargs: Tuple[Dict, Dict],
 ) -> None:
-    """Test GPTSQLStructStoreIndex."""
+    """Test SQLStructStoreIndex."""
     index_kwargs, query_kwargs = struct_kwargs
     docs = [Document(text="user_id:2,foo:bar"), Document(text="user_id:8,foo:hello")]
     engine = create_engine("sqlite:///:memory:")
     metadata_obj = MetaData()
     table_name = "test_table"
     # NOTE: table is created by tying to metadata_obj
     Table(
@@ -69,34 +75,39 @@
         metadata_obj,
         Column("user_id", Integer, primary_key=True),
         Column("foo", String(16), nullable=False),
     )
     metadata_obj.create_all(engine)
     sql_database = SQLDatabase(engine)
     # NOTE: we can use the default output parser for this
-    index = GPTSQLStructStoreIndex.from_documents(
+    index = SQLStructStoreIndex.from_documents(
         docs,
         sql_database=sql_database,
         table_name=table_name,
         service_context=mock_service_context,
         **index_kwargs
     )
 
     # query the index with SQL
-    sql_query_engine = GPTSQLStructStoreQueryEngine(index, **query_kwargs)
+    sql_query_engine = SQLStructStoreQueryEngine(index, **query_kwargs)
     task = sql_query_engine.aquery("SELECT user_id, foo FROM test_table")
     response = asyncio.run(task)
     assert str(response) == "[(2, 'bar'), (8, 'hello')]"
 
     # query the index with natural language
-    nl_query_engine = GPTNLStructStoreQueryEngine(index, **query_kwargs)
+    nl_query_engine = NLStructStoreQueryEngine(index, **query_kwargs)
     task = nl_query_engine.aquery("test_table:user_id,foo")
     response = asyncio.run(task)
     assert str(response) == "[(2, 'bar'), (8, 'hello')]"
 
+    nl_table_engine = NLSQLTableQueryEngine(index.sql_database)
+    task = nl_table_engine.aquery("test_table:user_id,foo")
+    response = asyncio.run(task)
+    assert str(response) == "[(2, 'bar'), (8, 'hello')]"
+
 
 def test_default_output_parser() -> None:
     """Test default output parser."""
     test_str = "user_id:2\n" "foo:bar\n" ",,testing:testing2..\n" "number:123,456,789\n"
     fields = default_output_parser(test_str)
     assert fields == {
         "user_id": "2",
```

### Comparing `llama_index-0.6.9/tests/indices/test_loading.py` & `llama_index-0.7.0/tests/indices/test_loading.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 import os
 from pathlib import Path
 from typing import List
+
 import pytest
-from llama_index.data_structs.node import Node
-from llama_index.indices.list.base import GPTListIndex
+
+from llama_index.indices.list.base import ListIndex
 from llama_index.indices.loading import (
     load_index_from_storage,
     load_indices_from_storage,
 )
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
-from llama_index.readers.schema.base import Document
+from llama_index.indices.vector_store.base import VectorStoreIndex
+from llama_index.schema import Document
+from llama_index.schema import BaseNode
 from llama_index.storage.docstore.simple_docstore import SimpleDocumentStore
 from llama_index.storage.index_store.simple_index_store import SimpleIndexStore
 from llama_index.storage.storage_context import StorageContext
 from llama_index.vector_stores.faiss import FaissVectorStore
 
 
 def test_load_index_from_storage_simple(
@@ -22,15 +24,15 @@
     tmp_path: Path,
     mock_service_context: ServiceContext,
 ) -> None:
     # construct simple (i.e. in memory) storage context
     storage_context = StorageContext.from_defaults()
 
     # construct index
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents=documents,
         storage_context=storage_context,
         service_context=mock_service_context,
     )
 
     # persist storage to disk
     storage_context.persist(str(tmp_path))
@@ -43,33 +45,33 @@
         new_storage_context, service_context=mock_service_context
     )
 
     assert index.index_id == new_index.index_id
 
 
 def test_load_index_from_storage_multiple(
-    nodes: List[Node],
+    nodes: List[BaseNode],
     tmp_path: Path,
     mock_service_context: ServiceContext,
 ) -> None:
     # construct simple (i.e. in memory) storage context
     storage_context = StorageContext.from_defaults()
 
     # add nodes to docstore
     storage_context.docstore.add_documents(nodes)
 
     # construct multiple indices
-    vector_index = GPTVectorStoreIndex(
+    vector_index = VectorStoreIndex(
         nodes=nodes,
         storage_context=storage_context,
         service_context=mock_service_context,
     )
     vector_id = vector_index.index_id
 
-    list_index = GPTListIndex(
+    list_index = ListIndex(
         nodes=nodes,
         storage_context=storage_context,
         service_context=mock_service_context,
     )
 
     list_id = list_index.index_id
 
@@ -105,15 +107,15 @@
     tmp_path: Path,
     mock_service_context: ServiceContext,
 ) -> None:
     # construct simple (i.e. in memory) storage context
     storage_context = StorageContext.from_defaults()
 
     # construct index
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents=documents,
         storage_context=storage_context,
         service_context=mock_service_context,
     )
 
     nodes = index.as_retriever().retrieve("test query str")
 
@@ -145,15 +147,15 @@
     storage_context = StorageContext.from_defaults(
         docstore=SimpleDocumentStore(),
         index_store=SimpleIndexStore(),
         vector_store=FaissVectorStore(faiss_index=faiss.IndexFlatL2(5)),
     )
 
     # construct index
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents=documents,
         storage_context=storage_context,
         service_context=mock_service_context,
     )
 
     nodes = index.as_retriever().retrieve("test query str")
```

### Comparing `llama_index-0.6.9/tests/indices/test_loading_graph.py` & `llama_index-0.7.0/tests/indices/test_loading_graph.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,46 +1,53 @@
 from pathlib import Path
 from typing import List
+
 from llama_index.indices.composability.graph import ComposableGraph
-from llama_index.indices.list.base import GPTListIndex
+from llama_index.indices.list.base import ListIndex
 from llama_index.indices.loading import load_graph_from_storage
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
-
-from llama_index.readers.schema.base import Document
+from llama_index.indices.vector_store.base import VectorStoreIndex
+from llama_index.schema import Document
 from llama_index.storage.storage_context import StorageContext
 
 
 def test_load_graph_from_storage_simple(
     documents: List[Document],
     tmp_path: Path,
     mock_service_context: ServiceContext,
 ) -> None:
     # construct simple (i.e. in memory) storage context
     storage_context = StorageContext.from_defaults()
 
     # construct index
-    vector_index = GPTVectorStoreIndex.from_documents(
+    vector_index_1 = VectorStoreIndex.from_documents(
+        documents=documents,
+        storage_context=storage_context,
+        service_context=mock_service_context,
+    )
+
+    # construct second index, testing vector store overlap
+    vector_index_2 = VectorStoreIndex.from_documents(
         documents=documents,
         storage_context=storage_context,
         service_context=mock_service_context,
     )
 
     # construct index
-    list_index = GPTListIndex.from_documents(
+    list_index = ListIndex.from_documents(
         documents=documents,
         storage_context=storage_context,
         service_context=mock_service_context,
     )
 
     # construct graph
     graph = ComposableGraph.from_indices(
-        GPTListIndex,
-        children_indices=[vector_index, list_index],
-        index_summaries=["vector index", "list index"],
+        ListIndex,
+        children_indices=[vector_index_1, vector_index_2, list_index],
+        index_summaries=["vector index 1", "vector index 2", "list index"],
         storage_context=storage_context,
         service_context=mock_service_context,
     )
 
     query_engine = graph.as_query_engine()
     response = query_engine.query("test query")
```

### Comparing `llama_index-0.6.9/tests/indices/test_node_utils.py` & `llama_index-0.7.0/tests/indices/test_node_utils.py`

 * *Files 15% similar despite different names*

```diff
@@ -2,15 +2,16 @@
 
 from typing import List
 
 import pytest
 
 from llama_index.langchain_helpers.text_splitter import TokenTextSplitter
 from llama_index.node_parser.node_utils import get_nodes_from_document
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
+from llama_index.schema import MetadataMode
 
 
 @pytest.fixture
 def text_splitter() -> TokenTextSplitter:
     """Get text splitter."""
     return TokenTextSplitter(chunk_size=20, chunk_overlap=0)
 
@@ -22,46 +23,51 @@
     doc_text = (
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
     return [
-        Document(doc_text, doc_id="test_doc_id", extra_info={"test_key": "test_val"})
+        Document(text=doc_text, id_="test_doc_id", metadata={"test_key": "test_val"})
     ]
 
 
 def test_get_nodes_from_document(
     documents: List[Document], text_splitter: TokenTextSplitter
 ) -> None:
     """Test get nodes from document have desired chunk size."""
     nodes = get_nodes_from_document(
         documents[0],
         text_splitter,
-        include_extra_info=False,
+        include_metadata=False,
     )
     assert len(nodes) == 2
     actual_chunk_sizes = [
-        len(text_splitter.tokenizer(node.get_text())) for node in nodes
+        len(text_splitter.tokenizer(node.get_content())) for node in nodes
     ]
     assert all(
         chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes
     )
 
 
-def test_get_nodes_from_document_with_extra_info(
+def test_get_nodes_from_document_with_metadata(
     documents: List[Document], text_splitter: TokenTextSplitter
 ) -> None:
-    """Test get nodes from document with extra info have desired chunk size."""
+    """Test get nodes from document with metadata have desired chunk size."""
     nodes = get_nodes_from_document(
         documents[0],
         text_splitter,
-        include_extra_info=True,
+        include_metadata=True,
     )
     assert len(nodes) == 3
     actual_chunk_sizes = [
-        len(text_splitter.tokenizer(node.get_text())) for node in nodes
+        len(text_splitter.tokenizer(node.get_content())) for node in nodes
     ]
     assert all(
         chunk_size <= text_splitter._chunk_size for chunk_size in actual_chunk_sizes
     )
-    assert all(["test_key: test_val" in n.get_text() for n in nodes])
+    assert all(
+        [
+            "test_key: test_val" in n.get_content(metadata_mode=MetadataMode.ALL)
+            for n in nodes
+        ]
+    )
```

### Comparing `llama_index-0.6.9/tests/indices/test_prompt_helper.py` & `llama_index-0.7.0/tests/indices/test_prompt_helper.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,197 +1,184 @@
 """Test PromptHelper."""
-from typing import List, cast
+from typing import cast
 
-from langchain import PromptTemplate as LangchainPrompt
+from llama_index.bridge.langchain import PromptTemplate as LangchainPrompt
 
-from llama_index.data_structs.node import Node
 from llama_index.indices.prompt_helper import PromptHelper
+from llama_index.indices.tree.utils import get_numbered_text_from_nodes
+from llama_index.prompts.utils import get_biggest_prompt, get_empty_prompt_txt
 from llama_index.prompts.base import Prompt
+from llama_index.schema import TextNode
 from tests.mock_utils.mock_utils import mock_tokenizer
 
 
-class TestPrompt(Prompt):
-    """Test prompt class."""
-
-    input_variables: List[str] = ["text"]
-
-
 def test_get_chunk_size() -> None:
     """Test get chunk size given prompt."""
     # test with 1 chunk
-    empty_prompt_text = "This is the prompt"
+    prompt = Prompt("This is the prompt")
     prompt_helper = PromptHelper(
-        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer
-    )
-    chunk_size = prompt_helper.get_chunk_size_given_prompt(
-        empty_prompt_text, 1, padding=0
+        context_window=11, num_output=1, chunk_overlap_ratio=0, tokenizer=mock_tokenizer
     )
+    chunk_size = prompt_helper._get_available_chunk_size(prompt, 1, padding=0)
     assert chunk_size == 6
 
     # test having 2 chunks
     prompt_helper = PromptHelper(
-        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer
-    )
-    chunk_size = prompt_helper.get_chunk_size_given_prompt(
-        empty_prompt_text, 2, padding=0
+        context_window=11, num_output=1, chunk_overlap_ratio=0, tokenizer=mock_tokenizer
     )
+    chunk_size = prompt_helper._get_available_chunk_size(prompt, 2, padding=0)
     assert chunk_size == 3
 
     # test with 2 chunks, and with chunk_size_limit
     prompt_helper = PromptHelper(
-        max_input_size=11,
+        context_window=11,
         num_output=1,
-        max_chunk_overlap=0,
+        chunk_overlap_ratio=0,
         tokenizer=mock_tokenizer,
         chunk_size_limit=2,
     )
-    chunk_size = prompt_helper.get_chunk_size_given_prompt(
-        empty_prompt_text, 2, padding=0
-    )
+    chunk_size = prompt_helper._get_available_chunk_size(prompt, 2, padding=0)
     assert chunk_size == 2
 
     # test padding
     prompt_helper = PromptHelper(
-        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer
-    )
-    chunk_size = prompt_helper.get_chunk_size_given_prompt(
-        empty_prompt_text, 2, padding=1
+        context_window=11, num_output=1, chunk_overlap_ratio=0, tokenizer=mock_tokenizer
     )
+    chunk_size = prompt_helper._get_available_chunk_size(prompt, 2, padding=1)
     assert chunk_size == 2
 
 
 def test_get_text_splitter() -> None:
     """Test get text splitter."""
     test_prompt_text = "This is the prompt{text}"
-    test_prompt = TestPrompt(test_prompt_text)
+    test_prompt = Prompt(test_prompt_text)
     prompt_helper = PromptHelper(
-        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer
+        context_window=11, num_output=1, chunk_overlap_ratio=0, tokenizer=mock_tokenizer
     )
     text_splitter = prompt_helper.get_text_splitter_given_prompt(
         test_prompt, 2, padding=1
     )
     assert text_splitter._chunk_size == 2
     test_text = "Hello world foo Hello world bar"
     text_chunks = text_splitter.split_text(test_text)
     assert text_chunks == ["Hello world", "foo Hello", "world bar"]
     truncated_text = text_splitter.truncate_text(test_text)
     assert truncated_text == "Hello world"
 
     # test with chunk_size_limit
     prompt_helper = PromptHelper(
-        max_input_size=11,
+        context_window=11,
         num_output=1,
-        max_chunk_overlap=0,
+        chunk_overlap_ratio=0,
         tokenizer=mock_tokenizer,
         chunk_size_limit=1,
     )
     text_splitter = prompt_helper.get_text_splitter_given_prompt(
         test_prompt, 2, padding=1
     )
     text_chunks = text_splitter.split_text(test_text)
     assert text_chunks == ["Hello", "world", "foo", "Hello", "world", "bar"]
 
 
 def test_get_text_splitter_partial() -> None:
     """Test get text splitter with a partially formatted prompt."""
 
-    class TestPromptFoo(Prompt):
-        """Test prompt class."""
-
-        input_variables: List[str] = ["foo", "text"]
-
     # test without partially formatting
     test_prompt_text = "This is the {foo} prompt{text}"
-    test_prompt = TestPromptFoo(test_prompt_text)
+    test_prompt = Prompt(test_prompt_text)
     prompt_helper = PromptHelper(
-        max_input_size=11, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer
+        context_window=11, num_output=1, chunk_overlap_ratio=0, tokenizer=mock_tokenizer
     )
     text_splitter = prompt_helper.get_text_splitter_given_prompt(
         test_prompt, 2, padding=1
     )
     test_text = "Hello world foo Hello world bar"
     text_chunks = text_splitter.split_text(test_text)
     assert text_chunks == ["Hello world", "foo Hello", "world bar"]
     truncated_text = text_splitter.truncate_text(test_text)
     assert truncated_text == "Hello world"
 
     # test with partially formatting
-    test_prompt = TestPromptFoo(test_prompt_text)
+    test_prompt = Prompt(test_prompt_text)
     test_prompt = test_prompt.partial_format(foo="bar")
     prompt_helper = PromptHelper(
-        max_input_size=12, num_output=1, max_chunk_overlap=0, tokenizer=mock_tokenizer
+        context_window=12, num_output=1, chunk_overlap_ratio=0, tokenizer=mock_tokenizer
     )
-    assert prompt_helper._get_empty_prompt_txt(test_prompt) == "This is the bar prompt"
+    assert get_empty_prompt_txt(test_prompt) == "This is the bar prompt"
     text_splitter = prompt_helper.get_text_splitter_given_prompt(
         test_prompt, 2, padding=1
     )
     test_text = "Hello world foo Hello world bar"
     text_chunks = text_splitter.split_text(test_text)
     assert text_chunks == ["Hello world", "foo Hello", "world bar"]
     truncated_text = text_splitter.truncate_text(test_text)
     assert truncated_text == "Hello world"
 
 
-def test_get_text_from_nodes() -> None:
-    """Test get_text_from_nodes."""
+def test_truncate() -> None:
+    """Test truncate."""
     # test prompt uses up one token
     test_prompt_txt = "test{text}"
-    test_prompt = TestPrompt(test_prompt_txt)
-    # set max_input_size=11
-    # For each text chunk, there's 4 tokens for text + 1 for the padding
+    test_prompt = Prompt(test_prompt_txt)
+    # set context_window=19
+    # For each text chunk, there's 4 tokens for text + 5 for the padding
     prompt_helper = PromptHelper(
-        max_input_size=11, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer
+        context_window=19, num_output=0, chunk_overlap_ratio=0, tokenizer=mock_tokenizer
     )
-    node1 = Node(text="This is a test foo bar")
-    node2 = Node(text="Hello world bar foo")
+    text_chunks = ["This is a test foo bar", "Hello world bar foo"]
 
-    response = prompt_helper.get_text_from_nodes([node1, node2], prompt=test_prompt)
-    assert str(response) == ("This is a test\n" "Hello world bar foo")
+    truncated_chunks = prompt_helper.truncate(
+        prompt=test_prompt, text_chunks=text_chunks
+    )
+    assert truncated_chunks == [
+        "This is a test",
+        "Hello world bar foo",
+    ]
 
 
 def test_get_numbered_text_from_nodes() -> None:
     """Test get_text_from_nodes."""
     # test prompt uses up one token
     test_prompt_txt = "test{text}"
-    test_prompt = TestPrompt(test_prompt_txt)
-    # set max_input_size=17
+    test_prompt = Prompt(test_prompt_txt)
+    # set context_window=17
     # For each text chunk, there's 3 for text, 5 for padding (including number)
     prompt_helper = PromptHelper(
-        max_input_size=17, num_output=0, max_chunk_overlap=0, tokenizer=mock_tokenizer
+        context_window=17, num_output=0, chunk_overlap_ratio=0, tokenizer=mock_tokenizer
     )
-    node1 = Node(text="This is a test foo bar")
-    node2 = Node(text="Hello world bar foo")
+    node1 = TextNode(text="This is a test foo bar")
+    node2 = TextNode(text="Hello world bar foo")
 
-    response = prompt_helper.get_numbered_text_from_nodes(
-        [node1, node2], prompt=test_prompt
+    text_splitter = prompt_helper.get_text_splitter_given_prompt(
+        prompt=test_prompt,
+        num_chunks=2,
     )
+    response = get_numbered_text_from_nodes([node1, node2], text_splitter=text_splitter)
     assert str(response) == ("(1) This is a\n\n(2) Hello world bar")
 
 
-def test_compact_text() -> None:
-    """Test compact text."""
+def test_repack() -> None:
+    """Test repack."""
     test_prompt_text = "This is the prompt{text}"
-    test_prompt = TestPrompt(test_prompt_text)
+    test_prompt = Prompt(test_prompt_text)
     prompt_helper = PromptHelper(
-        max_input_size=9,
+        context_window=13,
         num_output=1,
-        max_chunk_overlap=0,
+        chunk_overlap_ratio=0,
         tokenizer=mock_tokenizer,
         separator="\n\n",
     )
     text_chunks = ["Hello", "world", "foo", "Hello", "world", "bar"]
-    compacted_chunks = prompt_helper.compact_text_chunks(test_prompt, text_chunks)
+    compacted_chunks = prompt_helper.repack(test_prompt, text_chunks)
     assert compacted_chunks == ["Hello\n\nworld\n\nfoo", "Hello\n\nworld\n\nbar"]
 
 
 def test_get_biggest_prompt() -> None:
     """Test get_biggest_prompt from PromptHelper."""
-    # NOTE: inputs don't matter
-    prompt_helper = PromptHelper(max_input_size=1, num_output=1, max_chunk_overlap=0)
-    prompt1 = TestPrompt("This is the prompt{text}")
-    prompt2 = TestPrompt("This is the longer prompt{text}")
-    prompt3 = TestPrompt("This is the {text}")
-    biggest_prompt = prompt_helper.get_biggest_prompt([prompt1, prompt2, prompt3])
+    prompt1 = Prompt("This is the prompt{text}")
+    prompt2 = Prompt("This is the longer prompt{text}")
+    prompt3 = Prompt("This is the {text}")
+    biggest_prompt = get_biggest_prompt([prompt1, prompt2, prompt3])
 
     lc_biggest_template = cast(LangchainPrompt, biggest_prompt.prompt).template
     prompt2_template = cast(LangchainPrompt, prompt2.prompt).template
     assert lc_biggest_template == prompt2_template
```

### Comparing `llama_index-0.6.9/tests/indices/tree/conftest.py` & `llama_index-0.7.0/tests/indices/tree/conftest.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from typing import Dict, List, Tuple
 import pytest
 
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 from tests.mock_utils.mock_prompts import (
     MOCK_INSERT_PROMPT,
     MOCK_QUERY_PROMPT,
     MOCK_REFINE_PROMPT,
     MOCK_SUMMARY_PROMPT,
     MOCK_TEXT_QA_PROMPT,
 )
@@ -17,15 +17,15 @@
     # NOTE: one document for now
     doc_text = (
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
-    return [Document(doc_text)]
+    return [Document(text=doc_text)]
 
 
 @pytest.fixture
 def struct_kwargs() -> Tuple[Dict, Dict]:
     """Index kwargs."""
     index_kwargs = {
         "summary_template": MOCK_SUMMARY_PROMPT,
```

### Comparing `llama_index-0.6.9/tests/indices/tree/test_index.py` & `llama_index-0.7.0/tests/indices/tree/test_index.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 """Test tree index."""
 
 from typing import Any, Dict, List, Optional
 from unittest.mock import patch
 
 from llama_index.data_structs.data_structs import IndexGraph
-from llama_index.data_structs.node import Node
 from llama_index.indices.service_context import ServiceContext
 from llama_index.storage.docstore import BaseDocumentStore
-from llama_index.indices.tree.base import GPTTreeIndex
-from llama_index.readers.schema.base import Document
+from llama_index.indices.tree.base import TreeIndex
+from llama_index.schema import Document
+from llama_index.schema import BaseNode
 
 
 def _get_left_or_right_node(
     docstore: BaseDocumentStore,
     index_graph: IndexGraph,
-    node: Optional[Node],
+    node: Optional[BaseNode],
     left: bool = True,
-) -> Node:
+) -> BaseNode:
     """Get 'left' or 'right' node."""
     children_dict = index_graph.get_children(node)
     indices = list(children_dict.keys())
     index = min(indices) if left else max(indices)
     node_id = children_dict[index]
     return docstore.get_node(node_id)
 
@@ -28,27 +28,32 @@
 def test_build_tree(
     documents: List[Document],
     mock_service_context: ServiceContext,
     struct_kwargs: Dict,
 ) -> None:
     """Test build tree."""
     index_kwargs, _ = struct_kwargs
-    tree = GPTTreeIndex.from_documents(
+    tree = TreeIndex.from_documents(
         documents, service_context=mock_service_context, **index_kwargs
     )
     assert len(tree.index_struct.all_nodes) == 6
     # check contents of nodes
 
     nodes = tree.docstore.get_nodes(list(tree.index_struct.all_nodes.values()))
-    assert nodes[0].text == "Hello world."
-    assert nodes[1].text == "This is a test."
-    assert nodes[2].text == "This is another test."
-    assert nodes[3].text == "This is a test v2."
-    assert nodes[4].text == ("Hello world.\nThis is a test.")
-    assert nodes[5].text == ("This is another test.\nThis is a test v2.")
+    assert nodes[0].get_content() == "Hello world."
+    assert nodes[1].get_content() == "This is a test."
+    assert nodes[2].get_content() == "This is another test."
+    assert nodes[3].get_content() == "This is a test v2."
+    assert nodes[4].get_content() == ("Hello world.\nThis is a test.")
+    assert nodes[5].get_content() == ("This is another test.\nThis is a test v2.")
+
+    # test ref doc info
+    all_ref_doc_info = tree.ref_doc_info
+    for idx, ref_doc_id in enumerate(all_ref_doc_info.keys()):
+        assert documents[idx].doc_id == ref_doc_id
 
 
 def test_build_tree_with_embed(
     documents: List[Document],
     mock_service_context: ServiceContext,
     struct_kwargs: Dict,
 ) -> None:
@@ -56,30 +61,30 @@
     index_kwargs, _ = struct_kwargs
     doc_text = (
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
-    document = Document(doc_text, embedding=[0.1, 0.2, 0.3])
-    tree = GPTTreeIndex.from_documents(
+    document = Document(text=doc_text, embedding=[0.1, 0.2, 0.3])
+    tree = TreeIndex.from_documents(
         [document], service_context=mock_service_context, **index_kwargs
     )
     assert len(tree.index_struct.all_nodes) == 6
     # check contents of nodes
     all_nodes = tree.docstore.get_node_dict(tree.index_struct.all_nodes)
-    assert all_nodes[0].text == "Hello world."
-    assert all_nodes[1].text == "This is a test."
-    assert all_nodes[2].text == "This is another test."
-    assert all_nodes[3].text == "This is a test v2."
+    assert all_nodes[0].get_content() == "Hello world."
+    assert all_nodes[1].get_content() == "This is a test."
+    assert all_nodes[2].get_content() == "This is another test."
+    assert all_nodes[3].get_content() == "This is a test v2."
     # make sure all leaf nodes have embeddings
     for i in range(4):
         assert all_nodes[i].embedding == [0.1, 0.2, 0.3]
-    assert all_nodes[4].text == ("Hello world.\nThis is a test.")
-    assert all_nodes[5].text == ("This is another test.\nThis is a test v2.")
+    assert all_nodes[4].get_content() == ("Hello world.\nThis is a test.")
+    assert all_nodes[5].get_content() == ("This is another test.\nThis is a test v2.")
 
 
 OUTPUTS = [
     ("Hello world.\nThis is a test.", ""),
     ("This is another test.\nThis is a test v2.", ""),
 ]
 
@@ -89,122 +94,122 @@
     _mock_run_async_tasks: Any,
     documents: List[Document],
     mock_service_context: ServiceContext,
     struct_kwargs: Dict,
 ) -> None:
     """Test build tree with use_async."""
     index_kwargs, _ = struct_kwargs
-    tree = GPTTreeIndex.from_documents(
+    tree = TreeIndex.from_documents(
         documents, use_async=True, service_context=mock_service_context, **index_kwargs
     )
     assert len(tree.index_struct.all_nodes) == 6
     # check contents of nodes
     nodes = tree.docstore.get_nodes(list(tree.index_struct.all_nodes.values()))
-    assert nodes[0].text == "Hello world."
-    assert nodes[1].text == "This is a test."
-    assert nodes[2].text == "This is another test."
-    assert nodes[3].text == "This is a test v2."
-    assert nodes[4].text == ("Hello world.\nThis is a test.")
-    assert nodes[5].text == ("This is another test.\nThis is a test v2.")
+    assert nodes[0].get_content() == "Hello world."
+    assert nodes[1].get_content() == "This is a test."
+    assert nodes[2].get_content() == "This is another test."
+    assert nodes[3].get_content() == "This is a test v2."
+    assert nodes[4].get_content() == ("Hello world.\nThis is a test.")
+    assert nodes[5].get_content() == ("This is another test.\nThis is a test v2.")
 
 
 def test_build_tree_multiple(
     mock_service_context: ServiceContext,
     struct_kwargs: Dict,
 ) -> None:
     """Test build tree."""
     new_docs = [
-        Document("Hello world.\nThis is a test."),
-        Document("This is another test.\nThis is a test v2."),
+        Document(text="Hello world.\nThis is a test."),
+        Document(text="This is another test.\nThis is a test v2."),
     ]
     index_kwargs, _ = struct_kwargs
-    tree = GPTTreeIndex.from_documents(
+    tree = TreeIndex.from_documents(
         new_docs, service_context=mock_service_context, **index_kwargs
     )
     assert len(tree.index_struct.all_nodes) == 6
     # check contents of nodes
     nodes = tree.docstore.get_nodes(list(tree.index_struct.all_nodes.values()))
-    assert nodes[0].text == "Hello world."
-    assert nodes[1].text == "This is a test."
-    assert nodes[2].text == "This is another test."
-    assert nodes[3].text == "This is a test v2."
+    assert nodes[0].get_content() == "Hello world."
+    assert nodes[1].get_content() == "This is a test."
+    assert nodes[2].get_content() == "This is another test."
+    assert nodes[3].get_content() == "This is a test v2."
 
 
 def test_insert(
     documents: List[Document],
     mock_service_context: ServiceContext,
     struct_kwargs: Dict,
 ) -> None:
     """Test insert."""
     index_kwargs, _ = struct_kwargs
-    tree = GPTTreeIndex.from_documents(
+    tree = TreeIndex.from_documents(
         documents, service_context=mock_service_context, **index_kwargs
     )
 
     # test insert
-    new_doc = Document("This is a new doc.", doc_id="new_doc")
+    new_doc = Document(text="This is a new doc.", id_="new_doc")
     tree.insert(new_doc)
     # Before:
     # Left root node: "Hello world.\nThis is a test."
     # "Hello world.", "This is a test" are two children of the left root node
     # After:
     # "Hello world.\nThis is a test\n.\nThis is a new doc." is the left root node
     # "Hello world", "This is a test\n.This is a new doc." are the children
     # of the left root node.
     # "This is a test", "This is a new doc." are the children of
     # "This is a test\n.This is a new doc."
     left_root = _get_left_or_right_node(tree.docstore, tree.index_struct, None)
-    assert left_root.text == "Hello world.\nThis is a test."
+    assert left_root.get_content() == "Hello world.\nThis is a test."
     left_root2 = _get_left_or_right_node(tree.docstore, tree.index_struct, left_root)
     right_root2 = _get_left_or_right_node(
         tree.docstore, tree.index_struct, left_root, left=False
     )
-    assert left_root2.text == "Hello world."
-    assert right_root2.text == "This is a test.\nThis is a new doc."
+    assert left_root2.get_content() == "Hello world."
+    assert right_root2.get_content() == "This is a test.\nThis is a new doc."
     left_root3 = _get_left_or_right_node(tree.docstore, tree.index_struct, right_root2)
     right_root3 = _get_left_or_right_node(
         tree.docstore, tree.index_struct, right_root2, left=False
     )
-    assert left_root3.text == "This is a test."
-    assert right_root3.text == "This is a new doc."
+    assert left_root3.get_content() == "This is a test."
+    assert right_root3.get_content() == "This is a new doc."
     assert right_root3.ref_doc_id == "new_doc"
 
     # test insert from empty (no_id)
-    tree = GPTTreeIndex.from_documents(
+    tree = TreeIndex.from_documents(
         [], service_context=mock_service_context, **index_kwargs
     )
-    new_doc = Document("This is a new doc.")
+    new_doc = Document(text="This is a new doc.")
     tree.insert(new_doc)
     nodes = tree.docstore.get_nodes(list(tree.index_struct.all_nodes.values()))
     assert len(nodes) == 1
-    assert nodes[0].text == "This is a new doc."
+    assert nodes[0].get_content() == "This is a new doc."
 
     # test insert from empty (with_id)
-    tree = GPTTreeIndex.from_documents(
+    tree = TreeIndex.from_documents(
         [], service_context=mock_service_context, **index_kwargs
     )
-    new_doc = Document("This is a new doc.", doc_id="new_doc_test")
+    new_doc = Document(text="This is a new doc.", id_="new_doc_test")
     tree.insert(new_doc)
     assert len(tree.index_struct.all_nodes) == 1
     nodes = tree.docstore.get_nodes(list(tree.index_struct.all_nodes.values()))
-    assert nodes[0].text == "This is a new doc."
+    assert nodes[0].get_content() == "This is a new doc."
     assert nodes[0].ref_doc_id == "new_doc_test"
 
 
 def test_twice_insert_empty(
     mock_service_context: ServiceContext,
 ) -> None:
     """# test twice insert from empty (with_id)"""
-    tree = GPTTreeIndex.from_documents([], service_context=mock_service_context)
+    tree = TreeIndex.from_documents([], service_context=mock_service_context)
 
     # test first insert
-    new_doc = Document("This is a new doc.", doc_id="new_doc")
+    new_doc = Document(text="This is a new doc.", id_="new_doc")
     tree.insert(new_doc)
     # test second insert
-    new_doc_second = Document("This is a new doc2.", doc_id="new_doc_2")
+    new_doc_second = Document(text="This is a new doc2.", id_="new_doc_2")
     tree.insert(new_doc_second)
     assert len(tree.index_struct.all_nodes) == 2
 
 
 def _mock_tokenizer(text: str) -> int:
     """Mock tokenizer that splits by spaces."""
     return len(text.split(" "))
```

### Comparing `llama_index-0.6.9/tests/indices/tree/test_retrievers.py` & `llama_index-0.7.0/tests/indices/tree/test_retrievers.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 from typing import Dict, List
 
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.tree.base import GPTTreeIndex
-from llama_index.readers.schema.base import Document
+from llama_index.indices.tree.base import TreeIndex
+from llama_index.schema import Document
 
 
 def test_query(
     documents: List[Document],
     mock_service_context: ServiceContext,
     struct_kwargs: Dict,
 ) -> None:
     """Test query."""
     index_kwargs, query_kwargs = struct_kwargs
-    tree = GPTTreeIndex.from_documents(
+    tree = TreeIndex.from_documents(
         documents, service_context=mock_service_context, **index_kwargs
     )
 
     # test default query
     query_str = "What is?"
     retriever = tree.as_retriever()
     nodes = retriever.retrieve(query_str)
@@ -29,15 +29,15 @@
     struct_kwargs: Dict,
 ) -> None:
     """Test summarize query."""
     # create tree index without building tree
     index_kwargs, orig_query_kwargs = struct_kwargs
     index_kwargs = index_kwargs.copy()
     index_kwargs.update({"build_tree": False})
-    tree = GPTTreeIndex.from_documents(
+    tree = TreeIndex.from_documents(
         documents, service_context=mock_service_context, **index_kwargs
     )
 
     # test retrieve all leaf
     query_str = "What is?"
     retriever = tree.as_retriever(retriever_mode="all_leaf")
     nodes = retriever.retrieve(query_str)
```

### Comparing `llama_index-0.6.9/tests/indices/vector_store/auto_retriever/test_output_parser.py` & `llama_index-0.7.0/tests/indices/vector_store/auto_retriever/test_output_parser.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/indices/vector_store/conftest.py` & `llama_index-0.7.0/tests/indices/vector_store/conftest.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/indices/vector_store/mock_faiss.py` & `llama_index-0.7.0/tests/indices/vector_store/mock_faiss.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/indices/vector_store/mock_services.py` & `llama_index-0.7.0/tests/indices/vector_store/mock_services.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/indices/vector_store/test_faiss.py` & `llama_index-0.7.0/tests/indices/vector_store/test_faiss.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,82 +1,81 @@
 """Test vector store indexes."""
 
 import os
 from pathlib import Path
 from typing import List
 
 import pytest
-from llama_index.data_structs.node import Node
 
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
-
-from llama_index.readers.schema.base import Document
+from llama_index.indices.vector_store.base import VectorStoreIndex
+from llama_index.schema import TextNode
+from llama_index.schema import Document
 from llama_index.storage.storage_context import StorageContext
 from llama_index.vector_stores.faiss import FaissVectorStore
 from llama_index.vector_stores.types import NodeWithEmbedding, VectorStoreQuery
 
 
 @pytest.mark.skipif("CI" in os.environ, reason="no FAISS in CI")
 def test_build_faiss(
     documents: List[Document],
     faiss_storage_context: StorageContext,
     mock_service_context: ServiceContext,
 ) -> None:
-    """Test build GPTVectorStoreIndex with FaissVectoreStore."""
-    index = GPTVectorStoreIndex.from_documents(
+    """Test build VectorStoreIndex with FaissVectoreStore."""
+    index = VectorStoreIndex.from_documents(
         documents=documents,
         storage_context=faiss_storage_context,
         service_context=mock_service_context,
     )
     assert len(index.index_struct.nodes_dict) == 4
 
     node_ids = list(index.index_struct.nodes_dict.values())
     nodes = index.docstore.get_nodes(node_ids)
-    node_texts = [node.text for node in nodes]
+    node_texts = [node.get_content() for node in nodes]
     assert "Hello world." in node_texts
     assert "This is a test." in node_texts
     assert "This is another test." in node_texts
     assert "This is a test v2." in node_texts
 
 
 @pytest.mark.skipif("CI" in os.environ, reason="no FAISS in CI")
 def test_faiss_insert(
     documents: List[Document],
     faiss_storage_context: StorageContext,
     mock_service_context: ServiceContext,
 ) -> None:
-    """Test insert GPTVectorStoreIndex with FaissVectoreStore."""
-    index = GPTVectorStoreIndex.from_documents(
+    """Test insert VectorStoreIndex with FaissVectoreStore."""
+    index = VectorStoreIndex.from_documents(
         documents=documents,
         storage_context=faiss_storage_context,
         service_context=mock_service_context,
     )
 
     # insert into index
     index.insert(Document(text="This is a test v3."))
 
     # check contents of nodes
     node_ids = list(index.index_struct.nodes_dict.values())
     nodes = index.docstore.get_nodes(node_ids)
-    node_texts = [node.text for node in nodes]
+    node_texts = [node.get_content() for node in nodes]
     assert "This is a test v2." in node_texts
     assert "This is a test v3." in node_texts
 
 
 @pytest.mark.skipif("CI" in os.environ, reason="no FAISS in CI")
 def test_persist(tmp_path: Path) -> None:
     import faiss
 
     vector_store = FaissVectorStore(faiss_index=faiss.IndexFlatL2(5))
 
     vector_store.add(
         [
             NodeWithEmbedding(
-                node=Node("test text"),
+                node=TextNode(text="test text"),
                 embedding=[0, 0, 0, 1, 1],
             )
         ]
     )
 
     result = vector_store.query(VectorStoreQuery(query_embedding=[0, 0, 0, 1, 1]))
```

### Comparing `llama_index-0.6.9/tests/indices/vector_store/test_myscale.py` & `llama_index-0.7.0/tests/indices/vector_store/test_myscale.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 """Test MyScale indexes."""
 
 from typing import List, cast
 
 import pytest
 
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
+from llama_index.indices.vector_store.base import VectorStoreIndex
 from llama_index.storage.storage_context import StorageContext
 
 try:
     import clickhouse_connect
 except ImportError:
     clickhouse_connect = None  # type: ignore
 
-from llama_index.data_structs.node import Node
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
+from llama_index.schema import BaseNode
 from llama_index.vector_stores import MyScaleVectorStore
 from llama_index.vector_stores.types import VectorStoreQuery
 
 # local test only, update variable here for test
 MYSCALE_CLUSTER_URL = None
 MYSCALE_USERNAME = None
 MYSCALE_CLUSTER_PASSWORD = None
@@ -29,15 +29,15 @@
     # NOTE: one document for now
     doc_text = (
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
-    return [Document(doc_id="1", text=doc_text)]
+    return [Document(id_="1", text=doc_text)]
 
 
 @pytest.fixture
 def query() -> VectorStoreQuery:
     return VectorStoreQuery(query_str="What is?", doc_ids=["1"])
 
 
@@ -53,24 +53,22 @@
         host=MYSCALE_CLUSTER_URL,
         port=8443,
         username=MYSCALE_USERNAME,
         password=MYSCALE_CLUSTER_PASSWORD,
     )
     vector_store = MyScaleVectorStore(myscale_client=client)
     storage_context = StorageContext.from_defaults(vector_store=vector_store)
-    index = GPTVectorStoreIndex.from_documents(
-        documents, storage_context=storage_context
-    )
+    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
     query_engine = index.as_query_engine()
     response = query_engine.query("What is?")
     assert str(response).strip() == ("What is what?")
 
     with pytest.raises(NotImplementedError):
         for doc in documents:
-            index.delete(doc_id=cast(str, doc.doc_id))
+            index.delete_ref_doc(ref_doc_id=cast(str, doc.doc_id))
 
     cast(MyScaleVectorStore, index._vector_store).drop()
 
 
 @pytest.mark.skipif(
     clickhouse_connect is None
     or MYSCALE_CLUSTER_URL is None
@@ -83,17 +81,15 @@
         host=MYSCALE_CLUSTER_URL,
         port=8443,
         username=MYSCALE_USERNAME,
         password=MYSCALE_CLUSTER_PASSWORD,
     )
     vector_store = MyScaleVectorStore(myscale_client=client)
     storage_context = StorageContext.from_defaults(vector_store=vector_store)
-    index = GPTVectorStoreIndex.from_documents(
-        documents, storage_context=storage_context
-    )
+    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
     for doc in documents:
         index.insert(document=doc)
     query_engine = index.as_query_engine()
     response = query_engine.query("What is?")
     assert str(response).strip() == ("What is what?")
 
     cast(MyScaleVectorStore, index._vector_store).drop()
@@ -113,17 +109,15 @@
         host=MYSCALE_CLUSTER_URL,
         port=8443,
         username=MYSCALE_USERNAME,
         password=MYSCALE_CLUSTER_PASSWORD,
     )
     vector_store = MyScaleVectorStore(myscale_client=client)
     storage_context = StorageContext.from_defaults(vector_store=vector_store)
-    index = GPTVectorStoreIndex.from_documents(
-        documents, storage_context=storage_context
-    )
+    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)
     query.query_embedding = index.service_context.embed_model.get_query_embedding(
         cast(str, query.query_str)
     )
-    responseNodes = cast(List[Node], index._vector_store.query(query).nodes)
+    responseNodes = cast(List[BaseNode], index._vector_store.query(query).nodes)
     assert len(responseNodes) == 1
-    assert responseNodes[0].doc_id == "1"
+    assert responseNodes[0].id_ == "1"
     cast(MyScaleVectorStore, index._vector_store).drop()
```

### Comparing `llama_index-0.6.9/tests/indices/vector_store/test_pinecone.py` & `llama_index-0.7.0/tests/indices/vector_store/test_pinecone.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,62 +1,62 @@
 """Test pinecone indexes."""
 
 from typing import List
 
 import pytest
 
-from llama_index.data_structs.node import Node
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
-from llama_index.readers.schema.base import Document
+from llama_index.indices.vector_store.base import VectorStoreIndex
+from llama_index.schema import Document
+from llama_index.schema import TextNode
 from tests.indices.vector_store.utils import get_pinecone_storage_context
 from tests.mock_utils.mock_utils import mock_tokenizer
 
 
 @pytest.fixture
 def documents() -> List[Document]:
     """Get documents."""
     # NOTE: one document for now
     doc_text = (
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
-    return [Document(doc_text)]
+    return [Document(text=doc_text)]
 
 
 def test_build_pinecone(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
-    """Test build GPTVectorStoreIndex with PineconeVectorStore."""
+    """Test build VectorStoreIndex with PineconeVectorStore."""
     storage_context = get_pinecone_storage_context()
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents=documents,
         storage_context=storage_context,
         service_context=mock_service_context,
         tokenizer=mock_tokenizer,
     )
 
     retriever = index.as_retriever(similarity_top_k=1)
     nodes = retriever.retrieve("What is?")
     assert len(nodes) == 1
-    assert nodes[0].node.get_text() == "This is another test."
+    assert nodes[0].node.get_content() == "This is another test."
 
 
 def test_node_with_metadata(
     mock_service_context: ServiceContext,
 ) -> None:
     storage_context = get_pinecone_storage_context()
-    input_nodes = [Node(text="test node text", extra_info={"key": "value"})]
-    index = GPTVectorStoreIndex(
+    input_nodes = [TextNode(text="test node text", metadata={"key": "value"})]
+    index = VectorStoreIndex(
         input_nodes,
         storage_context=storage_context,
         service_context=mock_service_context,
     )
 
     retriever = index.as_retriever(similarity_top_k=1)
     nodes = retriever.retrieve("What is?")
     assert len(nodes) == 1
-    assert nodes[0].node.text == "test node text"
-    assert nodes[0].node.extra_info == {"key": "value"}
+    assert nodes[0].node.get_content() == "test node text"
+    assert nodes[0].node.metadata == {"key": "value"}
```

### Comparing `llama_index-0.6.9/tests/indices/vector_store/test_retrievers.py` & `llama_index-0.7.0/tests/indices/vector_store/test_retrievers.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,145 +1,144 @@
 from typing import List, cast
-from llama_index.data_structs.node import DocumentRelationship, Node
+
 from llama_index.indices.query.schema import QueryBundle
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
-from llama_index.readers.schema.base import Document
+from llama_index.indices.vector_store.base import VectorStoreIndex
+from llama_index.schema import Document
+from llama_index.schema import NodeRelationship, RelatedNodeInfo, TextNode
 from llama_index.storage.storage_context import StorageContext
 from llama_index.vector_stores.simple import SimpleVectorStore
 
 
 def test_faiss_query(
     documents: List[Document],
     faiss_storage_context: StorageContext,
     mock_service_context: ServiceContext,
 ) -> None:
     """Test embedding query."""
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents=documents,
         storage_context=faiss_storage_context,
         service_context=mock_service_context,
     )
 
     # test embedding query
     query_str = "What is?"
     retriever = index.as_retriever(similarity_top_k=1)
     nodes = retriever.retrieve(QueryBundle(query_str))
     assert len(nodes) == 1
-    assert nodes[0].node.text == "This is another test."
+    assert nodes[0].node.get_content() == "This is another test."
 
 
 def test_simple_query(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test embedding query."""
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents, service_context=mock_service_context
     )
 
     # test embedding query
     query_str = "What is?"
     retriever = index.as_retriever(similarity_top_k=1)
     nodes = retriever.retrieve(QueryBundle(query_str))
     assert len(nodes) == 1
-    assert nodes[0].node.text == "This is another test."
+    assert nodes[0].node.get_content() == "This is another test."
 
 
 def test_query_and_similarity_scores(
     mock_service_context: ServiceContext,
 ) -> None:
     """Test that sources nodes have similarity scores."""
     doc_text = (
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
-    document = Document(doc_text)
-    index = GPTVectorStoreIndex.from_documents(
+    document = Document(text=doc_text)
+    index = VectorStoreIndex.from_documents(
         [document], service_context=mock_service_context
     )
 
     # test embedding query
     query_str = "What is?"
     retriever = index.as_retriever()
     nodes = retriever.retrieve(QueryBundle(query_str))
     assert len(nodes) > 0
     assert nodes[0].score is not None
 
 
 def test_simple_check_ids(
     mock_service_context: ServiceContext,
 ) -> None:
-    """Test build GPTVectorStoreIndex."""
+    """Test build VectorStoreIndex."""
     ref_doc_id = "ref_doc_id_test"
-    source_rel = {DocumentRelationship.SOURCE: ref_doc_id}
+    source_rel = {NodeRelationship.SOURCE: RelatedNodeInfo(node_id=ref_doc_id)}
     all_nodes = [
-        Node("Hello world.", doc_id="node1", relationships=source_rel),
-        Node("This is a test.", doc_id="node2", relationships=source_rel),
-        Node("This is another test.", doc_id="node3", relationships=source_rel),
-        Node("This is a test v2.", doc_id="node4", relationships=source_rel),
+        TextNode(text="Hello world.", id_="node1", relationships=source_rel),
+        TextNode(text="This is a test.", id_="node2", relationships=source_rel),
+        TextNode(text="This is another test.", id_="node3", relationships=source_rel),
+        TextNode(text="This is a test v2.", id_="node4", relationships=source_rel),
     ]
-    index = GPTVectorStoreIndex(all_nodes, service_context=mock_service_context)
+    index = VectorStoreIndex(all_nodes, service_context=mock_service_context)
 
     # test query
     query_str = "What is?"
     retriever = index.as_retriever()
     nodes = retriever.retrieve(QueryBundle(query_str))
-    assert nodes[0].node.text == "This is another test."
+    assert nodes[0].node.get_content() == "This is another test."
     assert nodes[0].node.ref_doc_id == "ref_doc_id_test"
-    assert nodes[0].node.doc_id == "node3"
+    assert nodes[0].node.node_id == "node3"
     vector_store = cast(SimpleVectorStore, index._vector_store)
     assert "node3" in vector_store._data.embedding_dict
-    assert "node3" in vector_store._data.text_id_to_doc_id
+    assert "node3" in vector_store._data.text_id_to_ref_doc_id
 
 
 def test_faiss_check_ids(
     mock_service_context: ServiceContext,
     faiss_storage_context: StorageContext,
 ) -> None:
     """Test embedding query."""
 
     ref_doc_id = "ref_doc_id_test"
-    source_rel = {DocumentRelationship.SOURCE: ref_doc_id}
+    source_rel = {NodeRelationship.SOURCE: RelatedNodeInfo(node_id=ref_doc_id)}
     all_nodes = [
-        Node("Hello world.", doc_id="node1", relationships=source_rel),
-        Node("This is a test.", doc_id="node2", relationships=source_rel),
-        Node("This is another test.", doc_id="node3", relationships=source_rel),
-        Node("This is a test v2.", doc_id="node4", relationships=source_rel),
+        TextNode(text="Hello world.", id_="node1", relationships=source_rel),
+        TextNode(text="This is a test.", id_="node2", relationships=source_rel),
+        TextNode(text="This is another test.", id_="node3", relationships=source_rel),
+        TextNode(text="This is a test v2.", id_="node4", relationships=source_rel),
     ]
 
-    index = GPTVectorStoreIndex(
+    index = VectorStoreIndex(
         all_nodes,
         storage_context=faiss_storage_context,
         service_context=mock_service_context,
     )
 
     # test query
     query_str = "What is?"
     retriever = index.as_retriever()
     nodes = retriever.retrieve(QueryBundle(query_str))
-    assert nodes[0].node.text == "This is another test."
+    assert nodes[0].node.get_content() == "This is another test."
     assert nodes[0].node.ref_doc_id == "ref_doc_id_test"
-    assert nodes[0].node.doc_id == "node3"
+    assert nodes[0].node.node_id == "node3"
 
 
-def test_query_and_count_tokens(mock_service_context: ServiceContext) -> None:
+def test_query(mock_service_context: ServiceContext) -> None:
     """Test embedding query."""
     doc_text = (
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
-    document = Document(doc_text)
-    index = GPTVectorStoreIndex.from_documents(
+    document = Document(text=doc_text)
+    index = VectorStoreIndex.from_documents(
         [document], service_context=mock_service_context
     )
-    assert index.service_context.embed_model.total_tokens_used == 20
 
     # test embedding query
     query_str = "What is?"
     retriever = index.as_retriever()
     _ = retriever.retrieve(QueryBundle(query_str))
-    assert index.service_context.embed_model.last_token_usage == 3
```

### Comparing `llama_index-0.6.9/tests/indices/vector_store/test_simple.py` & `llama_index-0.7.0/tests/indices/vector_store/test_simple.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,57 +1,62 @@
 """Test vector store indexes."""
 
 from typing import Any, List, cast
 from llama_index.indices.loading import load_index_from_storage
 
 
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
+from llama_index.indices.vector_store.base import VectorStoreIndex
 
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 from llama_index.storage.storage_context import StorageContext
 from llama_index.vector_stores.simple import SimpleVectorStore
 
 
 def test_build_simple(
     mock_service_context: ServiceContext,
     documents: List[Document],
 ) -> None:
-    """Test build GPTVectorStoreIndex."""
+    """Test build VectorStoreIndex."""
 
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents=documents, service_context=mock_service_context
     )
-    assert isinstance(index, GPTVectorStoreIndex)
+    assert isinstance(index, VectorStoreIndex)
     assert len(index.index_struct.nodes_dict) == 4
     # check contents of nodes
     actual_node_tups = [
         ("Hello world.", [1, 0, 0, 0, 0]),
         ("This is a test.", [0, 1, 0, 0, 0]),
         ("This is another test.", [0, 0, 1, 0, 0]),
         ("This is a test v2.", [0, 0, 0, 1, 0]),
     ]
     for text_id in index.index_struct.nodes_dict.keys():
         node_id = index.index_struct.nodes_dict[text_id]
         node = index.docstore.get_node(node_id)
         # NOTE: this test breaks abstraction
         assert isinstance(index._vector_store, SimpleVectorStore)
         embedding = index._vector_store.get(text_id)
-        assert (node.text, embedding) in actual_node_tups
+        assert (node.get_content(), embedding) in actual_node_tups
+
+    # test ref doc info
+    all_ref_doc_info = index.ref_doc_info
+    for idx, ref_doc_id in enumerate(all_ref_doc_info.keys()):
+        assert documents[idx].node_id == ref_doc_id
 
 
 def test_simple_insert(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
-    """Test insert GPTVectorStoreIndex."""
-    index = GPTVectorStoreIndex.from_documents(
+    """Test insert VectorStoreIndex."""
+    index = VectorStoreIndex.from_documents(
         documents=documents, service_context=mock_service_context
     )
-    assert isinstance(index, GPTVectorStoreIndex)
+    assert isinstance(index, VectorStoreIndex)
     # insert into index
     index.insert(Document(text="This is a test v3."))
 
     # check contenst of nodes
     actual_node_tups = [
         ("Hello world.", [1, 0, 0, 0, 0]),
         ("This is a test.", [0, 1, 0, 0, 0]),
@@ -61,109 +66,133 @@
     ]
     for text_id in index.index_struct.nodes_dict.keys():
         node_id = index.index_struct.nodes_dict[text_id]
         node = index.docstore.get_node(node_id)
         # NOTE: this test breaks abstraction
         assert isinstance(index._vector_store, SimpleVectorStore)
         embedding = index._vector_store.get(text_id)
-        assert (node.text, embedding) in actual_node_tups
+        assert (node.get_content(), embedding) in actual_node_tups
 
 
 def test_simple_delete(
     mock_service_context: ServiceContext,
 ) -> None:
-    """Test delete GPTVectorStoreIndex."""
+    """Test delete VectorStoreIndex."""
     new_documents = [
-        Document("Hello world.", doc_id="test_id_0"),
-        Document("This is a test.", doc_id="test_id_1"),
-        Document("This is another test.", doc_id="test_id_2"),
-        Document("This is a test v2.", doc_id="test_id_3"),
+        Document(text="Hello world.", id_="test_id_0"),
+        Document(text="This is a test.", id_="test_id_1"),
+        Document(text="This is another test.", id_="test_id_2"),
+        Document(text="This is a test v2.", id_="test_id_3"),
     ]
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents=new_documents, service_context=mock_service_context
     )
-    assert isinstance(index, GPTVectorStoreIndex)
+    assert isinstance(index, VectorStoreIndex)
 
     # test delete
-    index.delete("test_id_0")
+    index.delete_ref_doc("test_id_0")
     assert len(index.index_struct.nodes_dict) == 3
-    assert len(index.index_struct.doc_id_dict) == 3
     actual_node_tups = [
         ("This is a test.", [0, 1, 0, 0, 0], "test_id_1"),
         ("This is another test.", [0, 0, 1, 0, 0], "test_id_2"),
         ("This is a test v2.", [0, 0, 0, 1, 0], "test_id_3"),
     ]
     for text_id in index.index_struct.nodes_dict.keys():
         node_id = index.index_struct.nodes_dict[text_id]
         node = index.docstore.get_node(node_id)
         # NOTE: this test breaks abstraction
         assert isinstance(index._vector_store, SimpleVectorStore)
         embedding = index._vector_store.get(text_id)
-        assert (node.text, embedding, node.ref_doc_id) in actual_node_tups
+        assert (node.get_content(), embedding, node.ref_doc_id) in actual_node_tups
 
     # test insert
-    index.insert(Document("Hello world backup.", doc_id="test_id_0"))
+    index.insert(Document(text="Hello world backup.", id_="test_id_0"))
     assert len(index.index_struct.nodes_dict) == 4
     actual_node_tups = [
         ("Hello world backup.", [1, 0, 0, 0, 0], "test_id_0"),
         ("This is a test.", [0, 1, 0, 0, 0], "test_id_1"),
         ("This is another test.", [0, 0, 1, 0, 0], "test_id_2"),
         ("This is a test v2.", [0, 0, 0, 1, 0], "test_id_3"),
     ]
     for text_id in index.index_struct.nodes_dict.keys():
         node_id = index.index_struct.nodes_dict[text_id]
         node = index.docstore.get_node(node_id)
         # NOTE: this test breaks abstraction
         assert isinstance(index._vector_store, SimpleVectorStore)
         embedding = index._vector_store.get(text_id)
-        assert (node.text, embedding, node.ref_doc_id) in actual_node_tups
+        assert (node.get_content(), embedding, node.ref_doc_id) in actual_node_tups
+
+
+def test_simple_delete_ref_node_from_docstore(
+    mock_service_context: ServiceContext,
+) -> None:
+    """Test delete VectorStoreIndex."""
+    new_documents = [
+        Document(text="This is a test.", id_="test_id_1"),
+        Document(text="This is another test.", id_="test_id_2"),
+    ]
+    index = VectorStoreIndex.from_documents(
+        documents=new_documents, service_context=mock_service_context
+    )
+    assert isinstance(index, VectorStoreIndex)
+
+    docstore = index.docstore.get_ref_doc_info("test_id_1")
+
+    assert docstore is not None
+
+    # test delete
+    index.delete_ref_doc("test_id_1", delete_from_docstore=True)
+
+    docstore = index.docstore.get_ref_doc_info("test_id_1")
+
+    assert docstore is None
 
 
 def test_simple_async(
     allow_networking: Any,
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     """Test simple vector index with use_async."""
 
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents=documents, use_async=True, service_context=mock_service_context
     )
-    assert isinstance(index, GPTVectorStoreIndex)
+    assert isinstance(index, VectorStoreIndex)
     assert len(index.index_struct.nodes_dict) == 4
     # check contents of nodes
     actual_node_tups = [
         ("Hello world.", [1, 0, 0, 0, 0]),
         ("This is a test.", [0, 1, 0, 0, 0]),
         ("This is another test.", [0, 0, 1, 0, 0]),
         ("This is a test v2.", [0, 0, 0, 1, 0]),
     ]
     for text_id in index.index_struct.nodes_dict.keys():
         node_id = index.index_struct.nodes_dict[text_id]
         node = index.docstore.get_node(node_id)
         vector_store = cast(SimpleVectorStore, index._vector_store)
         embedding = vector_store.get(text_id)
-        assert (node.text, embedding) in actual_node_tups
+        assert (node.get_content(), embedding) in actual_node_tups
 
 
 def test_simple_insert_save(
     documents: List[Document],
     mock_service_context: ServiceContext,
 ) -> None:
     storage_context = StorageContext.from_defaults()
-    index = GPTVectorStoreIndex.from_documents(
+    index = VectorStoreIndex.from_documents(
         documents=documents,
         service_context=mock_service_context,
         storage_context=storage_context,
     )
-    assert isinstance(index, GPTVectorStoreIndex)
+    assert isinstance(index, VectorStoreIndex)
 
     loaded_index = load_index_from_storage(storage_context=storage_context)
-    assert isinstance(loaded_index, GPTVectorStoreIndex)
+    assert isinstance(loaded_index, VectorStoreIndex)
     assert index.index_struct == loaded_index.index_struct
 
     # insert into index
     index.insert(Document(text="This is a test v3."))
 
     loaded_index = load_index_from_storage(storage_context=storage_context)
-    assert isinstance(loaded_index, GPTVectorStoreIndex)
+    assert isinstance(loaded_index, VectorStoreIndex)
     assert index.index_struct == loaded_index.index_struct
```

### Comparing `llama_index-0.6.9/tests/indices/vector_store/utils.py` & `llama_index-0.7.0/tests/indices/vector_store/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,14 +48,15 @@
         # sorted_distances = distances[indices][:top_k]
 
         matches = []
         for index in indices:
             tup = self._tuples[index]
             match = MagicMock()
             match.metadata = tup["metadata"]
+            match.id = tup["id"]
             matches.append(match)
 
         response = MagicMock()
         response.matches = matches
         return response
```

### Comparing `llama_index-0.6.9/tests/langchain_helpers/test_text_splitter.py` & `llama_index-0.7.0/tests/langchain_helpers/test_text_splitter.py`

 * *Files 4% similar despite different names*

```diff
@@ -40,25 +40,25 @@
     token = ("a" * 49) + "\n" + ("a" * 50)
     text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)
     chunks = text_splitter.split_text(token)
     assert len(chunks[0]) == 49
     assert len(chunks[1]) == 50
 
 
-def test_split_with_extra_info_str() -> None:
-    """Test split while taking into account chunk size used by extra info str."""
+def test_split_with_metadata_str() -> None:
+    """Test split while taking into account chunk size used by metadata str."""
     text = " ".join(["foo"] * 20)
-    extra_info_str = "test_extra_info_str"
+    metadata_str = "test_metadata_str"
 
     text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)
     chunks = text_splitter.split_text(text)
     assert len(chunks) == 1
 
     text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)
-    chunks = text_splitter.split_text(text, extra_info_str=extra_info_str)
+    chunks = text_splitter.split_text(text, metadata_str=metadata_str)
     assert len(chunks) == 2
 
 
 def test_split_diff_sentence_token() -> None:
     """Test case of a string that will split differently."""
     token_text_splitter = TokenTextSplitter(chunk_size=20, chunk_overlap=0)
     sentence_text_splitter = SentenceSplitter(chunk_size=20, chunk_overlap=0)
```

### Comparing `llama_index-0.6.9/tests/llm_predictor/test_base.py` & `llama_index-0.7.0/tests/llm_predictor/test_base.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,17 @@
 """LLM predictor tests."""
-import os
-from typing import Any, Tuple
+from typing import Any
 from unittest.mock import patch
 
-import pytest
-from langchain.llms.fake import FakeListLLM
 
 from llama_index.llm_predictor.structured import LLMPredictor, StructuredLLMPredictor
-from llama_index.output_parsers.base import BaseOutputParser
-from llama_index.prompts.default_prompts import DEFAULT_SIMPLE_INPUT_PROMPT
+from llama_index.types import BaseOutputParser
 from llama_index.prompts.prompts import Prompt, SimpleInputPrompt
 
 try:
-    from gptcache import Cache
 
     gptcache_installed = True
 except ImportError:
     gptcache_installed = False
 
 
 class MockOutputParser(BaseOutputParser):
@@ -27,71 +22,62 @@
         return output + "\n" + output
 
     def format(self, output: str) -> str:
         """Format output."""
         return output
 
 
-def mock_llmpredictor_predict(prompt: Prompt, **prompt_args: Any) -> Tuple[str, str]:
+def mock_llmpredictor_predict(prompt: Prompt, **prompt_args: Any) -> str:
     """Mock LLMPredictor predict."""
-    return prompt_args["query_str"], "mocked formatted prompt"
+    return prompt_args["query_str"]
 
 
 @patch.object(LLMPredictor, "predict", side_effect=mock_llmpredictor_predict)
 @patch.object(LLMPredictor, "__init__", return_value=None)
 def test_struct_llm_predictor(mock_init: Any, mock_predict: Any) -> None:
     """Test LLM predictor."""
     llm_predictor = StructuredLLMPredictor()
     output_parser = MockOutputParser()
     prompt = SimpleInputPrompt("{query_str}", output_parser=output_parser)
-    llm_prediction, formatted_output = llm_predictor.predict(
-        prompt, query_str="hello world"
-    )
+    llm_prediction = llm_predictor.predict(prompt, query_str="hello world")
     assert llm_prediction == "hello world\nhello world"
 
     # no change
     prompt = SimpleInputPrompt("{query_str}")
-    llm_prediction, formatted_output = llm_predictor.predict(
-        prompt, query_str="hello world"
-    )
+    llm_prediction = llm_predictor.predict(prompt, query_str="hello world")
     assert llm_prediction == "hello world"
 
 
-@pytest.mark.skipif(not gptcache_installed, reason="gptcache not installed")
-def test_struct_llm_predictor_with_cache() -> None:
-    """Test LLM predictor."""
-    from gptcache.processor.pre import get_prompt
-    from gptcache.manager.factory import get_data_manager
-    from langchain.cache import GPTCache
-
-    def init_gptcache_map(cache_obj: Cache) -> None:
-        cache_path = "test"
-        if os.path.isfile(cache_path):
-            os.remove(cache_path)
-        cache_obj.init(
-            pre_embedding_func=get_prompt,
-            data_manager=get_data_manager(data_path=cache_path),
-        )
-
-    responses = ["helloworld", "helloworld2"]
-
-    llm = FakeListLLM(responses=responses)
-    predictor = LLMPredictor(llm, False, GPTCache(init_gptcache_map))
-
-    prompt = DEFAULT_SIMPLE_INPUT_PROMPT
-    llm_prediction, formatted_output = predictor.predict(
-        prompt, query_str="hello world"
-    )
-    assert llm_prediction == "helloworld"
-
-    # due to cached result, faked llm is called only once
-    llm_prediction, formatted_output = predictor.predict(
-        prompt, query_str="hello world"
-    )
-    assert llm_prediction == "helloworld"
-
-    # no cache, return sequence
-    llm.cache = False
-    llm_prediction, formatted_output = predictor.predict(
-        prompt, query_str="hello world"
-    )
-    assert llm_prediction == "helloworld2"
+# TODO: bring back gptcache tests
+# @pytest.mark.skipif(not gptcache_installed, reason="gptcache not installed")
+# def test_struct_llm_predictor_with_cache() -> None:
+#     """Test LLM predictor."""
+#     from gptcache.processor.pre import get_prompt
+#     from gptcache.manager.factory import get_data_manager
+#     from llama_index.bridge.langchain import GPTCache
+
+#     def init_gptcache_map(cache_obj: Cache) -> None:
+#         cache_path = "test"
+#         if os.path.isfile(cache_path):
+#             os.remove(cache_path)
+#         cache_obj.init(
+#             pre_embedding_func=get_prompt,
+#             data_manager=get_data_manager(data_path=cache_path),
+#         )
+
+#     responses = ["helloworld", "helloworld2"]
+
+#     llm = FakeListLLM(responses=responses)
+#     predictor = LLMPredictor(llm, False, GPTCache(init_gptcache_map))
+
+#     prompt = DEFAULT_SIMPLE_INPUT_PROMPT
+#     llm_prediction = predictor.predict(prompt, query_str="hello world")
+#     assert llm_prediction == "helloworld"
+
+#     # due to cached result, faked llm is called only once
+#     llm_prediction = predictor.predict(prompt, query_str="hello world")
+#     assert llm_prediction == "helloworld"
+
+#     # no cache, return sequence
+#     llm.cache = False
+#     llm_prediction = predictor.predict(prompt, query_str="hello world")
+#     assert llm_prediction == "helloworld2"
```

### Comparing `llama_index-0.6.9/tests/logger/test_base.py` & `llama_index-0.7.0/tests/logger/test_base.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/mock_utils/mock_utils.py` & `llama_index-0.7.0/tests/mock_utils/mock_utils.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/output_parsers/test_base.py` & `llama_index-0.7.0/tests/output_parsers/test_base.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 """Test Output parsers."""
 
 
-from langchain.output_parsers import ResponseSchema
-from langchain.schema import BaseOutputParser as LCOutputParser
+from llama_index.bridge.langchain import (
+    ResponseSchema,
+    BaseOutputParser as LCOutputParser,
+)
 
 from llama_index.output_parsers.langchain import LangchainOutputParser
 
 
 class MockOutputParser(LCOutputParser):
     """Mock output parser.
```

### Comparing `llama_index-0.6.9/tests/output_parsers/test_selection.py` & `llama_index-0.7.0/tests/output_parsers/test_selection.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/playground/test_base.py` & `llama_index-0.7.0/tests/playground/test_base.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 """Test Playground."""
 
 from typing import List
 
 import pytest
-from llama_index.embeddings.base import BaseEmbedding
 
-from llama_index.indices.list.base import GPTListIndex
+from llama_index.embeddings.base import BaseEmbedding
+from llama_index.indices.list.base import ListIndex
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.tree.base import GPTTreeIndex
-from llama_index.indices.vector_store.base import GPTVectorStoreIndex
+from llama_index.indices.tree.base import TreeIndex
+from llama_index.indices.vector_store.base import VectorStoreIndex
 from llama_index.playground import DEFAULT_INDEX_CLASSES, DEFAULT_MODES, Playground
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 
 
 class MockEmbedding(BaseEmbedding):
     def _get_text_embedding(self, text: str) -> List[float]:
         """Mock get text embedding."""
         # assume dimensions are 5
         if text == "They're taking the Hobbits to Isengard!":
@@ -33,51 +33,51 @@
 
 
 def test_get_set_compare(
     mock_service_context: ServiceContext,
 ) -> None:
     """Test basic comparison of indices."""
     mock_service_context.embed_model = MockEmbedding()
-    documents = [Document("They're taking the Hobbits to Isengard!")]
+    documents = [Document(text="They're taking the Hobbits to Isengard!")]
 
     indices = [
-        GPTVectorStoreIndex.from_documents(
+        VectorStoreIndex.from_documents(
             documents=documents, service_context=mock_service_context
         ),
-        GPTListIndex.from_documents(documents, service_context=mock_service_context),
-        GPTTreeIndex.from_documents(
+        ListIndex.from_documents(documents, service_context=mock_service_context),
+        TreeIndex.from_documents(
             documents=documents, service_context=mock_service_context
         ),
     ]
 
     playground = Playground(indices=indices)  # type: ignore
 
     assert len(playground.indices) == 3
 
     results = playground.compare("Who is?", to_pandas=False)
     assert len(results) > 0
     assert len(results) <= 3 * len(DEFAULT_MODES)
 
     playground.indices = [
-        GPTVectorStoreIndex.from_documents(
+        VectorStoreIndex.from_documents(
             documents=documents, service_context=mock_service_context
         )
     ]
 
     assert len(playground.indices) == 1
 
 
 def test_from_docs(
     mock_service_context: ServiceContext,
 ) -> None:
     """Test initialization via a list of documents."""
     mock_service_context.embed_model = MockEmbedding()
     documents = [
-        Document("I can't carry it for you."),
-        Document("But I can carry you!"),
+        Document(text="I can't carry it for you."),
+        Document(text="But I can carry you!"),
     ]
 
     playground = Playground.from_docs(
         documents=documents, service_context=mock_service_context
     )
 
     assert len(playground.indices) == len(DEFAULT_INDEX_CLASSES)
@@ -90,19 +90,17 @@
             service_context=mock_service_context,
         )
 
 
 def test_validation() -> None:
     """Test validation of indices and modes."""
     with pytest.raises(ValueError):
-        _ = Playground(indices=["GPTVectorStoreIndex"])  # type: ignore
+        _ = Playground(indices=["VectorStoreIndex"])  # type: ignore
 
     with pytest.raises(ValueError):
-        _ = Playground(
-            indices=[GPTVectorStoreIndex, GPTListIndex, GPTTreeIndex]  # type: ignore
-        )
+        _ = Playground(indices=[VectorStoreIndex, ListIndex, TreeIndex])  # type: ignore
 
     with pytest.raises(ValueError):
         _ = Playground(indices=[])  # type: ignore
 
     with pytest.raises(TypeError):
         _ = Playground(retriever_modes={})  # type: ignore
```

### Comparing `llama_index-0.6.9/tests/prompts/test_base.py` & `llama_index-0.7.0/tests/prompts/test_base.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,122 +1,80 @@
 """Test prompts."""
 
-from typing import List
 from unittest.mock import MagicMock
 
 import pytest
-from langchain import PromptTemplate
-from langchain.chains.prompt_selector import ConditionalPromptSelector
-from langchain.chat_models.base import BaseChatModel
-from langchain.chat_models.openai import ChatOpenAI
 
+from llama_index.bridge.langchain import PromptTemplate
+from llama_index.llms.base import LLM
+from llama_index.llms.openai import OpenAI
 from llama_index.prompts.base import Prompt
+from llama_index.prompts.prompt_selector import PromptSelector
 
 
-class TestLanguageModel(ChatOpenAI):
-    """Test language model."""
-
-
-def is_test(llm: BaseChatModel) -> bool:
+def is_openai(llm: LLM) -> bool:
     """Test condition."""
-    return isinstance(llm, TestLanguageModel)
-
-
-class TestPrompt(Prompt):
-    """Test prompt class."""
-
-    input_variables: List[str] = ["text", "foo"]
-
-
-def test_prompt_validate() -> None:
-    """Test prompt validate."""
-    # assert passes
-    prompt_txt = "hello {text} {foo}"
-    TestPrompt(prompt_txt)
-
-    # assert fails (missing required values)
-    with pytest.raises(ValueError):
-        prompt_txt = "hello {tmp}"
-        TestPrompt(prompt_txt)
-
-    # assert fails (extraneous values)
-    with pytest.raises(ValueError):
-        prompt_txt = "hello {text} {foo} {text2}"
-        TestPrompt(prompt_txt)
+    return isinstance(llm, OpenAI)
 
 
 def test_partial_format() -> None:
     """Test partial format."""
     prompt_txt = "hello {text} {foo}"
-    prompt = TestPrompt(prompt_txt)
+    prompt = Prompt(prompt_txt)
 
     prompt_fmt = prompt.partial_format(foo="bar")
 
-    assert isinstance(prompt_fmt, TestPrompt)
+    assert isinstance(prompt_fmt, Prompt)
     assert prompt_fmt.format(text="world") == "hello world bar"
 
 
 def test_from_prompt() -> None:
     """Test new prompt from a partially formatted prompt."""
-
-    class TestPromptTextOnly(Prompt):
-        """Test prompt class."""
-
-        input_variables: List[str] = ["text"]
-
     prompt_txt = "hello {text} {foo}"
-    prompt = TestPrompt(prompt_txt)
+    prompt = Prompt(prompt_txt)
     prompt_fmt = prompt.partial_format(foo="bar")
 
-    prompt_new = TestPromptTextOnly.from_prompt(prompt_fmt)
-    assert isinstance(prompt_new, TestPromptTextOnly)
+    prompt_new = Prompt.from_prompt(prompt_fmt)
+    assert isinstance(prompt_new, Prompt)
 
     assert prompt_new.format(text="world2") == "hello world2 bar"
 
 
 def test_from_langchain_prompt() -> None:
     """Test from langchain prompt."""
     prompt_txt = "hello {text} {foo}"
     prompt = PromptTemplate(input_variables=["text", "foo"], template=prompt_txt)
-    prompt_new = TestPrompt.from_langchain_prompt(prompt)
+    prompt_new = Prompt.from_langchain_prompt(prompt)
 
-    assert isinstance(prompt_new, TestPrompt)
+    assert isinstance(prompt_new, Prompt)
     assert prompt_new.prompt == prompt
     assert prompt_new.format(text="world2", foo="bar") == "hello world2 bar"
 
-    # test errors if langchain prompt input var doesn't match
-    with pytest.raises(ValueError):
-        prompt_txt = "hello {text} {foo} {tmp}"
-        prompt = PromptTemplate(
-            input_variables=["text", "foo", "tmp"], template=prompt_txt
-        )
-        TestPrompt.from_langchain_prompt(prompt)
-
     # test errors if we specify both template and langchain prompt
     with pytest.raises(ValueError):
         prompt_txt = "hello {text} {foo}"
         prompt = PromptTemplate(input_variables=["text", "foo"], template=prompt_txt)
-        TestPrompt(template=prompt_txt, langchain_prompt=prompt)
+        Prompt(template=prompt_txt, langchain_prompt=prompt)
 
 
 def test_from_langchain_prompt_selector() -> None:
     """Test from langchain prompt selector."""
     prompt_txt = "hello {text} {foo}"
     prompt_txt_2 = "world {text} {foo}"
     prompt = PromptTemplate(input_variables=["text", "foo"], template=prompt_txt)
     prompt_2 = PromptTemplate(input_variables=["text", "foo"], template=prompt_txt_2)
 
-    test_prompt_selector = ConditionalPromptSelector(
-        default_prompt=prompt, conditionals=[(is_test, prompt_2)]
+    test_prompt_selector = PromptSelector(
+        default_prompt=prompt, conditionals=[(is_openai, prompt_2)]
     )
 
-    test_llm = MagicMock(spec=TestLanguageModel)
+    test_llm = MagicMock(spec=OpenAI)
 
-    prompt_new = TestPrompt.from_langchain_prompt_selector(test_prompt_selector)
-    assert isinstance(prompt_new, TestPrompt)
+    prompt_new = Prompt.from_langchain_prompt_selector(test_prompt_selector)
+    assert isinstance(prompt_new, Prompt)
     assert prompt_new.prompt == prompt
     assert prompt_new.format(text="world2", foo="bar") == "hello world2 bar"
     assert (
         prompt_new.format(llm=test_llm, text="world2", foo="bar") == "world world2 bar"
     )
 
     test_lc_prompt = prompt_new.get_langchain_prompt(llm=test_llm)
@@ -129,11 +87,11 @@
         prompt_txt = "hello {text} {foo}"
         prompt_txt_2 = "world {text} {foo} {tmp}"
         prompt = PromptTemplate(input_variables=["text", "foo"], template=prompt_txt)
         prompt_2 = PromptTemplate(
             input_variables=["text", "foo", "tmp"], template=prompt_txt_2
         )
 
-        test_prompt_selector = ConditionalPromptSelector(
-            prompt=prompt, conditionals=([is_test], [prompt_2])
+        test_prompt_selector = PromptSelector(
+            prompt=prompt, conditionals=([is_openai], [prompt_2])
         )
-        prompt_new = TestPrompt.from_langchain_prompt_selector(test_prompt_selector)
+        prompt_new = Prompt.from_langchain_prompt_selector(test_prompt_selector)
```

### Comparing `llama_index-0.6.9/tests/question_gen/test_llm_generators.py` & `llama_index-0.7.0/tests/question_gen/test_llm_generators.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/readers/test_file.py` & `llama_index-0.7.0/tests/readers/test_file.py`

 * *Files 3% similar despite different names*

```diff
@@ -198,15 +198,15 @@
             return {"filename": filename, "author": test_author}
 
         reader = SimpleDirectoryReader(tmp_dir, file_metadata=filename_to_metadata)
 
         documents = reader.load_data()
 
         for d in documents:
-            assert d.extra_info is not None and d.extra_info["author"] == test_author
+            assert d.metadata is not None and d.metadata["author"] == test_author
 
 
 def test_excluded_files() -> None:
     """Tests if files are excluded properly."""
     # test recursive
     with TemporaryDirectory() as tmp_dir:
         with open(f"{tmp_dir}/test1.txt", "w") as f:
@@ -268,7 +268,39 @@
                     )
                     input_file_names = [f.name for f in reader.input_files]
                     assert len(reader.input_files) == 2
                     assert set(input_file_names) == {
                         "test2.txt",
                         "test4.txt",
                     }
+
+
+def test_filename_as_doc_id() -> None:
+    """Test if file metadata is added to Document."""
+    # test file_metadata
+    with TemporaryDirectory() as tmp_dir:
+        with open(f"{tmp_dir}/test1.txt", "w") as f:
+            f.write("test1")
+        with open(f"{tmp_dir}/test2.txt", "w") as f:
+            f.write("test2")
+        with open(f"{tmp_dir}/test3.txt", "w") as f:
+            f.write("test3")
+        with open(f"{tmp_dir}/test4.md", "w") as f:
+            f.write("test4")
+        with open(f"{tmp_dir}/test5.json", "w") as f:
+            f.write('{"test_1": {"test_2": [1, 2, 3]}}')
+
+        reader = SimpleDirectoryReader(tmp_dir, filename_as_id=True)
+
+        documents = reader.load_data()
+
+        doc_paths = [
+            f"{tmp_dir}/test1.txt",
+            f"{tmp_dir}/test2.txt",
+            f"{tmp_dir}/test3.txt",
+            f"{tmp_dir}/test4.md",
+            f"{tmp_dir}/test5.json",
+        ]
+
+        # check paths. Split handles path_part_X doc_ids from md and json files
+        for doc in documents:
+            assert str(doc.node_id).split("_part")[0] in doc_paths
```

### Comparing `llama_index-0.6.9/tests/readers/test_mongo.py` & `llama_index-0.7.0/tests/readers/test_mongo.py`

 * *Files 8% similar despite different names*

```diff
@@ -18,17 +18,17 @@
     with patch("pymongo.collection.Collection.find") as mock_find:
         mock_find.return_value = mock_cursor
 
         reader = SimpleMongoReader("host", 1)
         documents = reader.load_data("my_db", "my_collection")
 
         assert len(documents) == 3
-        assert documents[0].text == "one"
-        assert documents[1].text == "two"
-        assert documents[2].text == "three"
+        assert documents[0].get_content() == "one"
+        assert documents[1].get_content() == "two"
+        assert documents[2].get_content() == "three"
 
 
 @pytest.mark.skipif(MongoClient is None, reason="pymongo not installed")
 def test_load_data_with_field_name() -> None:
     """Test Mongo reader using passed in field_names."""
     mock_cursor = [
         {"first": "first1", "second": "second1", "third": "third1"},
@@ -41,10 +41,10 @@
 
         reader = SimpleMongoReader("host", 1)
         documents = reader.load_data(
             "my_db", "my_collection", field_names=["first", "second", "third"]
         )
 
         assert len(documents) == 3
-        assert documents[0].text == "first1second1third1"
-        assert documents[1].text == "first2second2third2"
-        assert documents[2].text == "first3second3third3"
+        assert documents[0].get_content() == "first1second1third1"
+        assert documents[1].get_content() == "first2second2third2"
+        assert documents[2].get_content() == "first3second3third3"
```

### Comparing `llama_index-0.6.9/tests/selectors/test_llm_selectors.py` & `llama_index-0.7.0/tests/selectors/test_llm_selectors.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/storage/conftest.py` & `llama_index-0.7.0/tests/storage/conftest.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import pytest
 from llama_index.storage.kvstore.mongodb_kvstore import MongoDBKVStore
+from llama_index.storage.kvstore.redis_kvstore import RedisKVStore
 from llama_index.storage.kvstore.simple_kvstore import SimpleKVStore
 from tests.storage.kvstore.mock_mongodb import MockMongoClient
 
 
 @pytest.fixture()
 def mongo_client() -> MockMongoClient:
     return MockMongoClient()
@@ -13,7 +14,18 @@
 def mongo_kvstore(mongo_client: MockMongoClient) -> MongoDBKVStore:
     return MongoDBKVStore(mongo_client=mongo_client)  # type: ignore
 
 
 @pytest.fixture()
 def simple_kvstore() -> SimpleKVStore:
     return SimpleKVStore()
+
+
+@pytest.fixture()
+def redis_kvstore() -> "RedisKVStore":
+    try:
+        from redis import Redis
+
+        client = Redis.from_url(url="redis://127.0.0.1:6379")
+    except ImportError:
+        return RedisKVStore(redis_client=None, redis_url="redis://127.0.0.1:6379")
+    return RedisKVStore(redis_client=client)
```

### Comparing `llama_index-0.6.9/tests/storage/docstore/test_mongo_docstore.py` & `llama_index-0.7.0/tests/storage/docstore/test_mongo_docstore.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,27 +1,27 @@
 from typing import List
 
 import pytest
 
 from llama_index.storage.docstore.mongo_docstore import MongoDocumentStore
-from llama_index.readers.schema.base import Document
-from llama_index.schema import BaseDocument
+from llama_index.schema import Document
+from llama_index.schema import BaseNode
 from llama_index.storage.kvstore.mongodb_kvstore import MongoDBKVStore
 
 try:
     from pymongo import MongoClient
 except ImportError:
     MongoClient = None  # type: ignore
 
 
 @pytest.fixture
 def documents() -> List[Document]:
     return [
-        Document("doc_1"),
-        Document("doc_2"),
+        Document(text="doc_1"),
+        Document(text="doc_2"),
     ]
 
 
 @pytest.fixture()
 def mongodb_docstore(mongo_kvstore: MongoDBKVStore) -> MongoDocumentStore:
     return MongoDocumentStore(mongo_kvstore=mongo_kvstore)
 
@@ -32,25 +32,25 @@
 ) -> None:
     ds = mongodb_docstore
     assert len(ds.docs) == 0
 
     # test adding documents
     ds.add_documents(documents)
     assert len(ds.docs) == 2
-    assert all(isinstance(doc, BaseDocument) for doc in ds.docs.values())
+    assert all(isinstance(doc, BaseNode) for doc in ds.docs.values())
 
     # test updating documents
     ds.add_documents(documents)
     print(ds.docs)
     assert len(ds.docs) == 2
 
     # test getting documents
     doc0 = ds.get_document(documents[0].get_doc_id())
     assert doc0 is not None
-    assert documents[0].text == doc0.text
+    assert documents[0].get_content() == doc0.get_content()
 
     # test deleting documents
     ds.delete_document(documents[0].get_doc_id())
     assert len(ds.docs) == 1
 
 
 @pytest.mark.skipif(MongoClient is None, reason="pymongo not installed")
```

### Comparing `llama_index-0.6.9/tests/storage/docstore/test_simple_docstore.py` & `llama_index-0.7.0/tests/storage/docstore/test_simple_docstore.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,42 +1,43 @@
 """Test docstore."""
 
 
 from pathlib import Path
 import pytest
-from llama_index.data_structs.node import Node
+
+from llama_index.schema import TextNode
 from llama_index.storage.docstore import SimpleDocumentStore
-from llama_index.readers.schema.base import Document
+from llama_index.schema import Document
 from llama_index.storage.kvstore.simple_kvstore import SimpleKVStore
 
 
 @pytest.fixture()
 def simple_docstore(simple_kvstore: SimpleKVStore) -> SimpleDocumentStore:
     return SimpleDocumentStore(simple_kvstore=simple_kvstore)
 
 
 def test_docstore(simple_docstore: SimpleDocumentStore) -> None:
     """Test docstore."""
-    doc = Document("hello world", doc_id="d1", extra_info={"foo": "bar"})
-    node = Node("my node", doc_id="d2", node_info={"node": "info"})
+    doc = Document(text="hello world", id_="d1", metadata={"foo": "bar"})
+    node = TextNode(text="my node", id_="d2", metadata={"node": "info"})
 
     # test get document
     docstore = simple_docstore
     docstore.add_documents([doc, node])
     gd1 = docstore.get_document("d1")
     assert gd1 == doc
     gd2 = docstore.get_document("d2")
     assert gd2 == node
 
 
 def test_docstore_persist(tmp_path: Path) -> None:
     """Test docstore."""
     persist_path = str(tmp_path / "test_file.txt")
-    doc = Document("hello world", doc_id="d1", extra_info={"foo": "bar"})
-    node = Node("my node", doc_id="d2", node_info={"node": "info"})
+    doc = Document(text="hello world", id_="d1", metadata={"foo": "bar"})
+    node = TextNode(text="my node", id_="d2", metadata={"node": "info"})
 
     # add documents and then persist to dir
     docstore = SimpleDocumentStore()
     docstore.add_documents([doc, node])
     docstore.persist(persist_path)
 
     # load from persist dir and get documents
@@ -44,16 +45,16 @@
     gd1 = new_docstore.get_document("d1")
     assert gd1 == doc
     gd2 = new_docstore.get_document("d2")
     assert gd2 == node
 
 
 def test_docstore_dict() -> None:
-    doc = Document("hello world", doc_id="d1", extra_info={"foo": "bar"})
-    node = Node("my node", doc_id="d2", node_info={"node": "info"})
+    doc = Document(text="hello world", id_="d1", metadata={"foo": "bar"})
+    node = TextNode(text="my node", id_="d2", metadata={"node": "info"})
 
     # add documents and then save to dict
     docstore = SimpleDocumentStore()
     docstore.add_documents([doc, node])
     save_dict = docstore.to_dict()
 
     # load from dict and get documents
```

### Comparing `llama_index-0.6.9/tests/storage/test_storage_context.py` & `llama_index-0.7.0/tests/storage/test_storage_context.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 from llama_index.data_structs.data_structs import IndexDict
-from llama_index.data_structs.node import Node
+from llama_index.schema import TextNode
 from llama_index.storage.storage_context import StorageContext
 from llama_index.vector_stores.types import NodeWithEmbedding
 
 
 def test_storage_context_dict() -> None:
     storage_context = StorageContext.from_defaults()
 
     # add
-    node = Node("test")
+    node = TextNode(text="test")
     index_struct = IndexDict()
     storage_context.vector_store.add(
         [NodeWithEmbedding(node=node, embedding=[0.0, 0.0, 0.0])]
     )
     storage_context.docstore.add_documents([node])
     storage_context.index_store.add_index_struct(index_struct)
 
     # save
     save_dict = storage_context.to_dict()
 
     # load
     loaded_storage_context = StorageContext.from_dict(save_dict)
 
     # test
-    assert loaded_storage_context.docstore.get_node(node.get_doc_id()) == node
+    assert loaded_storage_context.docstore.get_node(node.node_id) == node
     assert (
         storage_context.index_store.get_index_struct(index_struct.index_id)
         == index_struct
     )
```

### Comparing `llama_index-0.6.9/tests/test_utils.py` & `llama_index-0.7.0/tests/test_utils.py`

 * *Files identical despite different names*

### Comparing `llama_index-0.6.9/tests/token_predictor/test_base.py` & `llama_index-0.7.0/tests/token_predictor/test_base.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,51 +1,46 @@
 """Test token predictor."""
 
 from typing import Any
-from unittest.mock import MagicMock, patch
+from unittest.mock import patch
 
-from langchain.llms.base import BaseLLM
-
-from llama_index.indices.keyword_table.base import GPTKeywordTableIndex
-from llama_index.indices.list.base import GPTListIndex
+from llama_index.indices.keyword_table.base import KeywordTableIndex
+from llama_index.indices.list.base import ListIndex
 from llama_index.indices.service_context import ServiceContext
-from llama_index.indices.tree.base import GPTTreeIndex
+from llama_index.indices.tree.base import TreeIndex
 from llama_index.langchain_helpers.text_splitter import TokenTextSplitter
-from llama_index.readers.schema.base import Document
-from llama_index.token_counter.mock_chain_wrapper import MockLLMPredictor
+from llama_index.schema import Document
+from llama_index.llm_predictor.mock import MockLLMPredictor
 from tests.mock_utils.mock_text_splitter import mock_token_splitter_newline
 
 
 @patch.object(TokenTextSplitter, "split_text", side_effect=mock_token_splitter_newline)
 def test_token_predictor(mock_split: Any) -> None:
     """Test token predictor."""
     # here, just assert that token predictor runs (before checking behavior)
     # TODO: mock token counting a bit more carefully
     doc_text = (
         "Hello world.\n"
         "This is a test.\n"
         "This is another test.\n"
         "This is a test v2."
     )
-    document = Document(doc_text)
-    llm = MagicMock(spec=BaseLLM)
-    llm_predictor = MockLLMPredictor(max_tokens=256, llm=llm)
+    document = Document(text=doc_text)
+    llm_predictor = MockLLMPredictor(max_tokens=256)
     service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)
 
     # test tree index
-    index = GPTTreeIndex.from_documents([document], service_context=service_context)
+    index = TreeIndex.from_documents([document], service_context=service_context)
     query_engine = index.as_query_engine()
     query_engine.query("What is?")
 
     # test keyword table index
-    index_keyword = GPTKeywordTableIndex.from_documents(
+    index_keyword = KeywordTableIndex.from_documents(
         [document], service_context=service_context
     )
     query_engine = index_keyword.as_query_engine()
     query_engine.query("What is?")
 
     # test list index
-    index_list = GPTListIndex.from_documents(
-        [document], service_context=service_context
-    )
+    index_list = ListIndex.from_documents([document], service_context=service_context)
     query_engine = index_list.as_query_engine()
     query_engine.query("What is?")
```

### Comparing `llama_index-0.6.9/tests/vector_stores/test_qdrant.py` & `llama_index-0.7.0/tests/vector_stores/test_qdrant.py`

 * *Files 20% similar despite different names*

```diff
@@ -3,36 +3,53 @@
 import pytest
 
 try:
     import qdrant_client
 except ImportError:
     qdrant_client = None  # type: ignore
 
-from llama_index.data_structs.node import DocumentRelationship, Node
+from llama_index.schema import NodeRelationship, RelatedNodeInfo, TextNode
 from llama_index.vector_stores import QdrantVectorStore
-from llama_index.vector_stores.types import NodeWithEmbedding, VectorStoreQuery
+from llama_index.vector_stores.types import (
+    NodeWithEmbedding,
+    VectorStoreQuery,
+    MetadataFilters,
+    ExactMatchFilter,
+)
 
 
 @pytest.fixture
 def node_embeddings() -> List[NodeWithEmbedding]:
     return [
         NodeWithEmbedding(
             embedding=[1.0, 0.0],
-            node=Node(
+            node=TextNode(
                 text="lorem ipsum",
-                doc_id="c330d77f-90bd-4c51-9ed2-57d8d693b3b0",
-                relationships={DocumentRelationship.SOURCE: "test-0"},
+                id_="c330d77f-90bd-4c51-9ed2-57d8d693b3b0",
+                relationships={
+                    NodeRelationship.SOURCE: RelatedNodeInfo(node_id="test-0")
+                },
+                metadata={
+                    "author": "Stephen King",
+                    "theme": "Friendship",
+                },
             ),
         ),
         NodeWithEmbedding(
             embedding=[0.0, 1.0],
-            node=Node(
+            node=TextNode(
                 text="lorem ipsum",
-                doc_id="c3d1e1dd-8fb4-4b8f-b7ea-7fa96038d39d",
-                relationships={DocumentRelationship.SOURCE: "test-1"},
+                id_="c3d1e1dd-8fb4-4b8f-b7ea-7fa96038d39d",
+                relationships={
+                    NodeRelationship.SOURCE: RelatedNodeInfo(node_id="test-1")
+                },
+                metadata={
+                    "director": "Francis Ford Coppola",
+                    "theme": "Mafia",
+                },
             ),
         ),
     ]
 
 
 @pytest.mark.skipif(qdrant_client is None, reason="qdrant-client not installed")
 def test_add_stores_data(node_embeddings: List[NodeWithEmbedding]) -> None:
@@ -73,44 +90,66 @@
     assert isinstance(query_filter.must[0], FieldCondition)  # type: ignore[index]
     assert query_filter.must[0].key == "doc_id"  # type: ignore[index]
     assert isinstance(query_filter.must[0].match, MatchAny)  # type: ignore[index]
     assert query_filter.must[0].match.any == ["1", "2", "3"]  # type: ignore[index]
 
 
 @pytest.mark.skipif(qdrant_client is None, reason="qdrant-client not installed")
-def test_build_query_filter_returns_text_filter() -> None:
-    from qdrant_client.http.models import FieldCondition, Filter, MatchText
+def test_build_query_filter_returns_empty_filter_on_query_str() -> None:
+    from qdrant_client.http.models import Filter
 
     client = qdrant_client.QdrantClient(":memory:")
     qdrant_vector_store = QdrantVectorStore(collection_name="test", client=client)
 
     query = VectorStoreQuery(query_str="lorem")
     query_filter = cast(Filter, qdrant_vector_store._build_query_filter(query))
 
     assert query_filter is not None
-    assert len(query_filter.must) == 1  # type: ignore[index, arg-type]
-    assert isinstance(query_filter.must[0], FieldCondition)  # type: ignore[index]
-    assert query_filter.must[0].key == "text"  # type: ignore[index]
-    assert isinstance(query_filter.must[0].match, MatchText)  # type: ignore[index]
-    assert query_filter.must[0].match.text == "lorem"  # type: ignore[index]
+    assert len(query_filter.must) == 0  # type: ignore[index, arg-type]
 
 
 @pytest.mark.skipif(qdrant_client is None, reason="qdrant-client not installed")
 def test_build_query_filter_returns_combined_filter() -> None:
-    from qdrant_client.http.models import FieldCondition, Filter, MatchAny, MatchText
+    from qdrant_client.http.models import (
+        FieldCondition,
+        Filter,
+        MatchAny,
+        MatchValue,
+        Range,
+    )
 
     client = qdrant_client.QdrantClient(":memory:")
     qdrant_vector_store = QdrantVectorStore(collection_name="test", client=client)
 
-    query = VectorStoreQuery(query_str="lorem", doc_ids=["1", "2", "3"])
+    filters = MetadataFilters(
+        filters=[
+            ExactMatchFilter(key="text_field", value="text_value"),
+            ExactMatchFilter(key="int_field", value=4),
+            ExactMatchFilter(key="float_field", value=3.5),
+        ]
+    )
+    query = VectorStoreQuery(doc_ids=["1", "2", "3"], filters=filters)
     query_filter = cast(Filter, qdrant_vector_store._build_query_filter(query))
 
     assert query_filter is not None
-    assert len(query_filter.must) == 2  # type: ignore[index, arg-type]
+    assert len(query_filter.must) == 4  # type: ignore[index, arg-type]
+
     assert isinstance(query_filter.must[0], FieldCondition)  # type: ignore[index]
     assert query_filter.must[0].key == "doc_id"  # type: ignore[index]
     assert isinstance(query_filter.must[0].match, MatchAny)  # type: ignore[index]
     assert query_filter.must[0].match.any == ["1", "2", "3"]  # type: ignore[index]
+
     assert isinstance(query_filter.must[1], FieldCondition)  # type: ignore[index]
-    assert query_filter.must[1].key == "text"  # type: ignore[index]
-    assert isinstance(query_filter.must[1].match, MatchText)  # type: ignore[index]
-    assert query_filter.must[1].match.text == "lorem"  # type: ignore[index]
+    assert query_filter.must[1].key == "text_field"  # type: ignore[index]
+    assert isinstance(query_filter.must[1].match, MatchValue)  # type: ignore[index]
+    assert query_filter.must[1].match.value == "text_value"  # type: ignore[index]
+
+    assert isinstance(query_filter.must[2], FieldCondition)  # type: ignore[index]
+    assert query_filter.must[2].key == "int_field"  # type: ignore[index]
+    assert isinstance(query_filter.must[2].match, MatchValue)  # type: ignore[index]
+    assert query_filter.must[2].match.value == 4  # type: ignore[index]
+
+    assert isinstance(query_filter.must[3], FieldCondition)  # type: ignore[index]
+    assert query_filter.must[3].key == "float_field"  # type: ignore[index]
+    assert isinstance(query_filter.must[3].range, Range)  # type: ignore[index]
+    assert query_filter.must[3].range.gte == 3.5  # type: ignore[index]
+    assert query_filter.must[3].range.lte == 3.5  # type: ignore[index]
```

### Comparing `llama_index-0.6.9/tests/vector_stores/test_weaviate.py` & `llama_index-0.7.0/tests/vector_stores/test_weaviate.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import sys
 from unittest.mock import MagicMock
-from llama_index.data_structs.node import DocumentRelationship, Node
+
+from llama_index.schema import NodeRelationship, RelatedNodeInfo, TextNode
 from llama_index.vector_stores.types import NodeWithEmbedding
 
 from llama_index.vector_stores.weaviate import WeaviateVectorStore
 
 
 def test_weaviate_add() -> None:
     # mock import
@@ -14,18 +15,20 @@
     weaviate_client.batch.__enter__.return_value = batch_context_manager
 
     vector_store = WeaviateVectorStore(weaviate_client=weaviate_client)
 
     vector_store.add(
         [
             NodeWithEmbedding(
-                node=Node(
+                node=TextNode(
                     text="test node text",
-                    doc_id="test node id",
-                    relationships={DocumentRelationship.SOURCE: "test doc id"},
+                    id_="test node id",
+                    relationships={
+                        NodeRelationship.SOURCE: RelatedNodeInfo(node_id="test doc id")
+                    },
                 ),
                 embedding=[0.5, 0.5],
             )
         ]
     )
 
     args, _ = batch_context_manager.add_data_object.call_args
```

